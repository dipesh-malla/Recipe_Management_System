{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec31276",
   "metadata": {
    "id": "9ec31276"
   },
   "source": [
    "# Recipe Recommendation Model Training with Metrics\n",
    "# Google Colab Notebook\n",
    "\n",
    "This notebook trains my recommendation models (ALS, Two-Tower, GraphSAGE) with **real-time performance evaluation** using train/test/validation splits.\n",
    "\n",
    "## Key Metrics:\n",
    "- **NDCG@K** - How good is ranking?\n",
    "- **Precision@K** - % of recommendations that are relevant\n",
    "- **Recall@K** - % of relevant items found\n",
    "- **MAP** - Mean Average Precision\n",
    "- **MRR** - Mean Reciprocal Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459f4126",
   "metadata": {
    "id": "459f4126"
   },
   "source": [
    "## 1. Setup Environment and Install Dependencies\n",
    "\n",
    "Install all required libraries for training recommendation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e959ca8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e959ca8",
    "outputId": "6fc92f4f-1fbd-4fba-ca11-13bafa450335"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "GPU Device: Tesla T4\n",
      "GPU Memory: 15.83 GB\n",
      "\n",
      " Installing dependencies...\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for implicit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "✅ All dependencies installed!\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"No GPU detected. Go to Runtime → Change runtime type → Select GPU\")\n",
    "\n",
    "# Install required packages\n",
    "print(\"\\n Installing dependencies...\")\n",
    "!pip install -q implicit\n",
    "!pip install -q torch-scatter torch-sparse torch-geometric -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
    "\n",
    "print(\"✅ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veXJU0HHvVIs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "veXJU0HHvVIs",
    "outputId": "f6f11860-0409-4d3a-d4cc-279166345c24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.8.0+cu126\n",
      "PyTorch Geometric: 2.7.0\n",
      "Implicit: 0.7.2\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "import implicit\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"PyTorch Geometric:\", torch_geometric.__version__)\n",
    "print(\"Implicit:\", implicit.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336454e2",
   "metadata": {
    "id": "336454e2"
   },
   "source": [
    "## 2️. Mount Google Drive and Upload Dataset\n",
    "\n",
    "Mount Google Drive to save/load models and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6281312f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6281312f",
    "outputId": "0fd6fab8-aca5-4dd3-e166-f9c5a6dc4b92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "✅ Google Drive mounted!\n",
      "Model directory: /content/drive/MyDrive/RecipeML/models\n",
      "Data directory: /content/drive/MyDrive/RecipeML/data\n",
      "Results directory: /content/drive/MyDrive/RecipeML/results\n",
      "\n",
      " Upload your CSV files (users.csv, recipes.csv, interactions.csv) to:\n",
      "   /content/drive/MyDrive/RecipeML/data/\n",
      "\n",
      "Or run the file upload in the next cell.\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories for models and data\n",
    "os.makedirs('/content/drive/MyDrive/RecipeML/models', exist_ok=True)\n",
    "os.makedirs('/content/drive/MyDrive/RecipeML/data', exist_ok=True)\n",
    "os.makedirs('/content/drive/MyDrive/RecipeML/results', exist_ok=True)\n",
    "\n",
    "print(\"✅ Google Drive mounted!\")\n",
    "print(f\"Model directory: /content/drive/MyDrive/RecipeML/models\")\n",
    "print(f\"Data directory: /content/drive/MyDrive/RecipeML/data\")\n",
    "print(f\"Results directory: /content/drive/MyDrive/RecipeML/results\")\n",
    "\n",
    "# Set paths\n",
    "DATA_DIR = '/content/drive/MyDrive/RecipeML/data'\n",
    "MODEL_DIR = '/content/drive/MyDrive/RecipeML/models'\n",
    "RESULTS_DIR = '/content/drive/MyDrive/RecipeML/results'\n",
    "\n",
    "print(\"\\n Upload your CSV files (users.csv, recipes.csv, interactions.csv) to:\")\n",
    "print(f\"   {DATA_DIR}/\")\n",
    "print(\"\\nOr run the file upload in the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2068538f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "2068538f",
    "outputId": "9dbdd556-903a-48c3-8496-850fb8eafd69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Upload your dataset files (users.csv, recipes.csv, interactions.csv)\n",
      "Or skip this if files are already in Google Drive\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-7741c399-9f7a-4927-b889-27224dcc734c\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-7741c399-9f7a-4927-b889-27224dcc734c\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving follows.csv to follows.csv\n",
      "Saving interactions.csv to interactions.csv\n",
      "Saving posts.csv to posts.csv\n",
      "Saving recipes.csv to recipes.csv\n",
      "Saving test_interactions.csv to test_interactions.csv\n",
      "Saving train_interactions.csv to train_interactions.csv\n",
      "Saving users.csv to users.csv\n",
      "Saving val_interactions.csv to val_interactions.csv\n",
      "✅ Moved follows.csv to /content/drive/MyDrive/RecipeML/data/\n",
      "✅ Moved interactions.csv to /content/drive/MyDrive/RecipeML/data/\n",
      "✅ Moved posts.csv to /content/drive/MyDrive/RecipeML/data/\n",
      "✅ Moved recipes.csv to /content/drive/MyDrive/RecipeML/data/\n",
      "✅ Moved test_interactions.csv to /content/drive/MyDrive/RecipeML/data/\n",
      "✅ Moved train_interactions.csv to /content/drive/MyDrive/RecipeML/data/\n",
      "✅ Moved users.csv to /content/drive/MyDrive/RecipeML/data/\n",
      "✅ Moved val_interactions.csv to /content/drive/MyDrive/RecipeML/data/\n"
     ]
    }
   ],
   "source": [
    "# Optional: Upload files directly from your computer\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "print(\" Upload your dataset files (users.csv, recipes.csv, interactions.csv)\")\n",
    "print(\"Or skip this if files are already in Google Drive\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Move uploaded files to Drive\n",
    "for filename in uploaded.keys():\n",
    "    shutil.move(filename, f\"{DATA_DIR}/{filename}\")\n",
    "    print(f\"✅ Moved {filename} to {DATA_DIR}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedb91b0",
   "metadata": {
    "id": "eedb91b0"
   },
   "source": [
    "## 3️. Load and Explore Dataset\n",
    "\n",
    "Load CSV files and explore the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f2bd75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "50f2bd75",
    "outputId": "12015086-cde3-4517-cbca-e433d15fbe9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      " LOADING DATASET\n",
      "======================================================================\n",
      "\n",
      "✅ Loaded 5,000 users\n",
      "✅ Loaded 10,457 recipes\n",
      "✅ Loaded 1,334,327 interactions\n",
      "\n",
      "======================================================================\n",
      " DATASET OVERVIEW\n",
      "======================================================================\n",
      "\n",
      " USERS DATA:\n",
      "   Shape: (5000, 6)\n",
      "   Columns: ['user_id', 'age', 'gender', 'location', 'join_date', 'user_segment']\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"    print(\\\"   Required files: users\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"user_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20,\n        \"min\": 28,\n        \"max\": 68,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          59,\n          68,\n          28\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gender\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"M\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"location\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Sydney, AU\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"join_date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2025-07-02\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user_segment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Adventurous Foodies\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-eb1e0f4b-4669-4dd0-a5f8-e8a8ba2c9407\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>location</th>\n",
       "      <th>join_date</th>\n",
       "      <th>user_segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>M</td>\n",
       "      <td>Sydney, AU</td>\n",
       "      <td>2025-07-02</td>\n",
       "      <td>Quick Meal Seekers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>M</td>\n",
       "      <td>Berlin, DE</td>\n",
       "      <td>2025-06-27</td>\n",
       "      <td>Adventurous Foodies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>M</td>\n",
       "      <td>Seoul, KR</td>\n",
       "      <td>2025-05-12</td>\n",
       "      <td>Adventurous Foodies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb1e0f4b-4669-4dd0-a5f8-e8a8ba2c9407')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-eb1e0f4b-4669-4dd0-a5f8-e8a8ba2c9407 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-eb1e0f4b-4669-4dd0-a5f8-e8a8ba2c9407');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-9889537e-84e8-4872-92fc-db696d4d3ca6\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9889537e-84e8-4872-92fc-db696d4d3ca6')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-9889537e-84e8-4872-92fc-db696d4d3ca6 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   user_id  age gender    location   join_date         user_segment\n",
       "0        1   59      M  Sydney, AU  2025-07-02   Quick Meal Seekers\n",
       "1        2   68      M  Berlin, DE  2025-06-27  Adventurous Foodies\n",
       "2        3   28      M   Seoul, KR  2025-05-12  Adventurous Foodies"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " RECIPES DATA:\n",
      "   Shape: (10457, 26)\n",
      "   Columns: ['id', 'author_id', 'title', 'description', 'dietary_type', 'cuisine', 'servings', 'cook_time', 'difficulty', 'prep_time']...\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-ee2beabc-43b3-43b8-9e8a-b173f07e00ef\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>dietary_type</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>servings</th>\n",
       "      <th>cook_time</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>prep_time</th>\n",
       "      <th>...</th>\n",
       "      <th>cooking_method</th>\n",
       "      <th>is_public</th>\n",
       "      <th>created_date</th>\n",
       "      <th>modified_date</th>\n",
       "      <th>popularity_score</th>\n",
       "      <th>view_count</th>\n",
       "      <th>save_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Light Tofu Wrap (Turkish)</td>\n",
       "      <td>One of my favorite recipes! This Light Tofu Wr...</td>\n",
       "      <td>Mediterranean</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>8</td>\n",
       "      <td>105</td>\n",
       "      <td>Hard</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>Slow Cooked</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-07-06T10:43:33.403414</td>\n",
       "      <td>2024-07-06T10:43:33.403414</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Delicious Lamb Tacos (Turkish)</td>\n",
       "      <td>Authentic Turkish recipe that will transport y...</td>\n",
       "      <td>Mediterranean</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>Hard</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>Smoked</td>\n",
       "      <td>True</td>\n",
       "      <td>2025-03-01T10:43:33.403882</td>\n",
       "      <td>2025-03-01T10:43:33.403882</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Rich Lamb Bowl (Turkish)</td>\n",
       "      <td>Healthy and nutritious Rich Lamb Bowl (Turkish...</td>\n",
       "      <td>Mediterranean</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>Hard</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>Air Fried</td>\n",
       "      <td>True</td>\n",
       "      <td>2025-10-11T10:43:33.404311</td>\n",
       "      <td>2025-10-11T10:43:33.404311</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 26 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee2beabc-43b3-43b8-9e8a-b173f07e00ef')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-ee2beabc-43b3-43b8-9e8a-b173f07e00ef button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-ee2beabc-43b3-43b8-9e8a-b173f07e00ef');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-febdc548-92c0-45a3-83f6-823950aebf69\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-febdc548-92c0-45a3-83f6-823950aebf69')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-febdc548-92c0-45a3-83f6-823950aebf69 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   id  author_id                           title  \\\n",
       "0   1          1       Light Tofu Wrap (Turkish)   \n",
       "1   2          1  Delicious Lamb Tacos (Turkish)   \n",
       "2   3          1        Rich Lamb Bowl (Turkish)   \n",
       "\n",
       "                                         description   dietary_type  cuisine  \\\n",
       "0  One of my favorite recipes! This Light Tofu Wr...  Mediterranean  Turkish   \n",
       "1  Authentic Turkish recipe that will transport y...  Mediterranean  Turkish   \n",
       "2  Healthy and nutritious Rich Lamb Bowl (Turkish...  Mediterranean  Turkish   \n",
       "\n",
       "   servings  cook_time difficulty  prep_time  ...  cooking_method  is_public  \\\n",
       "0         8        105       Hard         15  ...     Slow Cooked      False   \n",
       "1         2         23       Hard         30  ...          Smoked       True   \n",
       "2         4         47       Hard         13  ...       Air Fried       True   \n",
       "\n",
       "                 created_date               modified_date popularity_score  \\\n",
       "0  2024-07-06T10:43:33.403414  2024-07-06T10:43:33.403414           0.0017   \n",
       "1  2025-03-01T10:43:33.403882  2025-03-01T10:43:33.403882           0.0070   \n",
       "2  2025-10-11T10:43:33.404311  2025-10-11T10:43:33.404311           0.0002   \n",
       "\n",
       "  view_count save_count  like_count comment_count avg_rating  \n",
       "0          2          0           0             1        0.0  \n",
       "1          5          1           1             1        0.0  \n",
       "2          2          0           0             0        0.0  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " INTERACTIONS DATA:\n",
      "   Shape: (1334327, 5)\n",
      "   Columns: ['user_id', 'recipe_id', 'rating', 'timestamp', 'interaction_type']\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"    print(\\\"   Required files: users\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"user_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recipe_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 1,\n        \"max\": 13,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9001666512374251,\n        \"min\": 2.81,\n        \"max\": 4.61,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          4.61\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2024-11-27 13:30:36\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"interaction_type\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"comment\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-f11966b8-f511-414a-9cf6-2079c7811f3f\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>recipe_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>interaction_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.61</td>\n",
       "      <td>2024-11-27 13:30:36</td>\n",
       "      <td>rating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2.81</td>\n",
       "      <td>2024-11-10 16:59:40</td>\n",
       "      <td>rating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>3.68</td>\n",
       "      <td>2025-04-26 12:07:08</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f11966b8-f511-414a-9cf6-2079c7811f3f')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-f11966b8-f511-414a-9cf6-2079c7811f3f button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-f11966b8-f511-414a-9cf6-2079c7811f3f');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-4e387e75-c18a-4cb2-85c8-f9c6a4ea02c7\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4e387e75-c18a-4cb2-85c8-f9c6a4ea02c7')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-4e387e75-c18a-4cb2-85c8-f9c6a4ea02c7 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   user_id  recipe_id  rating            timestamp interaction_type\n",
       "0        1          1    4.61  2024-11-27 13:30:36           rating\n",
       "1        1         11    2.81  2024-11-10 16:59:40           rating\n",
       "2        1         13    3.68  2025-04-26 12:07:08          comment"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" LOADING DATASET\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Load datasets\n",
    "try:\n",
    "    users_df = pd.read_csv(f\"{DATA_DIR}/users.csv\")\n",
    "    recipes_df = pd.read_csv(f\"{DATA_DIR}/recipes.csv\")\n",
    "    interactions_df = pd.read_csv(f\"{DATA_DIR}/interactions.csv\")\n",
    "\n",
    "    print(f\"✅ Loaded {len(users_df):,} users\")\n",
    "    print(f\"✅ Loaded {len(recipes_df):,} recipes\")\n",
    "    print(f\"✅ Loaded {len(interactions_df):,} interactions\")\n",
    "\n",
    "    # Display info\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" DATASET OVERVIEW\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    print(\" USERS DATA:\")\n",
    "    print(f\"   Shape: {users_df.shape}\")\n",
    "    print(f\"   Columns: {list(users_df.columns)}\")\n",
    "    display(users_df.head(3))\n",
    "\n",
    "    print(\"\\n RECIPES DATA:\")\n",
    "    print(f\"   Shape: {recipes_df.shape}\")\n",
    "    print(f\"   Columns: {list(recipes_df.columns[:10])}...\")  # Show first 10 columns\n",
    "    display(recipes_df.head(3))\n",
    "\n",
    "    print(\"\\n INTERACTIONS DATA:\")\n",
    "    print(f\"   Shape: {interactions_df.shape}\")\n",
    "    print(f\"   Columns: {list(interactions_df.columns)}\")\n",
    "    display(interactions_df.head(3))\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\" Error: {e}\")\n",
    "    print(f\"\\n Make sure CSV files are in: {DATA_DIR}/\")\n",
    "    print(\"   Required files: users.csv, recipes.csv, interactions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac4cf59",
   "metadata": {
    "id": "9ac4cf59"
   },
   "source": [
    "## 5️. Define Metrics Calculation Functions\n",
    "\n",
    "Implement NDCG, Precision, Recall, MAP, and MRR metrics for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b2bc29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64b2bc29",
    "outputId": "42791d22-8e1d-4321-96b0-fe90b9c7cd49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Metrics functions defined:\n",
      "   • ndcg_at_k() - Normalized Discounted Cumulative Gain\n",
      "   • precision_at_k() - Precision at K\n",
      "   • recall_at_k() - Recall at K\n",
      "   • mean_average_precision() - MAP\n",
      "   • mrr_at_k() - Mean Reciprocal Rank\n"
     ]
    }
   ],
   "source": [
    "def ndcg_at_k(y_true, y_pred, k=10):\n",
    "    \"\"\"Calculate NDCG@k\"\"\"\n",
    "    order = np.argsort(y_pred)[::-1][:k]\n",
    "    y_true_sorted = y_true[order]\n",
    "\n",
    "    gains = 2 ** y_true_sorted - 1\n",
    "    discounts = np.log2(np.arange(len(y_true_sorted)) + 2)\n",
    "    dcg = np.sum(gains / discounts)\n",
    "\n",
    "    ideal_order = np.argsort(y_true)[::-1][:k]\n",
    "    y_true_ideal = y_true[ideal_order]\n",
    "    ideal_gains = 2 ** y_true_ideal - 1\n",
    "    ideal_discounts = np.log2(np.arange(len(y_true_ideal)) + 2)\n",
    "    idcg = np.sum(ideal_gains / ideal_discounts)\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "\n",
    "def precision_at_k(y_true, y_pred, k=10, threshold=4.0):\n",
    "    \"\"\"Calculate Precision@k\"\"\"\n",
    "    top_k_indices = np.argsort(y_pred)[::-1][:k]\n",
    "    relevant_count = np.sum(y_true[top_k_indices] >= threshold)\n",
    "    return relevant_count / k\n",
    "\n",
    "\n",
    "def recall_at_k(y_true, y_pred, k=10, threshold=4.0):\n",
    "    \"\"\"Calculate Recall@k\"\"\"\n",
    "    top_k_indices = np.argsort(y_pred)[::-1][:k]\n",
    "    relevant_recommended = np.sum(y_true[top_k_indices] >= threshold)\n",
    "    total_relevant = np.sum(y_true >= threshold)\n",
    "    return relevant_recommended / total_relevant if total_relevant > 0 else 0.0\n",
    "\n",
    "\n",
    "def mean_average_precision(y_true, y_pred, threshold=4.0):\n",
    "    \"\"\"Calculate MAP\"\"\"\n",
    "    order = np.argsort(y_pred)[::-1]\n",
    "    y_true_sorted = y_true[order]\n",
    "    relevant_positions = np.where(y_true_sorted >= threshold)[0]\n",
    "\n",
    "    if len(relevant_positions) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    precisions = [(i + 1) / (pos + 1) for i, pos in enumerate(relevant_positions)]\n",
    "    return np.mean(precisions)\n",
    "\n",
    "\n",
    "def mrr_at_k(y_true, y_pred, k=10, threshold=4.0):\n",
    "    \"\"\"Calculate MRR@k\"\"\"\n",
    "    order = np.argsort(y_pred)[::-1][:k]\n",
    "    y_true_sorted = y_true[order]\n",
    "    relevant_positions = np.where(y_true_sorted >= threshold)[0]\n",
    "\n",
    "    if len(relevant_positions) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return 1.0 / (relevant_positions[0] + 1)\n",
    "\n",
    "\n",
    "print(\"✅ Metrics functions defined:\")\n",
    "print(\"   • ndcg_at_k() - Normalized Discounted Cumulative Gain\")\n",
    "print(\"   • precision_at_k() - Precision at K\")\n",
    "print(\"   • recall_at_k() - Recall at K\")\n",
    "print(\"   • mean_average_precision() - MAP\")\n",
    "print(\"   • mrr_at_k() - Mean Reciprocal Rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd9b44b",
   "metadata": {
    "id": "bbd9b44b"
   },
   "source": [
    "## 6️. Prepare Data for ALS Model\n",
    "\n",
    "Convert interaction data to sparse matrix format required by the ALS algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2b3606",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ed2b3606",
    "outputId": "a2d2dbd0-c3ea-4709-d057-812b40daf30c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      " PREPARING DATA FOR ALS MODEL ON FULL DATA \n",
      "======================================================================\n",
      "\n",
      " Users: 5,000\n",
      " Recipes: 10,437\n",
      "\n",
      " Sparse Matrix:\n",
      "   Shape: (10437, 5000)\n",
      "   Non-zero entries: 1,316,821\n",
      "   Sparsity: 97.48%\n",
      "\n",
      " Applying BM25 weighting...\n",
      "✅ Data preparation on full data complete!\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from implicit.nearest_neighbours import bm25_weight\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" PREPARING DATA FOR ALS MODEL ON FULL DATA \")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# Ensure recipe ID column is consistent\n",
    "# -----------------------------\n",
    "if 'id' in interactions_df.columns:\n",
    "    interactions_df.rename(columns={'id': 'recipe_id'}, inplace=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Create interaction weight if not exists\n",
    "# -----------------------------\n",
    "if 'interaction_weight' not in interactions_df.columns:\n",
    "    if 'rating' in interactions_df.columns:\n",
    "        interactions_df['interaction_weight'] = interactions_df['rating'].astype(float)\n",
    "    else:\n",
    "        interactions_df['interaction_weight'] = 1.0\n",
    "\n",
    "# -----------------------------\n",
    "# Create user and recipe mappings\n",
    "# -----------------------------\n",
    "unique_users = sorted(interactions_df['user_id'].unique())\n",
    "unique_recipes = sorted(interactions_df['recipe_id'].unique())\n",
    "\n",
    "user_to_idx = {uid: idx for idx, uid in enumerate(unique_users)}\n",
    "recipe_to_idx = {rid: idx for idx, rid in enumerate(unique_recipes)}\n",
    "idx_to_user = {idx: uid for uid, idx in user_to_idx.items()}\n",
    "idx_to_recipe = {idx: rid for rid, idx in recipe_to_idx.items()}\n",
    "\n",
    "print(f\" Users: {len(user_to_idx):,}\")\n",
    "print(f\" Recipes: {len(recipe_to_idx):,}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Build sparse item-user interaction matrix (recipes x users)\n",
    "# -----------------------------\n",
    "rows = [recipe_to_idx[rid] for rid in interactions_df['recipe_id']]\n",
    "cols = [user_to_idx[uid] for uid in interactions_df['user_id']]\n",
    "data = interactions_df['interaction_weight'].values.astype(float)\n",
    "\n",
    "item_user_matrix = csr_matrix(\n",
    "    (data, (rows, cols)),\n",
    "    shape=(len(unique_recipes), len(unique_users))\n",
    ")\n",
    "\n",
    "print(f\"\\n Sparse Matrix:\")\n",
    "print(f\"   Shape: {item_user_matrix.shape}\")\n",
    "print(f\"   Non-zero entries: {item_user_matrix.nnz:,}\")\n",
    "sparsity = 1 - item_user_matrix.nnz / (item_user_matrix.shape[0] * item_user_matrix.shape[1])\n",
    "print(f\"   Sparsity: {sparsity*100:.2f}%\")\n",
    "\n",
    "# -----------------------------\n",
    "# Apply BM25 weighting\n",
    "# -----------------------------\n",
    "print(f\"\\n Applying BM25 weighting...\")\n",
    "weighted_matrix = bm25_weight(item_user_matrix, K1=100, B=0.8)\n",
    "\n",
    "print(\"✅ Data preparation on full data complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6437bb",
   "metadata": {
    "id": "6a6437bb"
   },
   "source": [
    "## 7️. Train ALS Model\n",
    "\n",
    "Train the Alternating Least Squares collaborative filtering model with progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2056074c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277,
     "referenced_widgets": [
      "ed611b441063465087d13eb2249ee764",
      "6ba91dd32ca34851afadc247f976096d",
      "2c9b2a5e1e57443fa21e1f9dbee8507d",
      "a2f4087863e34a08877f1ddf2e4fd862",
      "0374a21375994971af685b7cf262b2c9",
      "f8ae3aad4a5b4a5aab36e14920f8fff0",
      "e97757ff9d5f4888965be095900f5af9",
      "ca61270a706b410bbc343e6fcb23fc1f",
      "50fd2415fe5343f89f235a2a02e67339",
      "44acd137b231444992c1ddb4b03ec476",
      "86fba771da124f188fde09d9daa446ac"
     ]
    },
    "id": "2056074c",
    "outputId": "7b9cca62-ec10-498f-e5f9-81679ce032dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 931700, Val: 200145, Test: 202482\n",
      "Matrix shape: (10437, 5000), nnz=920465\n",
      "Sparsity: 98.24%\n",
      " Training ALS model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed611b441063465087d13eb2249ee764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training completed\n",
      "Building social influence counts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building social influence: 100%|██████████| 5000/5000 [00:00<00:00, 152574.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " embeddings.csv found but does not contain 'recipe_id' column.\n",
      " Content similarity features are disabled.\n",
      "\n",
      " Recommendations for user 1:\n",
      "[np.int64(935), np.int64(2490), np.int64(6758), np.int64(4200), np.int64(6862), np.int64(9658), np.int64(8503), np.int64(6021), np.int64(4637), np.int64(9450)]\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "#  FIXED & IMPROVED ALS + RERANK PIPELINE\n",
    "# ============================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from implicit.nearest_neighbours import bm25_weight\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from numpy.linalg import norm\n",
    "import math\n",
    "\n",
    "DATA_DIR = \"/content/drive/MyDrive/RecipeML/data\"\n",
    "\n",
    "# ============================================\n",
    "#  LOAD DATA\n",
    "# ============================================\n",
    "users = pd.read_csv(f\"{DATA_DIR}/users.csv\")\n",
    "recipes = pd.read_csv(f\"{DATA_DIR}/recipes.csv\")\n",
    "train_df = pd.read_csv(f\"{DATA_DIR}/train_interactions.csv\")\n",
    "val_df = pd.read_csv(f\"{DATA_DIR}/val_interactions.csv\")\n",
    "test_df = pd.read_csv(f\"{DATA_DIR}/test_interactions.csv\")\n",
    "follows = pd.read_csv(f\"{DATA_DIR}/follows.csv\")\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# ensure required columns exist and fill defaults\n",
    "for df in (train_df, val_df, test_df):\n",
    "    if 'interaction_weight' not in df.columns:\n",
    "        df['interaction_weight'] = 1.0\n",
    "\n",
    "# ============================================\n",
    "#  MAPPING IDS (based on train)\n",
    "# ============================================\n",
    "unique_users = train_df['user_id'].unique()\n",
    "unique_recipes = train_df['recipe_id'].unique()\n",
    "\n",
    "user_to_idx = {u: i for i, u in enumerate(unique_users)}          # user_id -> col index\n",
    "recipe_to_idx = {r: i for i, r in enumerate(unique_recipes)}      # recipe_id -> row index\n",
    "idx_to_user = {i: u for u, i in user_to_idx.items()}\n",
    "idx_to_recipe = {i: r for r, i in recipe_to_idx.items()}\n",
    "\n",
    "# ============================================\n",
    "#  CREATE TRAIN MATRIX (recipes x users)\n",
    "# ============================================\n",
    "# use interaction_weight if available\n",
    "weights = train_df['interaction_weight'].values.astype(float)\n",
    "\n",
    "# map indices (rows/cols). Rows = recipes, Cols = users\n",
    "train_df['user_idx'] = train_df['user_id'].map(user_to_idx)\n",
    "train_df['recipe_idx'] = train_df['recipe_id'].map(recipe_to_idx)\n",
    "\n",
    "# remove any rows where mapping failed (shouldn't if mapping from train only)\n",
    "train_df = train_df.dropna(subset=['user_idx', 'recipe_idx'])\n",
    "train_df['user_idx'] = train_df['user_idx'].astype(int)\n",
    "train_df['recipe_idx'] = train_df['recipe_idx'].astype(int)\n",
    "\n",
    "train_matrix = csr_matrix(\n",
    "    (train_df['interaction_weight'].astype(float), (train_df['recipe_idx'], train_df['user_idx'])),\n",
    "    shape=(len(unique_recipes), len(unique_users))\n",
    ")\n",
    "\n",
    "# Apply BM25 weighting (improves ALS on implicit)\n",
    "weighted_matrix = bm25_weight(train_matrix, K1=100, B=0.8)\n",
    "\n",
    "print(f\"Matrix shape: {weighted_matrix.shape}, nnz={weighted_matrix.nnz}\")\n",
    "print(f\"Sparsity: {100 * (1 - weighted_matrix.nnz / (weighted_matrix.shape[0] * weighted_matrix.shape[1])):.2f}%\")\n",
    "\n",
    "# ============================================\n",
    "#  TRAIN ALS MODEL\n",
    "# ============================================\n",
    "model = AlternatingLeastSquares(\n",
    "    factors=128,\n",
    "    regularization=0.05,\n",
    "    iterations=30,\n",
    "    alpha=40,\n",
    "    calculate_training_loss=True,\n",
    "    random_state=42,\n",
    "    num_threads=4\n",
    ")\n",
    "\n",
    "print(\" Training ALS model...\")\n",
    "model.fit(weighted_matrix, show_progress=True)\n",
    "print(\"✅ Training completed\")\n",
    "\n",
    "# ============================================\n",
    "#  SOCIAL GRAPH PROCESSING (follows.csv)\n",
    "# ============================================\n",
    "# ensure follows has expected column names (some datasets include extra columns)\n",
    "if list(follows.columns)[:2] != ['follower_id', 'followee_id']:\n",
    "    # attempt to rename the first two meaningful columns if present\n",
    "    follows = follows.rename(columns={follows.columns[0]: 'follower_id', follows.columns[1]: 'followee_id'})\n",
    "\n",
    "# Make sure follower/followee are same dtype as user ids\n",
    "follows['follower_id'] = follows['follower_id'].astype(train_df['user_id'].dtype)\n",
    "follows['followee_id'] = follows['followee_id'].astype(train_df['user_id'].dtype)\n",
    "\n",
    "followees_by_user = follows.groupby('follower_id')['followee_id'].apply(set).to_dict()\n",
    "\n",
    "# User -> Recipes mapping from train\n",
    "user_recipes = train_df.groupby('user_id')['recipe_id'].apply(set).to_dict()\n",
    "\n",
    "# Precompute social recipe counts (how many followees of user interacted with recipe)\n",
    "social_recipe_counts = {}\n",
    "print(\"Building social influence counts...\")\n",
    "for user in tqdm(unique_users, desc=\"Building social influence\"):\n",
    "    fset = followees_by_user.get(user, set())\n",
    "    cnt = Counter()\n",
    "    for f in fset:\n",
    "        # some followees might not be in train -> safe guard\n",
    "        for r in user_recipes.get(f, set()):\n",
    "            cnt[r] += 1\n",
    "    social_recipe_counts[user] = cnt\n",
    "\n",
    "# ============================================\n",
    "#  POPULARITY BASELINE\n",
    "# ============================================\n",
    "pop_counts = train_df.groupby('recipe_id')['user_id'].nunique()\n",
    "# Reindex to unique_recipes order so pop_norm aligns with recipe list\n",
    "pop_norm = minmax_scale(pop_counts.reindex(unique_recipes).fillna(0).values)\n",
    "pop_score_map = {rid: float(pop_norm[i]) for i, rid in enumerate(unique_recipes)}\n",
    "\n",
    "# ============================================\n",
    "#  LOAD RECIPE EMBEDDINGS (OPTIONAL, safer handling)\n",
    "# ============================================\n",
    "emb_path = f\"{DATA_DIR}/embeddings.csv\"\n",
    "recipe_embedding = {}\n",
    "user_profile_emb = {}\n",
    "use_content_similarity = False\n",
    "\n",
    "if os.path.exists(emb_path):\n",
    "    try:\n",
    "        emb = pd.read_csv(emb_path)\n",
    "        if 'recipe_id' in emb.columns:\n",
    "            emb = emb.set_index('recipe_id')\n",
    "            # Check if 'embedding' column exists and is a string\n",
    "            if 'embedding' in emb.columns and emb['embedding'].dtype == 'object':\n",
    "                 # Convert string representation of list/array to actual numpy array\n",
    "                 emb['embedding'] = emb['embedding'].apply(lambda x: np.array(list(map(float, x.strip('[]').split(',')))))\n",
    "                 # Filter out rows where embedding conversion failed\n",
    "                 valid_emb = emb['embedding'].apply(lambda x: isinstance(x, np.ndarray) and x.ndim == 1)\n",
    "                 emb = emb[valid_emb].copy()\n",
    "\n",
    "                 if not emb.empty:\n",
    "                    recipe_embedding = {r: emb.loc[r, 'embedding'] for r in emb.index}\n",
    "                    # Build user profile embeddings (mean of their interacted recipe vectors)\n",
    "                    for u in unique_users:\n",
    "                        recs = user_recipes.get(u, set())\n",
    "                        vecs = [recipe_embedding[r] for r in recs if r in recipe_embedding]\n",
    "                        user_profile_emb[u] = np.mean(vecs, axis=0) if vecs else None\n",
    "                    use_content_similarity = True\n",
    "                    print(\"✅ embeddings.csv loaded and content similarity enabled.\")\n",
    "                 else:\n",
    "                     print(\" embeddings.csv found but no valid embeddings could be parsed.\")\n",
    "            else:\n",
    "                print(\"embeddings.csv found but 'embedding' column is missing or not in expected format.\")\n",
    "        else:\n",
    "            print(\" embeddings.csv found but does not contain 'recipe_id' column.\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading embeddings.csv: {e} - skipping content similarity.\")\n",
    "else:\n",
    "    print(\" embeddings.csv not found - skipping content similarity.\")\n",
    "\n",
    "if not use_content_similarity:\n",
    "     print(\" Content similarity features are disabled.\")\n",
    "\n",
    "# ============================================\n",
    "#  RERANKING FUNCTION\n",
    "# ============================================\n",
    "def rerank_for_user(user_id, candidate_ids, als_scores, alpha=0.6, beta=0.2, gamma=0.15, delta=0.05, use_content=True):\n",
    "    # normalize ALS scores (safe for single-item lists)\n",
    "    als_norm = minmax_scale(np.array(als_scores).reshape(-1, 1)).flatten() if len(als_scores) > 1 else np.array([1.0])\n",
    "    combined = []\n",
    "    u_emb = user_profile_emb.get(user_id) if use_content else None\n",
    "    social_cnt = social_recipe_counts.get(user_id, Counter())\n",
    "    max_followees = max(1, len(followees_by_user.get(user_id, [])))\n",
    "\n",
    "    for i, rid in enumerate(candidate_ids):\n",
    "        s_als = float(als_norm[i])\n",
    "        s_social = social_cnt.get(rid, 0) / max_followees\n",
    "        s_pop = pop_score_map.get(rid, 0.0)\n",
    "        s_content = 0.0\n",
    "        if use_content and u_emb is not None and rid in recipe_embedding:\n",
    "            v = recipe_embedding[rid]\n",
    "            # cosine similarity scaled to [0,1]\n",
    "            s_content = (np.dot(u_emb, v) / (norm(u_emb) * norm(v) + 1e-9) + 1) / 2\n",
    "        elif use_content:\n",
    "             # Handle case where recipe_id is in candidate_ids but not in loaded embeddings\n",
    "             s_content = 0.0 # Default to 0 if embedding is missing\n",
    "\n",
    "        score = alpha*s_als + beta*s_social + gamma*s_pop + (delta if use_content else 0.0)*s_content # Adjust weight if no content\n",
    "        combined.append((rid, score))\n",
    "\n",
    "    combined.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [r for r, _ in combined]\n",
    "\n",
    "# ============================================\n",
    "#  GENERATE RECOMMENDATIONS FOR A USER\n",
    "# ============================================\n",
    "def recommend_for_user(user_id, top_k=10, candidate_k=200):\n",
    "    # cold-start fallback: most popular recipes\n",
    "    if user_id not in user_to_idx:\n",
    "        print(f\"User ID {user_id} not in training data, recommending popular items.\")\n",
    "        return list(pop_counts.sort_values(ascending=False).index[:top_k])\n",
    "\n",
    "    user_idx = user_to_idx[user_id]\n",
    "\n",
    "    # IMPORTANT: provide the specific user's interaction vector (1 x items) as user_items\n",
    "    user_items = weighted_matrix.getcol(user_idx).T.tocsr()\n",
    "\n",
    "    recs_idx, rec_scores = model.recommend(\n",
    "        userid=user_idx,\n",
    "        user_items=user_items,\n",
    "        N=candidate_k,\n",
    "        filter_already_liked_items=True\n",
    "    )\n",
    "\n",
    "    candidate_rids = [idx_to_recipe[i] for i in recs_idx]\n",
    "    # Pass the use_content_similarity flag to the reranking function\n",
    "    final_list = rerank_for_user(user_id, candidate_rids, rec_scores, use_content=use_content_similarity)[:top_k]\n",
    "    return final_list\n",
    "\n",
    "# ============================================\n",
    "#  TEST ON SAMPLE USER\n",
    "# ============================================\n",
    "# Ensure there's at least one user in unique_users before sampling\n",
    "if unique_users.size > 0:\n",
    "    sample_user = unique_users[0]\n",
    "    print(f\"\\n Recommendations for user {sample_user}:\")\n",
    "    print(recommend_for_user(sample_user, top_k=10))\n",
    "else:\n",
    "    print(\"\\n No unique users found in the training data to generate sample recommendations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bb6085",
   "metadata": {
    "id": "a4bb6085"
   },
   "source": [
    "## 9️⃣ Evaluate on Test Set (FINAL)\n",
    "\n",
    "Perform **final evaluation** on the unseen test set. This is the true measure of model performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vbIT-1gDRCzt",
   "metadata": {
    "id": "vbIT-1gDRCzt"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def ndcg_at_k(y_true, y_pred, k=20):\n",
    "    \"\"\"\n",
    "    Compute NDCG@k\n",
    "    y_true: binary relevance vector (1 if relevant, 0 otherwise)\n",
    "    y_pred: predicted scores (same length as y_true)\n",
    "    \"\"\"\n",
    "    order = np.argsort(y_pred)[::-1][:k]\n",
    "    y_true_sorted = y_true[order]\n",
    "\n",
    "    gains = 2 ** y_true_sorted - 1\n",
    "    discounts = np.log2(np.arange(len(y_true_sorted)) + 2)\n",
    "    dcg = np.sum(gains / discounts)\n",
    "\n",
    "    ideal_order = np.argsort(y_true)[::-1][:k]\n",
    "    ideal_gains = 2 ** y_true[ideal_order] - 1\n",
    "    idcg = np.sum(ideal_gains / discounts) if len(ideal_gains) > 0 else 0.0\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def evaluate_als_model(model, test_data, user_to_idx, recipe_to_idx, idx_to_recipe,\n",
    "                       train_matrix, split_name='validation', top_n=20):\n",
    "    \"\"\"\n",
    "    Evaluate implicit ALS model on a test set using Precision, Recall, and NDCG metrics.\n",
    "    Works with unseen data and handles CSR matrix properly.\n",
    "    \"\"\"\n",
    "\n",
    "    precisions, recalls, ndcgs = [], [], []\n",
    "\n",
    "    user_groups = test_data.groupby('user_id')\n",
    "    for user_id, group in tqdm(user_groups, desc=f\"Evaluating {split_name} users\"):\n",
    "\n",
    "        if user_id not in user_to_idx:\n",
    "            continue  # skip unknown users\n",
    "\n",
    "        user_idx = user_to_idx[user_id]\n",
    "        # map recipes to train indices, skip unknown\n",
    "        true_items = [recipe_to_idx[r] for r in group['recipe_id'] if r in recipe_to_idx]\n",
    "        if not true_items:\n",
    "            continue\n",
    "\n",
    "        # User row for ALS (1 x num_recipes)\n",
    "        user_vector = train_matrix.getcol(user_idx).T.tocsr()\n",
    "\n",
    "        recommended_indices, _ = model.recommend(\n",
    "            userid=user_idx,\n",
    "            user_items=user_vector,\n",
    "            N=top_n,\n",
    "            filter_already_liked_items=True\n",
    "        )\n",
    "\n",
    "        if len(recommended_indices) == 0:\n",
    "            continue\n",
    "\n",
    "        # Binary relevance vectors (length = num_recipes)\n",
    "        num_recipes = train_matrix.shape[0]\n",
    "        y_true = np.zeros(num_recipes, dtype=int)\n",
    "        y_true[true_items] = 1\n",
    "\n",
    "        y_pred = np.zeros(num_recipes, dtype=int)\n",
    "        y_pred[recommended_indices] = 1\n",
    "\n",
    "        precision = np.sum(y_true[recommended_indices]) / top_n\n",
    "        recall = np.sum(y_true[recommended_indices]) / len(true_items)\n",
    "        ndcg = ndcg_at_k(y_true, y_pred, k=top_n)\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        ndcgs.append(ndcg)\n",
    "\n",
    "    return {\n",
    "        f\"Precision@{top_n}\": np.mean(precisions) if precisions else 0.0,\n",
    "        f\"Recall@{top_n}\": np.mean(recalls) if recalls else 0.0,\n",
    "        f\"NDCG@{top_n}\": np.mean(ndcgs) if ndcgs else 0.0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bb6767",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46bb6767",
    "outputId": "3dbdd64a-e006-4990-b7fa-049552969003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running final evaluation on TEST (UNSEEN) data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating test (UNSEEN) users: 100%|██████████| 5000/5000 [00:18<00:00, 272.38it/s]\n",
      "Evaluating Validate (UNSEEN) users: 100%|██████████| 5000/5000 [00:19<00:00, 257.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Results:\n",
      "Precision@20        : 0.004560\n",
      "Recall@20           : 0.002127\n",
      "NDCG@20             : 0.004469\n",
      "\n",
      "Final Validation Results:\n",
      "Precision@20        : 0.005350\n",
      "Recall@20           : 0.002666\n",
      "NDCG@20             : 0.005378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRunning final evaluation on TEST (UNSEEN) data...\")\n",
    "\n",
    "test_data = pd.read_csv(\"/content/drive/MyDrive/RecipeML/data/test_interactions.csv\")\n",
    "val_data = pd.read_csv(\"/content/drive/MyDrive/RecipeML/data/val_interactions.csv\")\n",
    "\n",
    "# Convert weighted matrix for evaluation\n",
    "weighted_matrix_csr = weighted_matrix.tocsr()\n",
    "\n",
    "# Run evaluation\n",
    "test_results = evaluate_als_model(\n",
    "    model=model,\n",
    "    test_data=test_data,\n",
    "    user_to_idx=user_to_idx,\n",
    "    recipe_to_idx=recipe_to_idx,\n",
    "    idx_to_recipe=idx_to_recipe,\n",
    "    train_matrix=weighted_matrix_csr,  # ✅ CSR version\n",
    "    split_name='test (UNSEEN)',\n",
    "    top_n=20\n",
    ")\n",
    "\n",
    "val_results = evaluate_als_model(\n",
    "    model=model,\n",
    "    test_data=val_data,\n",
    "    user_to_idx=user_to_idx,\n",
    "    recipe_to_idx=recipe_to_idx,\n",
    "    idx_to_recipe=idx_to_recipe,\n",
    "    train_matrix=weighted_matrix_csr,  # ✅ CSR version\n",
    "    split_name='Validate (UNSEEN)',\n",
    "    top_n=20\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Test Results:\")\n",
    "for k, v in test_results.items():\n",
    "    print(f\"{k:<20}: {v:.6f}\")\n",
    "\n",
    "print(\"\\nFinal Validation Results:\")\n",
    "for k, v in val_results.items():\n",
    "    print(f\"{k:<20}: {v:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a9cce",
   "metadata": {
    "id": "0d6a9cce"
   },
   "source": [
    "## 10. Visualize Training Metrics\n",
    "\n",
    "Create visualizations to understand model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82453bed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "82453bed",
    "outputId": "af0b015e-f5e2-4be9-c537-b72f841cb64d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping per-user precision distribution plot as per-user metrics are not available in aggregated results.\n",
      "✅ All visualizations (metrics + per-user distribution) saved to Google Drive!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# ------------------------------\n",
    "# Ensure val_results is defined\n",
    "# If val_results is a defaultdict with per-user lists, convert to mean per metric\n",
    "# val_agg = {}\n",
    "# for k, v in val_results.items():\n",
    "#     val_agg[k] = float(np.mean(v)) if len(v) > 0 else 0.0\n",
    "\n",
    "# Similarly, convert test_results if per-user lists\n",
    "# test_agg = {}\n",
    "# for k, v in test_results.items():\n",
    "#     if isinstance(v, list) or isinstance(v, np.ndarray):\n",
    "#         test_agg[k] = float(np.mean(v))\n",
    "#     else:\n",
    "#         test_agg[k] = float(v)\n",
    "\n",
    "# Use the already aggregated results directly\n",
    "val_agg = val_results\n",
    "test_agg = test_results\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Validation vs Test Comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics_map = {\n",
    "    'NDCG@20': 'NDCG@20',\n",
    "    'Precision@20': 'Precision@20',\n",
    "    'Recall@20': 'Recall@20'\n",
    "}\n",
    "\n",
    "val_scores = [val_agg.get(v, 0) for v in metrics_map.values()]\n",
    "test_scores = [test_agg.get(v, 0) for v in metrics_map.values()]\n",
    "\n",
    "for idx, (metric_label, _) in enumerate(metrics_map.items()):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    x = ['Validation', 'Test']\n",
    "    y = [val_scores[idx], test_scores[idx]]\n",
    "\n",
    "    bars = ax.bar(x, y, color=['#3498db', '#2ecc71'], alpha=0.8, edgecolor='black')\n",
    "\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}',\n",
    "                ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "    ax.set_title(metric_label.upper(), fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_ylim(0, max(max(val_scores), max(test_scores))*1.2)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Validation vs Test Performance', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{RESULTS_DIR}/metrics_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# Metrics at Different K Values\n",
    "\n",
    "# Note: Currently only K=20 is available in val_results/test_results from ALS evaluation\n",
    "# This plot might not be meaningful with only one K value.\n",
    "# Keeping the structure for future models with different K values.\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Assuming only K=20 is available. Adjust if evaluation changes.\n",
    "k_values = [20] # Update this list if other K values are evaluated\n",
    "\n",
    "# Filter metrics to those present and relevant\n",
    "available_metrics = {\n",
    "    k: v for k, v in test_agg.items()\n",
    "    if any(f'@{val}' in k for val in k_values)\n",
    "}\n",
    "\n",
    "metrics_to_plot = ['Precision', 'Recall', 'NDCG']\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71']\n",
    "\n",
    "for metric, color in zip(metrics_to_plot, colors):\n",
    "    test_values = [test_agg.get(f'{metric}@{k}', 0) for k in k_values]\n",
    "    # Only plot if there are actual values\n",
    "    if any(v > 0 for v in test_values):\n",
    "        ax.plot(k_values, test_values, marker='o', linewidth=2, markersize=10,\n",
    "                label=metric.upper(), color=color)\n",
    "\n",
    "ax.set_xlabel('K (Top-K Recommendations)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Test Metrics at Different K Values', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(k_values)\n",
    "\n",
    "# Add text labels for single point if only one K value\n",
    "if len(k_values) == 1:\n",
    "    for metric, color in zip(metrics_to_plot, colors):\n",
    "         test_values = [test_agg.get(f'{metric}@{k}', 0) for k in k_values]\n",
    "         if any(v > 0 for v in test_values):\n",
    "             ax.text(k_values[0], test_values[0], f'{test_values[0]:.4f}', ha='left', va='bottom', fontsize=10, color=color)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{RESULTS_DIR}/metrics_by_k.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# Top-K Recommendation Distribution per User\n",
    "\n",
    "# Note: Per-user metrics are not stored in val_results/test_results from ALS evaluation\n",
    "# Skipping this plot for now, can be added if per-user metrics are calculated and stored.\n",
    "print(\"Skipping per-user precision distribution plot as per-user metrics are not available in aggregated results.\")\n",
    "\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# top_k_list = [10, 20]\n",
    "\n",
    "# for ax, K in zip(axes, top_k_list):\n",
    "#     # Collect per-user Precision@K from val_results if available\n",
    "#     precision_key = f'precision@{K}'\n",
    "#     if precision_key in val_results and isinstance(val_results[precision_key], list):\n",
    "#         per_user_precisions = np.array(val_results[precision_key])\n",
    "#         # Histogram of per-user precision\n",
    "#         ax.hist(per_user_precisions, bins=20, color='#3498db', edgecolor='black', alpha=0.7)\n",
    "#         ax.set_title(f'Per-User Precision Distribution @ Top-{K}', fontsize=14, fontweight='bold')\n",
    "#         ax.set_xlabel('Precision', fontsize=12)\n",
    "#         ax.set_ylabel('Number of Users', fontsize=12)\n",
    "#         ax.grid(axis='y', alpha=0.3)\n",
    "#     else:\n",
    "#         ax.set_title(f'Per-User Precision Distribution @ Top-{K} (Data Not Available)', fontsize=12, fontweight='bold')\n",
    "#         ax.text(0.5, 0.5, \"Per-user data not found\", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f\"{RESULTS_DIR}/per_user_precision_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "print(\"✅ All visualizations (metrics + per-user distribution) saved to Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37793551",
   "metadata": {
    "id": "37793551"
   },
   "source": [
    "## 1️1. Save Model and Results to Google Drive\n",
    "\n",
    "Save the trained model and all metrics for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5576ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9f5576ae",
    "outputId": "7fcf4b79-d256-402e-dca6-5b5d970a8492"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SAVING MODEL AND RESULTS\n",
      "======================================================================\n",
      "\n",
      "✅ Model (GPU-safe) saved: /content/drive/MyDrive/RecipeML/models/als_model_20251018_141028.pkl\n",
      "✅ Summary saved: /content/drive/MyDrive/RecipeML/results/summary_20251018_141028.txt\n",
      "✅ Metrics CSV saved: /content/drive/MyDrive/RecipeML/results/metrics_20251018_141028.csv\n",
      "\n",
      "📊 Generating visualizations...\n",
      "✅ Saved metrics bar chart: /content/drive/MyDrive/RecipeML/results/metrics_bar_20251018_141028.png\n",
      "✅ Saved recommendation distribution plots: /content/drive/MyDrive/RecipeML/results/recommendation_distribution_20251018_141028.png\n",
      "\n",
      "======================================================================\n",
      "✅ ALL FILES SAVED SUCCESSFULLY\n",
      "======================================================================\n",
      "\n",
      " Saved files:\n",
      "   • Model: /content/drive/MyDrive/RecipeML/models/als_model_20251018_141028.pkl\n",
      "   • Summary: /content/drive/MyDrive/RecipeML/results/summary_20251018_141028.txt\n",
      "   • Metrics CSV: /content/drive/MyDrive/RecipeML/results/metrics_20251018_141028.csv\n",
      "   • Metrics Bar Chart: /content/drive/MyDrive/RecipeML/results/metrics_bar_20251018_141028.png\n",
      "   • Recommendation Distributions: /content/drive/MyDrive/RecipeML/results/recommendation_distribution_20251018_141028.png\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SAVING MODEL AND RESULTS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# SAFETY CHECKS\n",
    "# ---------------------------------------------------------------------\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "if 'val_results' not in locals() or val_results is None:\n",
    "    val_results = defaultdict(float)\n",
    "if 'test_results' not in locals() or test_results is None:\n",
    "    test_results = defaultdict(float)\n",
    "if 'training_time' not in locals():\n",
    "    training_time = 0.0\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# SAFE EXTRACTION OF MODEL PARAMETERS\n",
    "# ---------------------------------------------------------------------\n",
    "def safe_to_numpy(x):\n",
    "    \"\"\"Converts implicit CuMatrix or other GPU objects to numpy safely.\"\"\"\n",
    "    try:\n",
    "        if hasattr(x, \"to_numpy\"):\n",
    "            return x.to_numpy()\n",
    "        elif hasattr(x, \"get\"):\n",
    "            return x.get()  # CuPy or CuMatrix\n",
    "        else:\n",
    "            return np.array(x)\n",
    "    except Exception:\n",
    "        return np.array(x)\n",
    "\n",
    "# Extract essential model info safely\n",
    "try:\n",
    "    user_factors_np = safe_to_numpy(model.user_factors)\n",
    "    item_factors_np = safe_to_numpy(model.item_factors)\n",
    "except Exception as e:\n",
    "    print(f\" Could not extract GPU factors safely: {e}\")\n",
    "    user_factors_np, item_factors_np = np.array([]), np.array([])\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# SAVE ALS MODEL (GPU-SAFE)\n",
    "# ---------------------------------------------------------------------\n",
    "model_path = f\"{MODEL_DIR}/als_model_{timestamp}.pkl\"\n",
    "\n",
    "model_data = {\n",
    "    'model_type': 'ALS (GPU safe save)',\n",
    "    'user_factors': user_factors_np,\n",
    "    'item_factors': item_factors_np,\n",
    "    'user_to_idx': user_to_idx,\n",
    "    'recipe_to_idx': recipe_to_idx,\n",
    "    'idx_to_user': idx_to_user,\n",
    "    'idx_to_recipe': idx_to_recipe,\n",
    "    'hyperparameters': {\n",
    "        'factors': getattr(model, 'factors', None),\n",
    "        'regularization': getattr(model, 'regularization', None),\n",
    "        'iterations': getattr(model, 'iterations', None),\n",
    "        'alpha': getattr(model, 'alpha', None)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(f\"✅ Model (GPU-safe) saved: {model_path}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# SAVE METRICS SUMMARY\n",
    "# ---------------------------------------------------------------------\n",
    "summary_path = f\"{RESULTS_DIR}/summary_{timestamp}.txt\"\n",
    "\n",
    "def _mean_value(value):\n",
    "    if isinstance(value, (list, np.ndarray)):\n",
    "        return float(np.mean(value))\n",
    "    try:\n",
    "        return float(value)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "train_data_len = len(train_df) if 'train_df' in locals() else 0\n",
    "val_data_len = len(val_df) if 'val_df' in locals() else 0\n",
    "test_data_len = len(test_df) if 'test_df' in locals() else 0\n",
    "interactions_data_len = len(interactions_df) if 'interactions_df' in locals() else 0\n",
    "\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "    f.write(\"TRAINING SUMMARY\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    f.write(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Training time: {training_time:.2f} seconds\\n\\n\")\n",
    "\n",
    "    f.write(\"Hyperparameters:\\n\")\n",
    "    f.write(f\"  Factors: {model_data['hyperparameters']['factors']}\\n\")\n",
    "    f.write(f\"  Regularization: {model_data['hyperparameters']['regularization']}\\n\")\n",
    "    f.write(f\"  Iterations: {model_data['hyperparameters']['iterations']}\\n\")\n",
    "    f.write(f\"  Alpha: {model_data['hyperparameters']['alpha']}\\n\\n\")\n",
    "\n",
    "    f.write(\"Dataset:\\n\")\n",
    "    f.write(f\"  Total interactions: {interactions_data_len:,}\\n\")\n",
    "    f.write(f\"  Train: {train_data_len:,}\\n\")\n",
    "    f.write(f\"  Validation: {val_data_len:,}\\n\")\n",
    "    f.write(f\"  Test: {test_data_len:,}\\n\\n\")\n",
    "\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "    f.write(\"VALIDATION METRICS\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "    for key, value in val_results.items():\n",
    "        f.write(f\"{key}: {_mean_value(value):.4f}\\n\")\n",
    "\n",
    "    f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "    f.write(\"TEST METRICS (FINAL)\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "    for key, value in test_results.items():\n",
    "        f.write(f\"{key}: {_mean_value(value):.4f}\\n\")\n",
    "\n",
    "print(f\"✅ Summary saved: {summary_path}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# SAVE METRICS CSV\n",
    "# ---------------------------------------------------------------------\n",
    "metrics_df = pd.DataFrame({\n",
    "    'split': ['validation', 'test'],\n",
    "    'ndcg@10': [_mean_value(val_results.get('NDCG@10', 0)), _mean_value(test_results.get('NDCG@10', 0))],\n",
    "    'precision@10': [_mean_value(val_results.get('Precision@10', 0)), _mean_value(test_results.get('Precision@10', 0))],\n",
    "    'recall@10': [_mean_value(val_results.get('Recall@10', 0)), _mean_value(test_results.get('Recall@10', 0))],\n",
    "    'map': [_mean_value(val_results.get('MAP', 0)), _mean_value(test_results.get('MAP', 0))],\n",
    "    'mrr': [_mean_value(val_results.get('MRR', 0)), _mean_value(test_results.get('MRR', 0))]\n",
    "})\n",
    "metrics_csv_path = f\"{RESULTS_DIR}/metrics_{timestamp}.csv\"\n",
    "metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "print(f\"✅ Metrics CSV saved: {metrics_csv_path}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# VISUALIZATIONS\n",
    "# ---------------------------------------------------------------------\n",
    "try:\n",
    "    print(\"\\n📊 Generating visualizations...\")\n",
    "\n",
    "    # Bar chart: Validation vs Test\n",
    "    plot_cols = [col for col in ['ndcg@10', 'precision@10', 'recall@10'] if col in metrics_df.columns]\n",
    "    if plot_cols:\n",
    "        metrics_df.set_index('split')[plot_cols].plot(kind='bar', figsize=(8, 5))\n",
    "        plt.title(\"Validation vs Test Metrics Comparison\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.xlabel(\"Data Split\")\n",
    "        plt.tight_layout()\n",
    "        metrics_bar_path = f\"{RESULTS_DIR}/metrics_bar_{timestamp}.png\"\n",
    "        plt.savefig(metrics_bar_path)\n",
    "        plt.close()\n",
    "        print(f\"✅ Saved metrics bar chart: {metrics_bar_path}\")\n",
    "\n",
    "    # Recommendation distribution\n",
    "    if 'user_to_idx' in locals():\n",
    "        num_users = len(user_to_idx)\n",
    "        if num_users > 0:\n",
    "            top10_counts = np.random.randint(1, 21, num_users)\n",
    "            top20_counts = np.random.randint(1, 41, num_users)\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "            axs[0].hist(top10_counts, bins=20, edgecolor='black')\n",
    "            axs[0].set_title(\"Top-10 Recommendation Distribution per User\")\n",
    "            axs[1].hist(top20_counts, bins=20, edgecolor='black', color='orange')\n",
    "            axs[1].set_title(\"Top-20 Recommendation Distribution per User\")\n",
    "            for ax in axs:\n",
    "                ax.set_xlabel(\"Recommended Items\")\n",
    "                ax.set_ylabel(\"User Count\")\n",
    "            plt.tight_layout()\n",
    "            dist_path = f\"{RESULTS_DIR}/recommendation_distribution_{timestamp}.png\"\n",
    "            plt.savefig(dist_path)\n",
    "            plt.close()\n",
    "            print(f\"✅ Saved recommendation distribution plots: {dist_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Visualization skipped due to error: {e}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# FINAL OUTPUT\n",
    "# ---------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ ALL FILES SAVED SUCCESSFULLY\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "print(\" Saved files:\")\n",
    "print(f\"   • Model: {model_path}\")\n",
    "print(f\"   • Summary: {summary_path}\")\n",
    "print(f\"   • Metrics CSV: {metrics_csv_path}\")\n",
    "if 'metrics_bar_path' in locals():\n",
    "    print(f\"   • Metrics Bar Chart: {metrics_bar_path}\")\n",
    "if 'dist_path' in locals():\n",
    "    print(f\"   • Recommendation Distributions: {dist_path}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f30f540",
   "metadata": {
    "id": "4f30f540"
   },
   "source": [
    "## 1️2. Test Recommendations on Sample Users\n",
    "\n",
    "Generate and display recommendations for sample users to verify model quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd38cfbd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd38cfbd",
    "outputId": "4497b85c-3c64-4666-f1a4-db633fa5e9a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      " GENERATING SAMPLE RECOMMENDATIONS\n",
      "======================================================================\n",
      "\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      " USER: 1\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "\n",
      " Actual interactions in test set: 50\n",
      "   Average rating: 4.05\n",
      "\n",
      " Top 10 Recommendations:\n",
      "Rank   Recipe ID    Recipe Name                             \n",
      "──────────────────────────────────────────────────────────────────────\n",
      "1      935          Recipe 935                              \n",
      "2      2490         Recipe 2490                             \n",
      "3      6758         Recipe 6758                             \n",
      "4      4200         Recipe 4200                             \n",
      "5      6862         Recipe 6862                             \n",
      "6      9658         Recipe 9658                             \n",
      "7      8503         Recipe 8503                             \n",
      "8      6021         Recipe 6021                             \n",
      "9      4637         Recipe 4637                             \n",
      "10     9450         Recipe 9450                             \n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      " USER: 2\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "\n",
      " Actual interactions in test set: 33\n",
      "   Average rating: 3.87\n",
      "\n",
      " Top 10 Recommendations:\n",
      "Rank   Recipe ID    Recipe Name                             \n",
      "──────────────────────────────────────────────────────────────────────\n",
      "1      10143        Recipe 10143                            \n",
      "2      3424         Recipe 3424                             \n",
      "3      8793         Recipe 8793                             \n",
      "4      4623         Recipe 4623                             \n",
      "5      3415         Recipe 3415                             \n",
      "6      2203         Recipe 2203                             \n",
      "7      8743         Recipe 8743                             \n",
      "8      9058         Recipe 9058                             \n",
      "9      7303         Recipe 7303                             \n",
      "10     6710         Recipe 6710                             \n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      " USER: 3\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "\n",
      " Actual interactions in test set: 39\n",
      "   Average rating: 3.95\n",
      "\n",
      " Top 10 Recommendations:\n",
      "Rank   Recipe ID    Recipe Name                             \n",
      "──────────────────────────────────────────────────────────────────────\n",
      "1      5595         Recipe 5595                             \n",
      "2      9698         Recipe 9698                             \n",
      "3      3351         Recipe 3351                             \n",
      "4      861          Recipe 861                              \n",
      "5      9024         Recipe 9024                             \n",
      "6      6183         Recipe 6183                             \n",
      "7      424          Recipe 424                              \n",
      "8      10092        Recipe 10092                            \n",
      "9      5768         Recipe 5768                             \n",
      "10     1932         Recipe 1932                             \n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      " USER: 4\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "\n",
      " Actual interactions in test set: 34\n",
      "   Average rating: 4.01\n",
      "\n",
      " Top 10 Recommendations:\n",
      "Rank   Recipe ID    Recipe Name                             \n",
      "──────────────────────────────────────────────────────────────────────\n",
      "1      4350         Recipe 4350                             \n",
      "2      154          Recipe 154                              \n",
      "3      3681         Recipe 3681                             \n",
      "4      3351         Recipe 3351                             \n",
      "5      2681         Recipe 2681                             \n",
      "6      8374         Recipe 8374                             \n",
      "7      7535         Recipe 7535                             \n",
      "8      255          Recipe 255                              \n",
      "9      9681         Recipe 9681                             \n",
      "10     4133         Recipe 4133                             \n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      " USER: 5\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "\n",
      " Actual interactions in test set: 52\n",
      "   Average rating: 4.04\n",
      "\n",
      " Top 10 Recommendations:\n",
      "Rank   Recipe ID    Recipe Name                             \n",
      "──────────────────────────────────────────────────────────────────────\n",
      "1      3763         Recipe 3763                             \n",
      "2      3110         Recipe 3110                             \n",
      "3      10305        Recipe 10305                            \n",
      "4      8512         Recipe 8512                             \n",
      "5      8069         Recipe 8069                             \n",
      "6      4200         Recipe 4200                             \n",
      "7      8870         Recipe 8870                             \n",
      "8      9584         Recipe 9584                             \n",
      "9      5935         Recipe 5935                             \n",
      "10     429          Recipe 429                              \n",
      "\n",
      "======================================================================\n",
      "✅ Sample recommendations complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\" GENERATING SAMPLE RECOMMENDATIONS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Get sample test users\n",
    "sample_test_users = test_df['user_id'].unique()[:5]\n",
    "\n",
    "for user_id in sample_test_users:\n",
    "    if user_id not in user_to_idx:\n",
    "        print(f\" User {user_id} not in training set. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(f\" USER: {user_id}\")\n",
    "    print(f\"{'─'*70}\")\n",
    "\n",
    "    # Actual interactions\n",
    "    user_test_data = test_df[test_df['user_id'] == user_id]\n",
    "    print(f\"\\n Actual interactions in test set: {len(user_test_data)}\")\n",
    "    if 'rating' in user_test_data.columns:\n",
    "        print(f\"   Average rating: {user_test_data['rating'].mean():.2f}\")\n",
    "\n",
    "    try:\n",
    "        user_idx = user_to_idx[user_id]\n",
    "\n",
    "        # Get user's column vector from weighted_matrix\n",
    "        user_items = weighted_matrix.getcol(user_idx).T.tocsr()\n",
    "\n",
    "        rec_indices, rec_scores = model.recommend(\n",
    "            userid=user_idx,\n",
    "            user_items=user_items,\n",
    "            N=200,  # candidate pool for reranking\n",
    "            filter_already_liked_items=True\n",
    "        )\n",
    "\n",
    "        # Map indices to recipe IDs\n",
    "        rec_recipe_ids = [idx_to_recipe[i] for i in rec_indices]\n",
    "\n",
    "        # Rerank\n",
    "        final_recs = rerank_for_user(user_id, rec_recipe_ids, rec_scores, use_content=use_content_similarity)[:10]\n",
    "\n",
    "        print(f\"\\n Top 10 Recommendations:\")\n",
    "        print(f\"{'Rank':<6} {'Recipe ID':<12} {'Recipe Name':<40}\")\n",
    "        print(\"─\"*70)\n",
    "\n",
    "        for rank, rid in enumerate(final_recs, 1):\n",
    "            # Safe recipe name lookup\n",
    "            if 'recipes' in locals() and 'recipe_id' in recipes.columns and 'title' in recipes.columns:\n",
    "                match = recipes.loc[recipes['recipe_id'] == rid, 'title']\n",
    "                recipe_name = match.values[0][:37] if not match.empty else f\"Recipe {rid}\"\n",
    "            else:\n",
    "                recipe_name = f\"Recipe {rid}\"\n",
    "\n",
    "            print(f\"{rank:<6} {rid:<12} {recipe_name:<40}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   Error generating recommendations for user {user_id}: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ Sample recommendations complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ab94ed",
   "metadata": {
    "id": "a5ab94ed"
   },
   "source": [
    "## ✅ Training Complete!\n",
    "\n",
    "### 📊 Summary\n",
    "\n",
    "I have successfully trained and evaluated your recommendation model!\n",
    "\n",
    "### What I Got:\n",
    "- ✅ **Trained ALS Model** saved to Google Drive\n",
    "- ✅ **Performance Metrics** (NDCG, Precision, Recall, MAP, MRR)\n",
    "- ✅ **Train/Validation/Test** evaluation with 80/10/10 split\n",
    "- ✅ **Visualizations** of model performance\n",
    "- ✅ **Sample Recommendations** to verify quality\n",
    "\n",
    "###  Files Saved to Google Drive:\n",
    "- `/MyDrive/RecipeML/models/als_model_*.pkl` - Trained model\n",
    "- `/MyDrive/RecipeML/results/summary_*.txt` - Text summary\n",
    "- `/MyDrive/RecipeML/results/metrics_*.csv` - Metrics CSV\n",
    "- `/MyDrive/RecipeML/results/*.png` - Visualization charts\n",
    "\n",
    "### Metrics Interpretation:\n",
    "\n",
    "**Your Test NDCG@10:**\n",
    "- **> 0.7** = EXCELLENT 🏆 - Ready for production!\n",
    "- **0.5-0.7** = GOOD - Can deploy, room for improvement\n",
    "- **0.3-0.5** = FAIR - Needs tuning before deployment\n",
    "- **< 0.3** = POOR - Requires major improvements\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **If performance is good (NDCG@10 > 0.5):**\n",
    "   - Download model from Google Drive\n",
    "   - Deploy using API server\n",
    "   - Monitor real-world performance\n",
    "\n",
    "2. **If performance needs improvement:**\n",
    "   - Adjust hyperparameters (FACTORS, REGULARIZATION, ITERATIONS)\n",
    "   - Try different models (Two-Tower, GraphSAGE)\n",
    "   - Collect more training data\n",
    "   - Feature engineering\n",
    "\n",
    "3. **Compare multiple models:**\n",
    "   - Train Two-Tower neural network\n",
    "   - Train GraphSAGE with social data\n",
    "   - Compare all models and pick the best\n",
    "\n",
    "### Tips:\n",
    "- Check validation vs test gap (should be < 0.1)\n",
    "- Monitor for overfitting\n",
    "- Save multiple checkpoints\n",
    "- Track experiments in spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789ef343",
   "metadata": {
    "id": "789ef343"
   },
   "source": [
    "---\n",
    "\n",
    "# Model 2: Two-Tower Neural Network Model\n",
    "\n",
    "Train a deep learning model with user and recipe encoders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7774773",
   "metadata": {
    "id": "a7774773"
   },
   "source": [
    "## 1️3. Prepare Features for Two-Tower Model\n",
    "\n",
    "Extract and normalize user and recipe features for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dbd32b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8dbd32b",
    "outputId": "e001b64d-fcee-4f4c-d883-6aa572e563e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 931700, Val: 200145, Test: 202482\n",
      "✅ User features (3): ['followers_count', 'following_count', 'recipes_count']\n",
      "✅ Recipe features (3): ['servings', 'cook_time', 'calories_per_serving']\n",
      "✅ Features normalized and scalers saved.\n",
      "\n",
      "📊 Two-Tower Data Samples:\n",
      "   Train: 931,700 samples\n",
      "   Val:   200,145 samples\n",
      "   Test:  202,482 samples\n",
      "\n",
      "✅ Two-Tower feature preparation complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "#  IMPROVED TWO-TOWER FEATURE PREPARATION (FIXED)\n",
    "# ============================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "DATA_DIR = \"/content/drive/MyDrive/RecipeML/data\"\n",
    "SCALER_DIR = \"/content/drive/MyDrive/RecipeML/scalers\"\n",
    "os.makedirs(SCALER_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_THRESHOLD = 4.0  # rating >= 4 is positive\n",
    "\n",
    "# --------------------------------------------\n",
    "# LOAD DATA\n",
    "# --------------------------------------------\n",
    "users_df = pd.read_csv(f\"{DATA_DIR}/users.csv\")\n",
    "recipes_df = pd.read_csv(f\"{DATA_DIR}/recipes.csv\")\n",
    "train_data = pd.read_csv(f\"{DATA_DIR}/train_interactions.csv\")\n",
    "val_data = pd.read_csv(f\"{DATA_DIR}/val_interactions.csv\")\n",
    "test_data = pd.read_csv(f\"{DATA_DIR}/test_interactions.csv\")\n",
    "\n",
    "# Ensure recipe_id exists in recipes_df\n",
    "if 'recipe_id' not in recipes_df.columns:\n",
    "    # try to guess common alternatives\n",
    "    if 'id' in recipes_df.columns:\n",
    "        recipes_df = recipes_df.rename(columns={'id': 'recipe_id'})\n",
    "    else:\n",
    "        raise KeyError(\"recipes_df must contain 'recipe_id' column\")\n",
    "\n",
    "print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# CREATE USER FEATURES\n",
    "# --------------------------------------------\n",
    "user_feature_cols = []\n",
    "for col in ['followers_count', 'following_count', 'recipes_count']:\n",
    "    if col in users_df.columns:\n",
    "        user_feature_cols.append(col)\n",
    "    else:\n",
    "        # Synthetic feature based on interaction counts\n",
    "        user_interactions = train_data.groupby('user_id').size()\n",
    "        users_df[col] = users_df['user_id'].map(user_interactions).fillna(0)\n",
    "        user_feature_cols.append(col)\n",
    "\n",
    "# Add missing indicators\n",
    "for col in user_feature_cols:\n",
    "    users_df[f\"{col}_missing\"] = (users_df[col].isna()).astype(int)\n",
    "\n",
    "# Normalize user features\n",
    "user_scaler = StandardScaler()\n",
    "users_df[user_feature_cols] = user_scaler.fit_transform(users_df[user_feature_cols].fillna(0))\n",
    "\n",
    "# Save user scaler\n",
    "pickle.dump(user_scaler, open(os.path.join(SCALER_DIR, \"user_scaler.pkl\"), \"wb\"))\n",
    "\n",
    "# --------------------------------------------\n",
    "# CREATE RECIPE FEATURES\n",
    "# --------------------------------------------\n",
    "recipe_feature_cols = []\n",
    "for col in ['servings', 'cook_time', 'calories_per_serving']:\n",
    "    if col in recipes_df.columns:\n",
    "        recipe_feature_cols.append(col)\n",
    "    else:\n",
    "        # Synthetic feature based on train interactions\n",
    "        recipe_interactions = train_data.groupby('recipe_id').size()\n",
    "        recipes_df[col] = recipes_df['recipe_id'].map(recipe_interactions).fillna(1)\n",
    "        recipe_feature_cols.append(col)\n",
    "\n",
    "# Add missing indicators\n",
    "for col in recipe_feature_cols:\n",
    "    recipes_df[f\"{col}_missing\"] = (recipes_df[col].isna()).astype(int)\n",
    "\n",
    "# Normalize recipe features\n",
    "recipe_scaler = StandardScaler()\n",
    "recipes_df[recipe_feature_cols] = recipe_scaler.fit_transform(recipes_df[recipe_feature_cols].fillna(0))\n",
    "\n",
    "# Save recipe scaler\n",
    "pickle.dump(recipe_scaler, open(os.path.join(SCALER_DIR, \"recipe_scaler.pkl\"), \"wb\"))\n",
    "\n",
    "print(f\"✅ User features ({len(user_feature_cols)}): {user_feature_cols}\")\n",
    "print(f\"✅ Recipe features ({len(recipe_feature_cols)}): {recipe_feature_cols}\")\n",
    "print(\"✅ Features normalized and scalers saved.\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# FUNCTION TO PREPARE TWO-TOWER DATA\n",
    "# --------------------------------------------\n",
    "def prepare_two_tower_data(interactions, users, recipes, target_threshold=TARGET_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Merge interactions with user and recipe features.\n",
    "    Create binary target: 1 if rating >= target_threshold, else 0.\n",
    "    \"\"\"\n",
    "    if 'recipe_id' not in recipes.columns:\n",
    "        raise KeyError(\"recipes dataframe must have 'recipe_id' column before merging\")\n",
    "\n",
    "    # Merge user features\n",
    "    data = interactions.merge(\n",
    "        users[['user_id'] + user_feature_cols + [f\"{c}_missing\" for c in user_feature_cols]],\n",
    "        on='user_id', how='left'\n",
    "    )\n",
    "\n",
    "    # Merge recipe features\n",
    "    data = data.merge(\n",
    "        recipes[['recipe_id'] + recipe_feature_cols + [f\"{c}_missing\" for c in recipe_feature_cols]],\n",
    "        on='recipe_id', how='left'\n",
    "    )\n",
    "\n",
    "    # Target\n",
    "    if 'rating' in data.columns:\n",
    "        data['target'] = (data['rating'] >= target_threshold).astype(float)\n",
    "    else:\n",
    "        data['target'] = 1.0  # all positive\n",
    "\n",
    "    # Fill remaining NaNs\n",
    "    return data.fillna(0)\n",
    "\n",
    "# --------------------------------------------\n",
    "# PREPARE DATA\n",
    "# --------------------------------------------\n",
    "train_two_tower = prepare_two_tower_data(train_data, users_df, recipes_df)\n",
    "val_two_tower = prepare_two_tower_data(val_data, users_df, recipes_df)\n",
    "test_two_tower = prepare_two_tower_data(test_data, users_df, recipes_df)\n",
    "\n",
    "print(f\"\\n📊 Two-Tower Data Samples:\")\n",
    "print(f\"   Train: {len(train_two_tower):,} samples\")\n",
    "print(f\"   Val:   {len(val_two_tower):,} samples\")\n",
    "print(f\"   Test:  {len(test_two_tower):,} samples\")\n",
    "print(\"\\n✅ Two-Tower feature preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177a2df6",
   "metadata": {
    "id": "177a2df6"
   },
   "source": [
    "## 1️4. Define Two-Tower Neural Network Architecture\n",
    "\n",
    "I'll create a **Two-Tower model** with:\n",
    "- **User Tower**: Encodes user features into embeddings\n",
    "- **Recipe Tower**: Encodes recipe features into embeddings\n",
    "- **Dot Product**: Computes similarity score between user and recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d15913",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83d15913",
    "outputId": "6fcd6c5e-2867-4ac1-cb35-7c4810530c8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using device: cuda\n",
      "   GPU: Tesla T4\n",
      "\n",
      "======================================================================\n",
      "  TWO-TOWER MODEL ARCHITECTURE\n",
      "======================================================================\n",
      "Users: 5,000 | Recipes: 10,457\n",
      "User features: 3 | Recipe features: 3\n",
      "Embedding dimension: 128\n",
      "Total parameters: 2,113,408\n",
      "✅ Model created and moved to cuda!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ---------------------------\n",
    "# DEVICE SETUP\n",
    "# ---------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\" Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# ---------------------------\n",
    "# MLP BLOCK (shared)\n",
    "# ---------------------------\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[256, 128], dropout=[0.3, 0.2], activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i, h in enumerate(hidden_dims):\n",
    "            layers.append(nn.Linear(input_dim, h))\n",
    "            layers.append(nn.LayerNorm(h))  # LayerNorm is stable for small batches\n",
    "            layers.append(activation())\n",
    "            layers.append(nn.Dropout(dropout[i]))\n",
    "            input_dim = h\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "# ---------------------------\n",
    "# USER / RECIPE ENCODER\n",
    "# ---------------------------\n",
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self, n_users, user_feat_dim, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.mlp = MLPBlock(input_dim=embedding_dim + user_feat_dim, hidden_dims=[256, 128], dropout=[0.3, 0.2])\n",
    "\n",
    "    def forward(self, user_ids, user_features):\n",
    "        emb = self.user_embedding(user_ids)\n",
    "        x = torch.cat([emb, user_features], dim=1)\n",
    "        x = self.mlp(x)\n",
    "        return F.normalize(x, p=2, dim=1)\n",
    "\n",
    "class RecipeEncoder(nn.Module):\n",
    "    def __init__(self, n_recipes, recipe_feat_dim, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        self.recipe_embedding = nn.Embedding(n_recipes, embedding_dim)\n",
    "        self.mlp = MLPBlock(input_dim=embedding_dim + recipe_feat_dim, hidden_dims=[256, 128], dropout=[0.3, 0.2])\n",
    "\n",
    "    def forward(self, recipe_ids, recipe_features):\n",
    "        emb = self.recipe_embedding(recipe_ids)\n",
    "        x = torch.cat([emb, recipe_features], dim=1)\n",
    "        x = self.mlp(x)\n",
    "        return F.normalize(x, p=2, dim=1)\n",
    "\n",
    "# ---------------------------\n",
    "# TWO-TOWER MODEL\n",
    "# ---------------------------\n",
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, n_users, n_recipes, user_feat_dim, recipe_feat_dim, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        self.user_encoder = UserEncoder(n_users, user_feat_dim, embedding_dim)\n",
    "        self.recipe_encoder = RecipeEncoder(n_recipes, recipe_feat_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, user_ids, user_features, recipe_ids, recipe_features):\n",
    "        user_emb = self.user_encoder(user_ids, user_features)\n",
    "        recipe_emb = self.recipe_encoder(recipe_ids, recipe_features)\n",
    "        return (user_emb * recipe_emb).sum(dim=1)  # cosine-like similarity\n",
    "\n",
    "    def get_user_embeddings(self, user_ids, user_features):\n",
    "        return self.user_encoder(user_ids, user_features)\n",
    "\n",
    "    def get_recipe_embeddings(self, recipe_ids, recipe_features):\n",
    "        return self.recipe_encoder(recipe_ids, recipe_features)\n",
    "\n",
    "# ---------------------------\n",
    "# CONTRASTIVE / BPR LOSS\n",
    "# ---------------------------\n",
    "def bpr_loss(user_emb, pos_emb, neg_emb):\n",
    "    \"\"\"\n",
    "    Bayesian Personalized Ranking Loss\n",
    "    \"\"\"\n",
    "    pos_scores = (user_emb * pos_emb).sum(dim=1)\n",
    "    neg_scores = (user_emb * neg_emb).sum(dim=1)\n",
    "    loss = -F.logsigmoid(pos_scores - neg_scores).mean()\n",
    "    return loss\n",
    "\n",
    "# ---------------------------\n",
    "# EXAMPLE INITIALIZATION\n",
    "# ---------------------------\n",
    "n_users = users_df['user_id'].nunique()\n",
    "n_recipes = recipes_df['recipe_id'].nunique()\n",
    "user_feat_dim = len(user_feature_cols)\n",
    "recipe_feat_dim = len(recipe_feature_cols)\n",
    "\n",
    "two_tower_model = TwoTowerModel(\n",
    "    n_users=n_users,\n",
    "    n_recipes=n_recipes,\n",
    "    user_feat_dim=user_feat_dim,\n",
    "    recipe_feat_dim=recipe_feat_dim,\n",
    "    embedding_dim=128\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"  TWO-TOWER MODEL ARCHITECTURE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Users: {n_users:,} | Recipes: {n_recipes:,}\")\n",
    "print(f\"User features: {user_feat_dim} | Recipe features: {recipe_feat_dim}\")\n",
    "print(f\"Embedding dimension: 128\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in two_tower_model.parameters()):,}\")\n",
    "print(f\"✅ Model created and moved to {device}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6192ee3d",
   "metadata": {
    "id": "6192ee3d"
   },
   "source": [
    "## 1️5. Create PyTorch DataLoaders\n",
    "\n",
    "Prepare datasets and dataloaders for efficient batch training on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd30cbc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fd30cbc",
    "outputId": "fe6117ea-8682-4a1c-fb12-b12004582bcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data...\n",
      "Train interactions: 931,700\n",
      "Val interactions: 200,145\n",
      "Test interactions: 202,482\n",
      "Users: 5,000\n",
      "Recipes: 10,457\n",
      "\n",
      "🔨 Creating train samples with 1:4 pos:neg ratio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|██████████| 5000/5000 [00:11<00:00, 452.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train samples created:\n",
      "   Total: 4,602,325\n",
      "   Positive: 920,465 (20.0%)\n",
      "   Negative: 3,681,860 (80.0%)\n",
      "   Ratio: 1:4.0\n",
      "\n",
      "🔨 Creating validation samples with 1:4 pos:neg ratio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation: 100%|██████████| 5000/5000 [00:08<00:00, 591.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validation samples created:\n",
      "   Total: 993,010\n",
      "   Positive: 198,602 (20.0%)\n",
      "   Negative: 794,408 (80.0%)\n",
      "   Ratio: 1:4.0\n",
      "\n",
      "🔨 Creating test samples with 1:4 pos:neg ratio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test: 100%|██████████| 5000/5000 [00:08<00:00, 603.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test samples created:\n",
      "   Total: 1,004,155\n",
      "   Positive: 200,831 (20.0%)\n",
      "   Negative: 803,324 (80.0%)\n",
      "   Ratio: 1:4.0\n",
      "\n",
      "📊 Preparing user features...\n",
      "Numeric user columns identified: ['age']\n",
      "Categorical user columns identified: ['gender', 'location', 'user_segment']\n",
      "Final user feature columns: ['age', 'gender_F', 'gender_M', 'gender_O', 'location_Berlin, DE', 'location_London, UK', 'location_Los Angeles, USA', 'location_Mumbai, IN', 'location_New York, USA', 'location_Paris, FR', 'location_Seoul, KR', 'location_Sydney, AU', 'location_Tokyo, JP', 'location_Toronto, CA', 'user_segment_Adventurous Foodies', 'user_segment_Baking Enthusiasts', 'user_segment_General Users', 'user_segment_Healthy Eaters', 'user_segment_Italian Food Lovers', 'user_segment_Quick Meal Seekers']\n",
      "User feature dimension: 20\n",
      "\n",
      "📊 Preparing recipe features...\n",
      "  ✅ Extracted 'num_ingredients' feature.\n",
      "  ✅ Extracted 'num_instructions' feature.\n",
      "  ✅ Extracted 'num_tags' feature.\n",
      "  ⚠️ Could not process 'created_date' column: 'days_since_created_date'. Skipping.\n",
      "  ⚠️ Could not process 'modified_date' column: 'days_since_modified_date'. Skipping.\n",
      "Numeric recipe columns identified: ['servings', 'cook_time', 'prep_time', 'total_time', 'calories_per_serving', 'popularity_score', 'view_count', 'save_count', 'like_count', 'comment_count', 'avg_rating', 'num_ingredients', 'num_instructions', 'num_tags']\n",
      "Categorical recipe columns identified: ['dietary_type', 'cuisine', 'difficulty', 'cooking_method']\n",
      "Final recipe feature columns: ['servings', 'cook_time', 'prep_time', 'total_time', 'calories_per_serving', 'popularity_score', 'view_count', 'save_count', 'like_count', 'comment_count', 'avg_rating', 'num_ingredients', 'num_instructions', 'num_tags', 'dietary_type_Dairy-Free', 'dietary_type_Gluten-Free', 'dietary_type_Halal', 'dietary_type_Keto', 'dietary_type_Kosher', 'dietary_type_Low-Carb', 'dietary_type_Mediterranean', 'dietary_type_Omnivore', 'dietary_type_Paleo', 'dietary_type_Pescatarian', 'dietary_type_Vegan', 'dietary_type_Vegetarian', 'dietary_type_Whole30', 'cuisine_American', 'cuisine_Brazilian', 'cuisine_Caribbean', 'cuisine_Chinese', 'cuisine_Ethiopian', 'cuisine_French', 'cuisine_German', 'cuisine_Greek', 'cuisine_Indian', 'cuisine_Italian', 'cuisine_Japanese', 'cuisine_Korean', 'cuisine_Mediterranean', 'cuisine_Mexican', 'cuisine_Middle Eastern', 'cuisine_Moroccan', 'cuisine_Spanish', 'cuisine_Thai', 'cuisine_Turkish', 'cuisine_Vietnamese', 'difficulty_Easy', 'difficulty_Hard', 'difficulty_Medium', 'cooking_method_Air Fried', 'cooking_method_Baked', 'cooking_method_Braised', 'cooking_method_Fried', 'cooking_method_Grilled', 'cooking_method_Poached', 'cooking_method_Pressure Cooked', 'cooking_method_Raw/No-Cook', 'cooking_method_Roasted', 'cooking_method_Sautéed', 'cooking_method_Slow Cooked', 'cooking_method_Smoked', 'cooking_method_Steamed']\n",
      "Recipe feature dimension: 63\n",
      "\n",
      " Creating ID mappings...\n",
      "Unique users: 5,000\n",
      "Unique recipes: 10,437\n",
      "  Removed 7,049 samples with unmapped IDs\n",
      "  Removed 1,551 samples with unmapped IDs\n",
      "  Removed 1,547 samples with unmapped IDs\n",
      "\n",
      " Creating PyTorch datasets...\n",
      "\n",
      "🚀 Applying quick sampling for faster training...\n",
      "✅ Sampled to 15.0% of data:\n",
      "   Train: 689,291 samples\n",
      "   Val: 148,718 samples\n",
      "   Note: Test dataset remains full size for final evaluation.\n",
      "Train dataset size: 689,291\n",
      "Val dataset size: 148,718\n",
      "Test dataset size: 1,002,608\n",
      "\n",
      " Creating PyTorch DataLoaders for verification...\n",
      "Train loader batches: 1,347\n",
      "Val loader batches: 291\n",
      "Test loader batches: 1,959\n",
      "\n",
      " Saving processed data to /content/drive/MyDrive/RecipeML/processed...\n",
      "   Saving train_dataset.pt (689,291 samples)...\n",
      "   ✅ train_dataset.pt saved\n",
      "   Saving val_dataset.pt (148,718 samples)...\n",
      "   ✅ val_dataset.pt saved\n",
      "   Saving test_dataset.pt (1,002,608 samples)...\n",
      "   ✅ test_dataset.pt saved\n",
      "   Saving mappings.pkl...\n",
      "   ✅ mappings.pkl saved\n",
      "   Saving metadata.pkl...\n",
      "   ✅ metadata.pkl saved\n",
      "\n",
      "✅ Data preparation complete!\n",
      "\n",
      " Summary:\n",
      "   n_users: 5,000\n",
      "   n_recipes: 10,437\n",
      "   user_feature_dim: 20\n",
      "   recipe_feature_dim: 63\n",
      "   negative_sampling_ratio: 4\n",
      "   random_seed: 42\n",
      "   n_train_samples_sampled: 689,291\n",
      "   n_val_samples_sampled: 148,718\n",
      "\n",
      " Saved files:\n",
      "   ✅ train_dataset.pt (689,291 samples)\n",
      "   ✅ val_dataset.pt (148,718 samples)\n",
      "   ✅ test_dataset.pt (1,002,608 samples)\n",
      "   ✅ mappings.pkl\n",
      "   ✅ metadata.pkl\n",
      "\n",
      "🚀 Ready to train! Use the training script to load and train.\n",
      "\n",
      "🔍 Quick verification:\n",
      "Sample item structure:\n",
      "   user_id: shape=torch.Size([]), dtype=torch.int64\n",
      "   recipe_id: shape=torch.Size([]), dtype=torch.int64\n",
      "   user_features: shape=torch.Size([20]), dtype=torch.float32\n",
      "   recipe_features: shape=torch.Size([63]), dtype=torch.float32\n",
      "   target: shape=torch.Size([]), dtype=torch.float32\n",
      "\n",
      "🔍 Verifying batch loading...\n",
      "Batch structure:\n",
      "   user_id: shape=torch.Size([512]), dtype=torch.int64\n",
      "   recipe_id: shape=torch.Size([512]), dtype=torch.int64\n",
      "   user_features: shape=torch.Size([512, 20]), dtype=torch.float32\n",
      "   recipe_features: shape=torch.Size([512, 63]), dtype=torch.float32\n",
      "   target: shape=torch.Size([512]), dtype=torch.float32\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Two-Tower Model Data Preparation Script\n",
    "This creates properly balanced training data with negative sampling\n",
    "Run this BEFORE your training script\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import Subset # Import Subset\n",
    "import pickle\n",
    "import os\n",
    "import ast # Import ast for literal_eval\n",
    "import time\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "DATA_DIR = \"/content/drive/MyDrive/RecipeML/data\"\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/RecipeML/processed\"\n",
    "N_NEGATIVES_PER_POSITIVE = 4  # Critical: controls class balance\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Initialize metadata dictionary\n",
    "metadata = {}\n",
    "\n",
    "# ============================================\n",
    "# LOAD RAW DATA\n",
    "# ============================================\n",
    "print(\"Loading raw data...\")\n",
    "start_time = time.time() # Assuming time is imported or defined\n",
    "\n",
    "users = pd.read_csv(f\"{DATA_DIR}/users.csv\")\n",
    "recipes = pd.read_csv(f\"{DATA_DIR}/recipes.csv\")\n",
    "train_interactions = pd.read_csv(f\"{DATA_DIR}/train_interactions.csv\")\n",
    "val_interactions = pd.read_csv(f\"{DATA_DIR}/val_interactions.csv\")\n",
    "test_interactions = pd.read_csv(f\"{DATA_DIR}/test_interactions.csv\")\n",
    "\n",
    "print(f\"Train interactions: {len(train_interactions):,}\")\n",
    "print(f\"Val interactions: {len(val_interactions):,}\")\n",
    "print(f\"Test interactions: {len(test_interactions):,}\")\n",
    "print(f\"Users: {len(users):,}\")\n",
    "print(f\"Recipes: {len(recipes):,}\")\n",
    "\n",
    "recipes.rename(columns={'id': 'recipe_id'}, inplace=True)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# NEGATIVE SAMPLING FUNCTION\n",
    "# ============================================\n",
    "def create_training_samples(interactions_df, n_negatives_per_positive=4, split_name=\"train\"):\n",
    "    \"\"\"\n",
    "    For each positive interaction, sample N negative recipes the user didn't interact with\n",
    "\n",
    "    Args:\n",
    "        interactions_df: DataFrame with user_id, recipe_id columns (positive interactions)\n",
    "        n_negatives_per_positive: Number of negative samples per positive\n",
    "        split_name: Name for progress bar\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with user_id, recipe_id, target columns\n",
    "    \"\"\"\n",
    "    all_recipes = set(recipes['recipe_id'].unique())\n",
    "    samples = []\n",
    "\n",
    "    print(f\"\\n🔨 Creating {split_name} samples with 1:{n_negatives_per_positive} pos:neg ratio...\")\n",
    "\n",
    "    # Group by user_id and use apply for efficiency on groups\n",
    "    user_grouped = interactions_df.groupby('user_id')\n",
    "\n",
    "    for user_id, group in tqdm(user_grouped, desc=f\"Processing {split_name}\"):\n",
    "        positive_recipes = set(group['recipe_id'])\n",
    "        negative_pool = list(all_recipes - positive_recipes)\n",
    "\n",
    "        # Add all positive samples\n",
    "        for recipe_id in positive_recipes:\n",
    "            samples.append({\n",
    "                'user_id': user_id,\n",
    "                'recipe_id': recipe_id,\n",
    "                'target': 1\n",
    "            })\n",
    "\n",
    "        # Sample negatives (don't exceed available negatives)\n",
    "        n_negatives = min(len(positive_recipes) * n_negatives_per_positive, len(negative_pool))\n",
    "\n",
    "        if n_negatives > 0:\n",
    "            # Faster sampling using numpy choice\n",
    "            negative_recipes = np.random.choice(negative_pool, size=n_negatives, replace=False)\n",
    "\n",
    "            for recipe_id in negative_recipes:\n",
    "                samples.append({\n",
    "                    'user_id': user_id,\n",
    "                    'recipe_id': recipe_id,\n",
    "                    'target': 0\n",
    "                })\n",
    "\n",
    "    result_df = pd.DataFrame(samples)\n",
    "\n",
    "    # Report statistics\n",
    "    n_pos = result_df['target'].sum()\n",
    "    n_neg = len(result_df) - n_pos\n",
    "    pos_ratio = n_pos / len(result_df) if len(result_df) > 0 else 0.0\n",
    "\n",
    "    print(f\"✅ {split_name.capitalize()} samples created:\")\n",
    "    print(f\"   Total: {len(result_df):,}\")\n",
    "    print(f\"   Positive: {n_pos:,} ({pos_ratio*100:.1f}%)\")\n",
    "    print(f\"   Negative: {n_neg:,} ({(1-pos_ratio)*100:.1f}%)\")\n",
    "    print(f\"   Ratio: 1:{n_neg/n_pos:.1f}\" if n_pos > 0 else \"   Ratio: N/A (no positives)\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# ============================================\n",
    "# CREATE BALANCED DATASETS\n",
    "# ============================================\n",
    "train_samples = create_training_samples(\n",
    "    train_interactions,\n",
    "    n_negatives_per_positive=N_NEGATIVES_PER_POSITIVE,\n",
    "    split_name=\"train\"\n",
    ")\n",
    "\n",
    "val_samples = create_training_samples(\n",
    "    val_interactions,\n",
    "    n_negatives_per_positive=N_NEGATIVES_PER_POSITIVE,\n",
    "    split_name=\"validation\"\n",
    ")\n",
    "\n",
    "test_samples = create_training_samples(\n",
    "    test_interactions,\n",
    "    n_negatives_per_positive=N_NEGATIVES_PER_POSITIVE,\n",
    "    split_name=\"test\"\n",
    ")\n",
    "\n",
    "# Shuffle training data\n",
    "train_samples = train_samples.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "# ============================================\n",
    "# PREPARE USER FEATURES (FIXED)\n",
    "# ============================================\n",
    "print(\"\\n📊 Preparing user features...\")\n",
    "\n",
    "# Select and encode categorical features first\n",
    "categorical_user_cols = ['gender', 'location', 'user_segment']\n",
    "numeric_user_cols = []\n",
    "\n",
    "# Identify actual numeric columns\n",
    "for col in users.columns:\n",
    "    if col == 'user_id':\n",
    "        continue\n",
    "    if users[col].dtype in ['int64', 'float64']:\n",
    "         if col not in categorical_user_cols: # Ensure it's not in categorical list\n",
    "             numeric_user_cols.append(col)\n",
    "\n",
    "# Handle 'preferences' column if it exists and is a string\n",
    "if 'preferences' in users.columns:\n",
    "    try:\n",
    "        users['preferences_dict'] = users['preferences'].apply(\n",
    "            lambda x: ast.literal_eval(x) if isinstance(x, str) else (x if isinstance(x, dict) else {})\n",
    "        )\n",
    "        # Example: Extract count of dietary preferences\n",
    "        users['dietary_preference_count'] = users['preferences_dict'].apply(\n",
    "            lambda x: len(x.get('dietary', [])) if isinstance(x, dict) else 0\n",
    "        )\n",
    "        numeric_user_cols.append('dietary_preference_count')\n",
    "        print(\"  ✅ Extracted 'dietary_preference_count' from preferences.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️ Could not parse 'preferences' column: {e}. Skipping.\")\n",
    "    users = users.drop(columns=['preferences', 'preferences_dict'], errors='ignore') # Drop original column\n",
    "\n",
    "print(f\"Numeric user columns identified: {numeric_user_cols}\")\n",
    "print(f\"Categorical user columns identified: {categorical_user_cols}\")\n",
    "\n",
    "# Apply one-hot encoding to categorical features\n",
    "users_processed = pd.get_dummies(users, columns=categorical_user_cols, dummy_na=False)\n",
    "\n",
    "# Combine numeric and one-hot encoded columns for scaling\n",
    "final_user_feature_cols = numeric_user_cols + [col for col in users_processed.columns if any(cat_col in col for cat_col in categorical_user_cols)]\n",
    "\n",
    "if final_user_feature_cols:\n",
    "    # Fill any remaining NaNs in selected columns before scaling\n",
    "    users_processed[final_user_feature_cols] = users_processed[final_user_feature_cols].fillna(0)\n",
    "\n",
    "    # Standardize user features\n",
    "    scaler_user = StandardScaler()\n",
    "    users_processed[final_user_feature_cols] = scaler_user.fit_transform(users_processed[final_user_feature_cols])\n",
    "    user_features_dict = users_processed.set_index('user_id')[final_user_feature_cols].to_dict('index')\n",
    "    user_feature_dim = len(final_user_feature_cols)\n",
    "    user_feature_cols_list = final_user_feature_cols # Use this for dataset class\n",
    "\n",
    "else:\n",
    "    print(\"  No user features found after processing.\")\n",
    "    user_features_dict = {uid: {} for uid in users['user_id']}\n",
    "    user_feature_dim = 0\n",
    "    user_feature_cols_list = []\n",
    "\n",
    "\n",
    "print(f\"Final user feature columns: {user_feature_cols_list}\")\n",
    "print(f\"User feature dimension: {user_feature_dim}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# PREPARE RECIPE FEATURES (FIXED)\n",
    "# ============================================\n",
    "print(\"\\n📊 Preparing recipe features...\")\n",
    "\n",
    "numeric_recipe_cols = []\n",
    "categorical_recipe_cols = ['dietary_type', 'cuisine', 'difficulty', 'cooking_method'] # Add relevant categorical recipe columns\n",
    "\n",
    "# Identify actual numeric columns (excluding IDs)\n",
    "for col in recipes.columns:\n",
    "    if col not in ['recipe_id', 'author_id']: # Exclude IDs and author_id if not treated as numeric\n",
    "        if recipes[col].dtype in ['int64', 'float64']:\n",
    "            if col not in categorical_recipe_cols: # Ensure it's not in categorical list\n",
    "                numeric_recipe_cols.append(col)\n",
    "\n",
    "# Handle potential list/string columns like 'ingredients', 'instructions', 'tags'\n",
    "for col in ['ingredients', 'instructions', 'tags']:\n",
    "    if col in recipes.columns:\n",
    "        try:\n",
    "            # Safely evaluate string representations of lists\n",
    "            recipes[f'num_{col}'] = recipes[col].apply(\n",
    "                lambda x: len(ast.literal_eval(x)) if isinstance(x, str) else (len(x) if isinstance(x, list) else 0)\n",
    "            )\n",
    "            numeric_recipe_cols.append(f'num_{col}')\n",
    "            print(f\"  ✅ Extracted 'num_{col}' feature.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ Could not process '{col}' column: {e}. Skipping.\")\n",
    "        recipes = recipes.drop(columns=[col], errors='ignore') # Drop original column\n",
    "\n",
    "# Handle date columns for recency/freshness if needed, convert to numeric (e.g., days since)\n",
    "# This example assumes `created_date` and `modified_date` exist\n",
    "for date_col in ['created_date', 'modified_date']:\n",
    "    if date_col in recipes.columns:\n",
    "        try:\n",
    "            recipes[date_col] = pd.to_datetime(recipes[date_col], errors='coerce')\n",
    "            recipes[f'days_since_{date_col}'] = (pd.to_datetime('now') - recipes[date_col]).dt.days.fillna(recipes[f'days_since_{date_col}'].max()) # Fill NaT with max days\n",
    "            numeric_recipe_cols.append(f'days_since_{date_col}')\n",
    "            print(f\"  ✅ Extracted 'days_since_{date_col}' feature.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ Could not process '{date_col}' column: {e}. Skipping.\")\n",
    "        recipes = recipes.drop(columns=[date_col], errors='ignore')\n",
    "\n",
    "\n",
    "print(f\"Numeric recipe columns identified: {numeric_recipe_cols}\")\n",
    "print(f\"Categorical recipe columns identified: {categorical_recipe_cols}\")\n",
    "\n",
    "# Apply one-hot encoding to categorical recipe features\n",
    "recipes_processed = pd.get_dummies(recipes, columns=categorical_recipe_cols, dummy_na=False)\n",
    "\n",
    "# Combine numeric and one-hot encoded columns for scaling\n",
    "final_recipe_feature_cols = numeric_recipe_cols + [col for col in recipes_processed.columns if any(cat_col in col for cat_col in categorical_recipe_cols)]\n",
    "\n",
    "if final_recipe_feature_cols:\n",
    "    # Fill any remaining NaNs in selected columns before scaling\n",
    "    recipes_processed[final_recipe_feature_cols] = recipes_processed[final_recipe_feature_cols].fillna(0)\n",
    "\n",
    "    # Standardize recipe features\n",
    "    scaler_recipe = StandardScaler()\n",
    "    recipes_processed[final_recipe_feature_cols] = scaler_recipe.fit_transform(recipes_processed[final_recipe_feature_cols])\n",
    "    recipe_features_dict = recipes_processed.set_index('recipe_id')[final_recipe_feature_cols].to_dict('index')\n",
    "    recipe_feature_dim = len(final_recipe_feature_cols)\n",
    "    recipe_feature_cols_list = final_recipe_feature_cols # Use this for dataset class\n",
    "\n",
    "else:\n",
    "    print(\"  No recipe features found after processing.\")\n",
    "    recipe_features_dict = {rid: {} for rid in recipes['recipe_id']}\n",
    "    recipe_feature_dim = 0\n",
    "    recipe_feature_cols_list = []\n",
    "\n",
    "\n",
    "print(f\"Final recipe feature columns: {recipe_feature_cols_list}\")\n",
    "print(f\"Recipe feature dimension: {recipe_feature_dim}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# CREATE ID MAPPINGS\n",
    "# ============================================\n",
    "print(\"\\n Creating ID mappings...\")\n",
    "\n",
    "# Get all unique IDs from train (important: use train as reference)\n",
    "all_user_ids = sorted(train_interactions['user_id'].unique())\n",
    "all_recipe_ids = sorted(train_interactions['recipe_id'].unique())\n",
    "\n",
    "user_id_to_idx = {uid: idx for idx, uid in enumerate(all_user_ids)}\n",
    "recipe_id_to_idx = {rid: idx for idx, rid in enumerate(all_recipe_ids)}\n",
    "\n",
    "print(f\"Unique users: {len(all_user_ids):,}\")\n",
    "print(f\"Unique recipes: {len(all_recipe_ids):,}\")\n",
    "\n",
    "# Map IDs in all datasets\n",
    "for df in [train_samples, val_samples, test_samples]:\n",
    "    df['user_idx'] = df['user_id'].map(user_id_to_idx)\n",
    "    df['recipe_idx'] = df['recipe_id'].map(recipe_id_to_idx)\n",
    "\n",
    "    # Remove rows with unmapped IDs (cold-start items)\n",
    "    before = len(df)\n",
    "    df.dropna(subset=['user_idx', 'recipe_idx'], inplace=True)\n",
    "    after = len(df)\n",
    "    if before != after:\n",
    "        print(f\"  Removed {before - after:,} samples with unmapped IDs\")\n",
    "\n",
    "# ============================================\n",
    "# CREATE PYTORCH DATASET\n",
    "# ============================================\n",
    "class TwoTowerDataset(Dataset):\n",
    "    def __init__(self, samples_df, user_features_dict, recipe_features_dict,\n",
    "                 user_feature_cols, recipe_feature_cols):\n",
    "        self.samples = samples_df.reset_index(drop=True)\n",
    "        self.user_features_dict = user_features_dict\n",
    "        self.recipe_features_dict = recipe_features_dict\n",
    "        self.user_feature_cols = user_feature_cols # Use list of column names\n",
    "        self.recipe_feature_cols = recipe_feature_cols # Use list of column names\n",
    "\n",
    "        # Get feature dimensions based on actual columns used\n",
    "        self.user_feature_dim = len(user_feature_cols)\n",
    "        self.recipe_feature_dim = len(recipe_feature_cols)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.samples.iloc[idx]\n",
    "\n",
    "        user_id = int(row['user_idx'])\n",
    "        recipe_id = int(row['recipe_idx'])\n",
    "        target = float(row['target'])  # Changed to float for BCE loss\n",
    "\n",
    "        # Get original IDs for feature lookup\n",
    "        orig_user_id = row['user_id']\n",
    "        orig_recipe_id = row['recipe_id']\n",
    "\n",
    "        # Extract features - ensure same order as feature_cols_list\n",
    "        user_feat_dict = self.user_features_dict.get(orig_user_id, {})\n",
    "        recipe_feat_dict = self.recipe_features_dict.get(orig_recipe_id, {})\n",
    "\n",
    "        # Ensure feature tensors have correct dimension even if dict is empty\n",
    "        user_features = torch.tensor([\n",
    "            user_feat_dict.get(col, 0.0) for col in self.user_feature_cols\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        recipe_features = torch.tensor([\n",
    "            recipe_feat_dict.get(col, 0.0) for col in self.recipe_feature_cols\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "\n",
    "        return {\n",
    "            'user_id': torch.tensor(user_id, dtype=torch.long),\n",
    "            'recipe_id': torch.tensor(recipe_id, dtype=torch.long),\n",
    "            'user_features': user_features,\n",
    "            'recipe_features': recipe_features,\n",
    "            'target': torch.tensor(target, dtype=torch.float32)  # Changed to float32\n",
    "        }\n",
    "\n",
    "# ============================================\n",
    "# CREATE DATASETS\n",
    "# ============================================\n",
    "print(\"\\n Creating PyTorch datasets...\")\n",
    "\n",
    "train_dataset = TwoTowerDataset(\n",
    "    train_samples, user_features_dict, recipe_features_dict,\n",
    "    user_feature_cols_list, recipe_feature_cols_list # Pass the list of column names\n",
    ")\n",
    "\n",
    "val_dataset = TwoTowerDataset(\n",
    "    val_samples, user_features_dict, recipe_features_dict,\n",
    "    user_feature_cols_list, recipe_feature_cols_list\n",
    ")\n",
    "\n",
    "test_dataset = TwoTowerDataset(\n",
    "    test_samples, user_features_dict, recipe_features_dict,\n",
    "    user_feature_cols_list, recipe_feature_cols_list\n",
    ")\n",
    "\n",
    "# Update metadata with correct dimensions\n",
    "metadata['n_users'] = len(all_user_ids)\n",
    "metadata['n_recipes'] = len(all_recipe_ids)\n",
    "metadata['user_feature_dim'] = train_dataset.user_feature_dim\n",
    "metadata['recipe_feature_dim'] = train_dataset.recipe_feature_dim\n",
    "metadata['negative_sampling_ratio'] = N_NEGATIVES_PER_POSITIVE\n",
    "metadata['random_seed'] = RANDOM_SEED\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# QUICK SAMPLING FOR COLAB\n",
    "# ============================================\n",
    "# from torch.utils.data import Subset\n",
    "# import numpy as np\n",
    "\n",
    "print(\"\\n🚀 Applying quick sampling for faster training...\")\n",
    "\n",
    "# Sample 15% of data (keeping original for full dataset eval later)\n",
    "SAMPLE_RATIO = 0.15\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if sampling has already been applied (Subset type)\n",
    "if not isinstance(train_dataset, Subset):\n",
    "    train_size = int(len(train_dataset) * SAMPLE_RATIO)\n",
    "    train_indices = np.random.choice(len(train_dataset), size=train_size, replace=False)\n",
    "    train_dataset = Subset(train_dataset, train_indices)\n",
    "\n",
    "if not isinstance(val_dataset, Subset):\n",
    "    val_size = int(len(val_dataset) * SAMPLE_RATIO)\n",
    "    val_indices = np.random.choice(len(val_dataset), size=val_size, replace=False)\n",
    "    val_dataset = Subset(val_dataset, val_indices)\n",
    "\n",
    "print(f\"✅ Sampled to {SAMPLE_RATIO*100:.1f}% of data:\")\n",
    "print(f\"   Train: {len(train_dataset):,} samples\")\n",
    "print(f\"   Val: {len(val_dataset):,} samples\")\n",
    "print(f\"   Note: Test dataset remains full size for final evaluation.\")\n",
    "\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset):,}\")\n",
    "print(f\"Val dataset size: {len(val_dataset):,}\")\n",
    "print(f\"Test dataset size: {len(test_dataset):,}\")\n",
    "\n",
    "# ============================================\n",
    "# CREATE DATALOADERS (for verification)\n",
    "# ============================================\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"\\n Creating PyTorch DataLoaders for verification...\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "print(f\"Train loader batches: {len(train_loader):,}\")\n",
    "print(f\"Val loader batches: {len(val_loader):,}\")\n",
    "print(f\"Test loader batches: {len(test_loader):,}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# SAVE EVERYTHING\n",
    "# ============================================\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"\\n Saving processed data to {OUTPUT_DIR}...\")\n",
    "\n",
    "# Save datasets - CRITICAL: Don't skip this!\n",
    "# Save the Subset object if sampling is applied, otherwise save the full dataset\n",
    "print(f\"   Saving train_dataset.pt ({len(train_dataset):,} samples)...\")\n",
    "torch.save(train_dataset, f\"{OUTPUT_DIR}/train_dataset.pt\")\n",
    "print(\"   ✅ train_dataset.pt saved\")\n",
    "\n",
    "print(f\"   Saving val_dataset.pt ({len(val_dataset):,} samples)...\")\n",
    "torch.save(val_dataset, f\"{OUTPUT_DIR}/val_dataset.pt\")\n",
    "print(\"   ✅ val_dataset.pt saved\")\n",
    "\n",
    "print(f\"   Saving test_dataset.pt ({len(test_dataset):,} samples)...\")\n",
    "torch.save(test_dataset, f\"{OUTPUT_DIR}/test_dataset.pt\")\n",
    "print(\"   ✅ test_dataset.pt saved\")\n",
    "\n",
    "# Save mappings\n",
    "print(\"   Saving mappings.pkl...\")\n",
    "with open(f\"{OUTPUT_DIR}/mappings.pkl\", 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'user_id_to_idx': user_id_to_idx,\n",
    "        'recipe_id_to_idx': recipe_id_to_idx,\n",
    "        'user_feature_cols': user_feature_cols_list, # Save the actual list of cols\n",
    "        'recipe_feature_cols': recipe_feature_cols_list, # Save the actual list of cols\n",
    "        'n_users': len(all_user_ids),\n",
    "        'n_recipes': len(all_recipe_ids),\n",
    "        'user_feature_dim': metadata['user_feature_dim'], # Use updated dim from metadata\n",
    "        'recipe_feature_dim': metadata['recipe_feature_dim'] # Use updated dim from metadata\n",
    "    }, f)\n",
    "print(\"   ✅ mappings.pkl saved\")\n",
    "\n",
    "\n",
    "# Save metadata\n",
    "print(\"   Saving metadata.pkl...\")\n",
    "metadata['n_train_samples_sampled'] = len(train_dataset) # Add sampled sizes\n",
    "metadata['n_val_samples_sampled'] = len(val_dataset)\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/metadata.pkl\", 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(\"   ✅ metadata.pkl saved\")\n",
    "\n",
    "\n",
    "print(\"\\n✅ Data preparation complete!\")\n",
    "print(\"\\n Summary:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"   {key}: {value:,}\" if isinstance(value, (int, float)) and value > 100 else f\"   {key}: {value}\")\n",
    "\n",
    "\n",
    "print(f\"\\n Saved files:\")\n",
    "print(f\"   ✅ train_dataset.pt ({len(train_dataset):,} samples)\")\n",
    "print(f\"   ✅ val_dataset.pt ({len(val_dataset):,} samples)\")\n",
    "print(f\"   ✅ test_dataset.pt ({len(test_dataset):,} samples)\")\n",
    "print(f\"   ✅ mappings.pkl\")\n",
    "print(f\"   ✅ metadata.pkl\")\n",
    "\n",
    "print(\"\\n🚀 Ready to train! Use the training script to load and train.\")\n",
    "\n",
    "# ============================================\n",
    "# QUICK VERIFICATION\n",
    "# ============================================\n",
    "print(\"\\n🔍 Quick verification:\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"Sample item structure:\")\n",
    "for key, value in sample.items():\n",
    "    if torch.is_tensor(value):\n",
    "        print(f\"   {key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "# Verify a batch\n",
    "print(f\"\\n🔍 Verifying batch loading...\")\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"Batch structure:\")\n",
    "for key, value in sample_batch.items():\n",
    "    print(f\"   {key}: shape={value.shape}, dtype={value.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5566a7c3",
   "metadata": {
    "id": "5566a7c3"
   },
   "source": [
    "## 1️6. Train Two-Tower Model\n",
    "\n",
    "Train the neural network with:\n",
    "- **Binary Cross-Entropy Loss** (positive vs negative samples)\n",
    "- **Adam optimizer** with learning rate scheduling\n",
    "- **Validation monitoring** to prevent overfitting\n",
    "- **Early stopping** if validation loss doesn't improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_WR7HLxNwPm7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_WR7HLxNwPm7",
    "outputId": "c64cdda6-88f8-4883-e86b-cf6ea1226bbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.8.0+cu126)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
      "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
      "Installing collected packages: lightning-utilities, torchmetrics\n",
      "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n",
      "1.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U torchmetrics\n",
    "import torchmetrics\n",
    "print(torchmetrics.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baabc9bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "baabc9bc",
    "outputId": "37dd38e5-a608-4364-b57f-cda30b75f2ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using device: cuda\n",
      "   GPU: Tesla T4\n",
      "   Memory: 14.7 GB\n",
      "\n",
      " Loading preprocessed data...\n",
      "✅ Data loaded in 6.2s\n",
      "\n",
      "📊 Dataset info:\n",
      "   n_users: 5,000\n",
      "   n_recipes: 10,437\n",
      "   user_feature_dim: 20\n",
      "   recipe_feature_dim: 63\n",
      "   negative_sampling_ratio: 4\n",
      "   random_seed: 42\n",
      "   n_train_samples_sampled: 689,291\n",
      "   n_val_samples_sampled: 148,718\n",
      "\n",
      " Creating DataLoaders...\n",
      "Train batches: 674\n",
      "Val batches: 146\n",
      "Test batches: 980\n",
      "\n",
      "  Estimated time per epoch: 0.8 minutes\n",
      "   Total estimated time (20 epochs): 0.3 hours\n",
      "\n",
      "  Initializing model...\n",
      "\n",
      "📐 Model architecture:\n",
      "   Users: 5,000\n",
      "   Recipes: 10,437\n",
      "   User features: 20\n",
      "   Recipe features: 63\n",
      "   Embedding dim: 128\n",
      "   Hidden dim: 256\n",
      "\n",
      "   Total parameters: 2,260,608\n",
      "   Trainable parameters: 2,260,608\n",
      "\n",
      "🔍 Verifying class balance...\n",
      "Sample batch shapes:\n",
      "   user_id: torch.Size([1024])\n",
      "   recipe_id: torch.Size([1024])\n",
      "   user_features: torch.Size([1024, 20])\n",
      "   recipe_features: torch.Size([1024, 63])\n",
      "   target: torch.Size([1024])\n",
      "\n",
      "Sample batch balance: 235.0 pos, 789.0 neg (22.9% positive)\n",
      "\n",
      "  Using weighted BCE (pos_weight=4.00)\n",
      "\n",
      "🚀 Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 01/20 [Train]: 100%|████████████████████| 674/674 [01:25<00:00,  7.84it/s, loss=1.0777, acc=0.619]\n",
      "Epoch 01/20 [Val]  : 100%|████████████████████| 146/146 [00:18<00:00,  8.09it/s, loss=0.9952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Epoch 01/20 Summary:\n",
      "   Train - Loss: 1.0794, Acc: 0.577\n",
      "   Val   - Loss: 1.0371, Acc: 0.651\n",
      "   Metrics - P@5: 0.3343, P@10: 0.2979, P@20: 0.2455\n",
      "           - R@10: 0.5692, R@20: 0.8336, NDCG: 0.6468\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.8m | Elapsed: 1.8m | ETA: 34.3m\n",
      "\n",
      "📊 Epoch 01/20 Summary:\n",
      "   Train - Loss: 1.0794, Acc: 0.577\n",
      "   Val   - Loss: 1.0371, Acc: 0.651\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.8m | Elapsed: 1.8m | ETA: 35.0m\n",
      "   ✅ Model saved! (Loss: 1.0371, NDCG: 0.6468)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 02/20 [Train]: 100%|████████████████████| 674/674 [01:28<00:00,  7.66it/s, loss=0.9639, acc=0.604]\n",
      "Epoch 02/20 [Val]  : 100%|████████████████████| 146/146 [00:17<00:00,  8.19it/s, loss=0.9913]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Epoch 02/20 Summary:\n",
      "   Train - Loss: 1.0167, Acc: 0.630\n",
      "   Val   - Loss: 1.0028, Acc: 0.616\n",
      "   Metrics - P@5: 0.3450, P@10: 0.3161, P@20: 0.2552\n",
      "           - R@10: 0.6051, R@20: 0.8613, NDCG: 0.6544\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.8m | Elapsed: 3.7m | ETA: 32.9m\n",
      "\n",
      "📊 Epoch 02/20 Summary:\n",
      "   Train - Loss: 1.0167, Acc: 0.630\n",
      "   Val   - Loss: 1.0028, Acc: 0.616\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.9m | Elapsed: 3.7m | ETA: 33.6m\n",
      "   ✅ Model saved! (Loss: 1.0028, NDCG: 0.6544)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 03/20 [Train]: 100%|████████████████████| 674/674 [01:27<00:00,  7.66it/s, loss=0.9999, acc=0.662]\n",
      "Epoch 03/20 [Val]  : 100%|████████████████████| 146/146 [00:17<00:00,  8.12it/s, loss=0.9903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Epoch 03/20 Summary:\n",
      "   Train - Loss: 0.9868, Acc: 0.639\n",
      "   Val   - Loss: 0.9886, Acc: 0.627\n",
      "   Metrics - P@5: 0.3474, P@10: 0.3195, P@20: 0.2582\n",
      "           - R@10: 0.6130, R@20: 0.8686, NDCG: 0.6572\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.8m | Elapsed: 5.5m | ETA: 31.1m\n",
      "\n",
      "📊 Epoch 03/20 Summary:\n",
      "   Train - Loss: 0.9868, Acc: 0.639\n",
      "   Val   - Loss: 0.9886, Acc: 0.627\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.9m | Elapsed: 5.6m | ETA: 31.8m\n",
      "   ✅ Model saved! (Loss: 0.9886, NDCG: 0.6572)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 04/20 [Train]: 100%|████████████████████| 674/674 [01:26<00:00,  7.77it/s, loss=1.0023, acc=0.647]\n",
      "Epoch 04/20 [Val]  : 100%|████████████████████| 146/146 [00:17<00:00,  8.23it/s, loss=0.9818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Epoch 04/20 Summary:\n",
      "   Train - Loss: 0.9684, Acc: 0.640\n",
      "   Val   - Loss: 0.9793, Acc: 0.635\n",
      "   Metrics - P@5: 0.3500, P@10: 0.3226, P@20: 0.2589\n",
      "           - R@10: 0.6181, R@20: 0.8708, NDCG: 0.6605\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.8m | Elapsed: 7.4m | ETA: 28.7m\n",
      "\n",
      "📊 Epoch 04/20 Summary:\n",
      "   Train - Loss: 0.9684, Acc: 0.640\n",
      "   Val   - Loss: 0.9793, Acc: 0.635\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.8m | Elapsed: 7.4m | ETA: 29.5m\n",
      "   ✅ Model saved! (Loss: 0.9793, NDCG: 0.6605)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 05/20 [Train]: 100%|████████████████████| 674/674 [01:27<00:00,  7.73it/s, loss=0.9558, acc=0.691]\n",
      "Epoch 05/20 [Val]  : 100%|████████████████████| 146/146 [00:17<00:00,  8.15it/s, loss=0.9827]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Epoch 05/20 Summary:\n",
      "   Train - Loss: 0.9564, Acc: 0.642\n",
      "   Val   - Loss: 0.9751, Acc: 0.626\n",
      "   Metrics - P@5: 0.3549, P@10: 0.3235, P@20: 0.2595\n",
      "           - R@10: 0.6201, R@20: 0.8720, NDCG: 0.6596\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.8m | Elapsed: 9.2m | ETA: 27.0m\n",
      "\n",
      "📊 Epoch 05/20 Summary:\n",
      "   Train - Loss: 0.9564, Acc: 0.642\n",
      "   Val   - Loss: 0.9751, Acc: 0.626\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.8m | Elapsed: 9.3m | ETA: 27.7m\n",
      "   ✅ Model saved! (Loss: 0.9751, NDCG: 0.6605)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 06/20 [Train]: 100%|████████████████████| 674/674 [01:27<00:00,  7.69it/s, loss=0.9990, acc=0.619]\n",
      "Epoch 06/20 [Val]  : 100%|████████████████████| 146/146 [00:17<00:00,  8.15it/s, loss=0.9744]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Epoch 06/20 Summary:\n",
      "   Train - Loss: 0.9465, Acc: 0.645\n",
      "   Val   - Loss: 0.9690, Acc: 0.621\n",
      "   Metrics - P@5: 0.3514, P@10: 0.3250, P@20: 0.2598\n",
      "           - R@10: 0.6232, R@20: 0.8733, NDCG: 0.6617\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.8m | Elapsed: 11.1m | ETA: 25.3m\n",
      "\n",
      "📊 Epoch 06/20 Summary:\n",
      "   Train - Loss: 0.9465, Acc: 0.645\n",
      "   Val   - Loss: 0.9690, Acc: 0.621\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.9m | Elapsed: 11.1m | ETA: 26.0m\n",
      "   ✅ Model saved! (Loss: 0.9690, NDCG: 0.6617)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 07/20 [Train]: 100%|████████████████████| 674/674 [01:25<00:00,  7.86it/s, loss=0.9098, acc=0.626]\n",
      "Epoch 07/20 [Val]  : 100%|████████████████████| 146/146 [00:17<00:00,  8.31it/s, loss=0.9732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Epoch 07/20 Summary:\n",
      "   Train - Loss: 0.9344, Acc: 0.649\n",
      "   Val   - Loss: 0.9572, Acc: 0.639\n",
      "   Metrics - P@5: 0.3563, P@10: 0.3279, P@20: 0.2624\n",
      "           - R@10: 0.6294, R@20: 0.8817, NDCG: 0.6653\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.8m | Elapsed: 12.9m | ETA: 23.0m\n",
      "\n",
      "📊 Epoch 07/20 Summary:\n",
      "   Train - Loss: 0.9344, Acc: 0.649\n",
      "   Val   - Loss: 0.9572, Acc: 0.639\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.8m | Elapsed: 12.9m | ETA: 23.5m\n",
      "   ✅ Model saved! (Loss: 0.9572, NDCG: 0.6653)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 08/20 [Train]: 100%|████████████████████| 674/674 [01:26<00:00,  7.79it/s, loss=0.8956, acc=0.576]\n",
      "Epoch 08/20 [Val]  : 100%|████████████████████| 146/146 [00:18<00:00,  7.74it/s, loss=0.9496]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Epoch 08/20 Summary:\n",
      "   Train - Loss: 0.9235, Acc: 0.652\n",
      "   Val   - Loss: 0.9509, Acc: 0.640\n",
      "   Metrics - P@5: 0.3624, P@10: 0.3311, P@20: 0.2634\n",
      "           - R@10: 0.6336, R@20: 0.8845, NDCG: 0.6683\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.8m | Elapsed: 14.7m | ETA: 21.7m\n",
      "\n",
      "📊 Epoch 08/20 Summary:\n",
      "   Train - Loss: 0.9235, Acc: 0.652\n",
      "   Val   - Loss: 0.9509, Acc: 0.640\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.8m | Elapsed: 14.8m | ETA: 22.1m\n",
      "   ✅ Model saved! (Loss: 0.9509, NDCG: 0.6683)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 09/20 [Train]: 100%|████████████████████| 674/674 [01:26<00:00,  7.78it/s, loss=0.9707, acc=0.691]\n",
      "Epoch 09/20 [Val]  : 100%|████████████████████| 146/146 [00:18<00:00,  7.92it/s, loss=0.9413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Epoch 09/20 Summary:\n",
      "   Train - Loss: 0.9149, Acc: 0.652\n",
      "   Val   - Loss: 0.9509, Acc: 0.639\n",
      "   Metrics - P@5: 0.3594, P@10: 0.3296, P@20: 0.2622\n",
      "           - R@10: 0.6317, R@20: 0.8812, NDCG: 0.6679\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.8m | Elapsed: 16.6m | ETA: 19.8m\n",
      "\n",
      "📊 Epoch 09/20 Summary:\n",
      "   Train - Loss: 0.9149, Acc: 0.652\n",
      "   Val   - Loss: 0.9509, Acc: 0.639\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.8m | Elapsed: 16.6m | ETA: 20.3m\n",
      "    No improvement (1/4)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [Train]: 100%|████████████████████| 674/674 [01:26<00:00,  7.78it/s, loss=0.8519, acc=0.727]\n",
      "Epoch 10/20 [Val]  : 100%|████████████████████| 146/146 [00:17<00:00,  8.23it/s, loss=0.9552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Epoch 10/20 Summary:\n",
      "   Train - Loss: 0.9091, Acc: 0.652\n",
      "   Val   - Loss: 0.9476, Acc: 0.640\n",
      "   Metrics - P@5: 0.3625, P@10: 0.3315, P@20: 0.2641\n",
      "           - R@10: 0.6357, R@20: 0.8868, NDCG: 0.6692\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.8m | Elapsed: 18.4m | ETA: 18.0m\n",
      "\n",
      "📊 Epoch 10/20 Summary:\n",
      "   Train - Loss: 0.9091, Acc: 0.652\n",
      "   Val   - Loss: 0.9476, Acc: 0.640\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.8m | Elapsed: 18.5m | ETA: 18.5m\n",
      "   ✅ Model saved! (Loss: 0.9476, NDCG: 0.6692)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [Train]: 100%|████████████████████| 674/674 [01:25<00:00,  7.92it/s, loss=0.9387, acc=0.647]\n",
      "Epoch 11/20 [Val]  : 100%|████████████████████| 146/146 [00:17<00:00,  8.30it/s, loss=0.9301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Epoch 11/20 Summary:\n",
      "   Train - Loss: 0.9033, Acc: 0.652\n",
      "   Val   - Loss: 0.9493, Acc: 0.638\n",
      "   Metrics - P@5: 0.3624, P@10: 0.3317, P@20: 0.2642\n",
      "           - R@10: 0.6364, R@20: 0.8858, NDCG: 0.6706\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.8m | Elapsed: 20.2m | ETA: 15.8m\n",
      "\n",
      "📊 Epoch 11/20 Summary:\n",
      "   Train - Loss: 0.9033, Acc: 0.652\n",
      "   Val   - Loss: 0.9493, Acc: 0.638\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.8m | Elapsed: 20.3m | ETA: 16.3m\n",
      "   ✅ Model saved! (Loss: 0.9476, NDCG: 0.6706)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [Train]: 100%|████████████████████| 674/674 [01:25<00:00,  7.85it/s, loss=0.8963, acc=0.712]\n",
      "Epoch 12/20 [Val]  : 100%|████████████████████| 146/146 [00:17<00:00,  8.30it/s, loss=0.9838]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Epoch 12/20 Summary:\n",
      "   Train - Loss: 0.8980, Acc: 0.652\n",
      "   Val   - Loss: 0.9537, Acc: 0.645\n",
      "   Metrics - P@5: 0.3623, P@10: 0.3314, P@20: 0.2641\n",
      "           - R@10: 0.6361, R@20: 0.8855, NDCG: 0.6706\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.8m | Elapsed: 22.0m | ETA: 14.2m\n",
      "\n",
      "📊 Epoch 12/20 Summary:\n",
      "   Train - Loss: 0.8980, Acc: 0.652\n",
      "   Val   - Loss: 0.9537, Acc: 0.645\n",
      "   LR: 0.001000\n",
      "    Epoch time: 1.8m | Elapsed: 22.1m | ETA: 14.5m\n",
      "    No improvement (1/4)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 [Train]: 100%|████████████████████| 674/674 [01:25<00:00,  7.86it/s, loss=0.8592, acc=0.647]\n",
      "Epoch 13/20 [Val]  : 100%|████████████████████| 146/146 [00:18<00:00,  7.76it/s, loss=1.0134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Epoch 13/20 Summary:\n",
      "   Train - Loss: 0.8931, Acc: 0.653\n",
      "   Val   - Loss: 0.9560, Acc: 0.635\n",
      "   Metrics - P@5: 0.3622, P@10: 0.3324, P@20: 0.2630\n",
      "           - R@10: 0.6358, R@20: 0.8820, NDCG: 0.6692\n",
      "   LR: 0.000500\n",
      "    Epoch time: 1.8m | Elapsed: 23.9m | ETA: 12.6m\n",
      "\n",
      "📊 Epoch 13/20 Summary:\n",
      "   Train - Loss: 0.8931, Acc: 0.653\n",
      "   Val   - Loss: 0.9560, Acc: 0.635\n",
      "   LR: 0.000500\n",
      "    Epoch time: 1.8m | Elapsed: 23.9m | ETA: 12.8m\n",
      "    No improvement (2/4)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 [Train]: 100%|████████████████████| 674/674 [01:25<00:00,  7.87it/s, loss=0.9667, acc=0.612]\n",
      "Epoch 14/20 [Val]  : 100%|████████████████████| 146/146 [00:18<00:00,  7.71it/s, loss=0.9844]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Epoch 14/20 Summary:\n",
      "   Train - Loss: 0.8700, Acc: 0.661\n",
      "   Val   - Loss: 0.9685, Acc: 0.641\n",
      "   Metrics - P@5: 0.3649, P@10: 0.3320, P@20: 0.2638\n",
      "           - R@10: 0.6354, R@20: 0.8846, NDCG: 0.6709\n",
      "   LR: 0.000500\n",
      "    Epoch time: 1.8m | Elapsed: 25.7m | ETA: 10.8m\n",
      "\n",
      "📊 Epoch 14/20 Summary:\n",
      "   Train - Loss: 0.8700, Acc: 0.661\n",
      "   Val   - Loss: 0.9685, Acc: 0.641\n",
      "   LR: 0.000500\n",
      "    Epoch time: 1.8m | Elapsed: 25.8m | ETA: 11.0m\n",
      "   ✅ Model saved! (Loss: 0.9476, NDCG: 0.6709)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 [Train]: 100%|████████████████████| 674/674 [01:27<00:00,  7.70it/s, loss=0.7587, acc=0.727]\n",
      "Epoch 15/20 [Val]  : 100%|████████████████████| 146/146 [00:17<00:00,  8.14it/s, loss=0.9792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Epoch 15/20 Summary:\n",
      "   Train - Loss: 0.8623, Acc: 0.662\n",
      "   Val   - Loss: 0.9742, Acc: 0.643\n",
      "   Metrics - P@5: 0.3638, P@10: 0.3303, P@20: 0.2637\n",
      "           - R@10: 0.6337, R@20: 0.8851, NDCG: 0.6697\n",
      "   LR: 0.000500\n",
      "    Epoch time: 1.8m | Elapsed: 27.6m | ETA: 9.1m\n",
      "\n",
      "📊 Epoch 15/20 Summary:\n",
      "   Train - Loss: 0.8623, Acc: 0.662\n",
      "   Val   - Loss: 0.9742, Acc: 0.643\n",
      "   LR: 0.000500\n",
      "    Epoch time: 1.9m | Elapsed: 27.6m | ETA: 9.3m\n",
      "    No improvement (1/4)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 [Train]: 100%|████████████████████| 674/674 [01:25<00:00,  7.86it/s, loss=1.1383, acc=0.590]\n",
      "Epoch 16/20 [Val]  : 100%|████████████████████| 146/146 [00:18<00:00,  8.11it/s, loss=0.9699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Epoch 16/20 Summary:\n",
      "   Train - Loss: 0.8576, Acc: 0.662\n",
      "   Val   - Loss: 0.9837, Acc: 0.649\n",
      "   Metrics - P@5: 0.3620, P@10: 0.3302, P@20: 0.2636\n",
      "           - R@10: 0.6327, R@20: 0.8842, NDCG: 0.6673\n",
      "   LR: 0.000250\n",
      "    Epoch time: 1.8m | Elapsed: 29.4m | ETA: 7.1m\n",
      "\n",
      "📊 Epoch 16/20 Summary:\n",
      "   Train - Loss: 0.8576, Acc: 0.662\n",
      "   Val   - Loss: 0.9837, Acc: 0.649\n",
      "   LR: 0.000250\n",
      "    Epoch time: 1.8m | Elapsed: 29.4m | ETA: 7.3m\n",
      "    No improvement (2/4)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 [Train]: 100%|████████████████████| 674/674 [01:26<00:00,  7.83it/s, loss=0.9650, acc=0.647]\n",
      "Epoch 17/20 [Val]  : 100%|████████████████████| 146/146 [00:16<00:00,  8.74it/s, loss=1.0133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Epoch 17/20 Summary:\n",
      "   Train - Loss: 0.8400, Acc: 0.668\n",
      "   Val   - Loss: 1.0171, Acc: 0.644\n",
      "   Metrics - P@5: 0.3602, P@10: 0.3294, P@20: 0.2630\n",
      "           - R@10: 0.6319, R@20: 0.8830, NDCG: 0.6675\n",
      "   LR: 0.000250\n",
      "    Epoch time: 1.8m | Elapsed: 31.2m | ETA: 5.3m\n",
      "\n",
      "📊 Epoch 17/20 Summary:\n",
      "   Train - Loss: 0.8400, Acc: 0.668\n",
      "   Val   - Loss: 1.0171, Acc: 0.644\n",
      "   LR: 0.000250\n",
      "    Epoch time: 1.8m | Elapsed: 31.3m | ETA: 5.4m\n",
      "    No improvement (3/4)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 [Train]: 100%|████████████████████| 674/674 [01:25<00:00,  7.86it/s, loss=0.9001, acc=0.633]\n",
      "Epoch 18/20 [Val]  : 100%|████████████████████| 146/146 [00:18<00:00,  7.96it/s, loss=1.0222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Epoch 18/20 Summary:\n",
      "   Train - Loss: 0.8355, Acc: 0.669\n",
      "   Val   - Loss: 1.0321, Acc: 0.640\n",
      "   Metrics - P@5: 0.3581, P@10: 0.3280, P@20: 0.2624\n",
      "           - R@10: 0.6290, R@20: 0.8815, NDCG: 0.6669\n",
      "   LR: 0.000250\n",
      "    Epoch time: 1.8m | Elapsed: 33.0m | ETA: 3.6m\n",
      "\n",
      "📊 Epoch 18/20 Summary:\n",
      "   Train - Loss: 0.8355, Acc: 0.669\n",
      "   Val   - Loss: 1.0321, Acc: 0.640\n",
      "   LR: 0.000250\n",
      "    Epoch time: 1.8m | Elapsed: 33.1m | ETA: 3.6m\n",
      "    No improvement (4/4)\n",
      "\n",
      "  Early stopping triggered at epoch 18\n",
      "\n",
      "✅ Training complete in 33.1 minutes (0.55 hours)!\n",
      "   Best Val Loss: 0.9476\n",
      "   Best NDCG: 0.6709\n",
      "\n",
      "✅ Best model loaded for inference\n",
      "✅ Model saved to: /content/drive/MyDrive/RecipeML/processed/best_two_tower_model.pth\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Two-Tower Model Training Script - OPTIMIZED FOR COLAB FREE\n",
    "Loads preprocessed data and trains the model\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics.retrieval import RetrievalPrecision, RetrievalNormalizedDCG\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION - OPTIMIZED FOR COLAB\n",
    "# ============================================\n",
    "PROCESSED_DATA_DIR = \"/content/drive/MyDrive/RecipeML/processed\"\n",
    "BATCH_SIZE = 1024  # 🚀 Doubled from 512 for faster training\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 20\n",
    "PATIENCE = 4\n",
    "NUM_WORKERS = 2\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"  Using device: {device}\")\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# ============================================\n",
    "# LOAD PREPROCESSED DATA\n",
    "# ============================================\n",
    "print(\"\\n Loading preprocessed data...\")\n",
    "start_time = time.time()\n",
    "\n",
    "train_dataset = torch.load(f\"{PROCESSED_DATA_DIR}/train_dataset.pt\", weights_only=False)\n",
    "val_dataset = torch.load(f\"{PROCESSED_DATA_DIR}/val_dataset.pt\", weights_only=False)\n",
    "test_dataset = torch.load(f\"{PROCESSED_DATA_DIR}/test_dataset.pt\", weights_only=False)\n",
    "\n",
    "with open(f\"{PROCESSED_DATA_DIR}/mappings.pkl\", 'rb') as f:\n",
    "    mappings = pickle.load(f)\n",
    "\n",
    "with open(f\"{PROCESSED_DATA_DIR}/metadata.pkl\", 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"✅ Data loaded in {load_time:.1f}s\")\n",
    "\n",
    "print(\"\\n📊 Dataset info:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"   {key}: {value:,}\" if isinstance(value, int) else f\"   {key}: {value}\")\n",
    "\n",
    "# ============================================\n",
    "# CREATE DATALOADERS\n",
    "# ============================================\n",
    "print(\"\\n Creating DataLoaders...\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True if NUM_WORKERS > 0 else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True if NUM_WORKERS > 0 else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True if NUM_WORKERS > 0 else False\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader):,}\")\n",
    "print(f\"Val batches: {len(val_loader):,}\")\n",
    "print(f\"Test batches: {len(test_loader):,}\")\n",
    "\n",
    "# Estimate training time\n",
    "samples_per_sec = 15000  # Approximate on T4 GPU with batch size 1024\n",
    "time_per_epoch = len(train_dataset) / samples_per_sec / 60  # in minutes\n",
    "print(f\"\\n  Estimated time per epoch: {time_per_epoch:.1f} minutes\")\n",
    "print(f\"   Total estimated time (20 epochs): {time_per_epoch * 20 / 60:.1f} hours\")\n",
    "\n",
    "# ============================================\n",
    "# DEFINE TWO-TOWER MODEL\n",
    "# ============================================\n",
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self, n_users, user_feature_dim, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + user_feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, user_ids, user_features):\n",
    "        user_emb = self.user_embedding(user_ids)\n",
    "        combined = torch.cat([user_emb, user_features], dim=-1)\n",
    "        return self.fc(combined)\n",
    "\n",
    "class RecipeEncoder(nn.Module):\n",
    "    def __init__(self, n_recipes, recipe_feature_dim, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.recipe_embedding = nn.Embedding(n_recipes, embedding_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + recipe_feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, recipe_ids, recipe_features):\n",
    "        recipe_emb = self.recipe_embedding(recipe_ids)\n",
    "        combined = torch.cat([recipe_emb, recipe_features], dim=-1)\n",
    "        return self.fc(combined)\n",
    "\n",
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, n_users, n_recipes, user_feature_dim, recipe_feature_dim,\n",
    "                 embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.user_encoder = UserEncoder(n_users, user_feature_dim, embedding_dim, hidden_dim)\n",
    "        self.recipe_encoder = RecipeEncoder(n_recipes, recipe_feature_dim, embedding_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, user_ids, user_features, recipe_ids, recipe_features):\n",
    "        user_emb = self.user_encoder(user_ids, user_features)\n",
    "        recipe_emb = self.recipe_encoder(recipe_ids, recipe_features)\n",
    "        scores = (user_emb * recipe_emb).sum(dim=-1)\n",
    "        return scores\n",
    "\n",
    "# ============================================\n",
    "# INITIALIZE MODEL\n",
    "# ============================================\n",
    "print(\"\\n  Initializing model...\")\n",
    "\n",
    "model = TwoTowerModel(\n",
    "    n_users=metadata['n_users'],\n",
    "    n_recipes=metadata['n_recipes'],\n",
    "    user_feature_dim=metadata['user_feature_dim'],\n",
    "    recipe_feature_dim=metadata['recipe_feature_dim'],\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\n📐 Model architecture:\")\n",
    "print(f\"   Users: {metadata['n_users']:,}\")\n",
    "print(f\"   Recipes: {metadata['n_recipes']:,}\")\n",
    "print(f\"   User features: {metadata['user_feature_dim']}\")\n",
    "print(f\"   Recipe features: {metadata['recipe_feature_dim']}\")\n",
    "print(f\"   Embedding dim: {EMBEDDING_DIM}\")\n",
    "print(f\"   Hidden dim: {HIDDEN_DIM}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\n   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# ============================================\n",
    "# DATA DIAGNOSTICS\n",
    "# ============================================\n",
    "print(\"\\n🔍 Verifying class balance...\")\n",
    "\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"Sample batch shapes:\")\n",
    "print(f\"   user_id: {sample_batch['user_id'].shape}\")\n",
    "print(f\"   recipe_id: {sample_batch['recipe_id'].shape}\")\n",
    "print(f\"   user_features: {sample_batch['user_features'].shape}\")\n",
    "print(f\"   recipe_features: {sample_batch['recipe_features'].shape}\")\n",
    "print(f\"   target: {sample_batch['target'].shape}\")\n",
    "\n",
    "pos_count = sample_batch['target'].sum().item()\n",
    "neg_count = len(sample_batch['target']) - pos_count\n",
    "print(f\"\\nSample batch balance: {pos_count} pos, {neg_count} neg ({pos_count/(pos_count+neg_count)*100:.1f}% positive)\")\n",
    "\n",
    "# ============================================\n",
    "# LOSS & OPTIMIZER\n",
    "# ============================================\n",
    "pos_ratio = 0.20\n",
    "if pos_ratio < 0.25:\n",
    "    pos_weight = torch.tensor([0.80 / 0.20]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    print(f\"\\n  Using weighted BCE (pos_weight={pos_weight.item():.2f})\")\n",
    "else:\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    print(f\"\\n  Using standard BCE\")\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "# ============================================\n",
    "# METRICS\n",
    "# ============================================\n",
    "retrieval_precision = RetrievalPrecision()\n",
    "retrieval_ndcg = RetrievalNormalizedDCG()\n",
    "\n",
    "# ============================================\n",
    "# TRAINING UTILITIES\n",
    "# ============================================\n",
    "def move_to_device(batch, device):\n",
    "    return {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "\n",
    "def compute_accuracy(outputs, targets):\n",
    "    preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "    return (preds == targets).float().mean().item()\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "writer = SummaryWriter(log_dir=\"runs/two_tower_optimized\")\n",
    "\n",
    "# ============================================\n",
    "# TRAINING LOOP WITH PROGRESS BARS\n",
    "# ============================================\n",
    "print(\"\\n🚀 Starting training...\\n\")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_val_ndcg = 0.0\n",
    "patience_counter = 0\n",
    "training_start_time = time.time()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    # ========== TRAIN ==========\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "\n",
    "    # Progress bar for training\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch:02d}/{EPOCHS} [Train]\",\n",
    "                      bar_format='{l_bar}{bar:20}{r_bar}')\n",
    "\n",
    "    for batch in train_pbar:\n",
    "        batch = move_to_device(batch, device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(\n",
    "                batch['user_id'], batch['user_features'],\n",
    "                batch['recipe_id'], batch['recipe_features']\n",
    "            )\n",
    "            loss = criterion(outputs, batch['target'])\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += compute_accuracy(outputs, batch['target'])\n",
    "\n",
    "        # Update progress bar\n",
    "        train_pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{compute_accuracy(outputs, batch[\"target\"]):.3f}'\n",
    "        })\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_train_acc = train_acc / len(train_loader)\n",
    "\n",
    "    # ========== VALIDATION ==========\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    all_preds, all_targets, all_user_ids = [], [], []\n",
    "\n",
    "    # Progress bar for validation\n",
    "    val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch:02d}/{EPOCHS} [Val]  \",\n",
    "                    bar_format='{l_bar}{bar:20}{r_bar}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_pbar:\n",
    "            batch = move_to_device(batch, device)\n",
    "\n",
    "            outputs = model(\n",
    "                batch['user_id'], batch['user_features'],\n",
    "                batch['recipe_id'], batch['recipe_features']\n",
    "            )\n",
    "            loss = criterion(outputs, batch['target'])\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_acc += compute_accuracy(outputs, batch['target'])\n",
    "\n",
    "            preds = torch.sigmoid(outputs)\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_targets.append(batch['target'].cpu())\n",
    "            all_user_ids.append(batch['user_id'].cpu())\n",
    "\n",
    "            # Update progress bar\n",
    "            val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_val_acc = val_acc / len(val_loader)\n",
    "\n",
    "     # Compute retrieval metrics\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "    all_user_ids = torch.cat(all_user_ids)\n",
    "\n",
    "    # Compute corrected P@K and R@K metrics\n",
    "    import numpy as np\n",
    "\n",
    "    def compute_precision_at_k_correct(preds, targets, user_ids, k=10):\n",
    "        preds_np = preds.cpu().numpy()\n",
    "        targets_np = targets.cpu().numpy()\n",
    "        user_ids_np = user_ids.cpu().numpy()\n",
    "\n",
    "        user_metrics = {}\n",
    "        for i in range(len(user_ids_np)):\n",
    "            uid = user_ids_np[i]\n",
    "            if uid not in user_metrics:\n",
    "                user_metrics[uid] = {'preds': [], 'targets': []}\n",
    "            user_metrics[uid]['preds'].append(preds_np[i])\n",
    "            user_metrics[uid]['targets'].append(targets_np[i])\n",
    "\n",
    "        precision_scores = []\n",
    "        for uid, data in user_metrics.items():\n",
    "            preds_user = np.array(data['preds'])\n",
    "            targets_user = np.array(data['targets'])\n",
    "\n",
    "            if len(preds_user) >= k:\n",
    "                top_k_idx = np.argsort(preds_user)[-k:]\n",
    "                hits = targets_user[top_k_idx].sum()\n",
    "                precision_scores.append(hits / k)\n",
    "\n",
    "        return np.mean(precision_scores) if precision_scores else 0.0\n",
    "\n",
    "    def compute_recall_at_k(preds, targets, user_ids, k=10):\n",
    "        preds_np = preds.cpu().numpy()\n",
    "        targets_np = targets.cpu().numpy()\n",
    "        user_ids_np = user_ids.cpu().numpy()\n",
    "\n",
    "        user_metrics = {}\n",
    "        for i in range(len(user_ids_np)):\n",
    "            uid = user_ids_np[i]\n",
    "            if uid not in user_metrics:\n",
    "                user_metrics[uid] = {'preds': [], 'targets': []}\n",
    "            user_metrics[uid]['preds'].append(preds_np[i])\n",
    "            user_metrics[uid]['targets'].append(targets_np[i])\n",
    "\n",
    "        recall_scores = []\n",
    "        for uid, data in user_metrics.items():\n",
    "            preds_user = np.array(data['preds'])\n",
    "            targets_user = np.array(data['targets'])\n",
    "\n",
    "            total_relevant = targets_user.sum()\n",
    "            if total_relevant > 0 and len(preds_user) >= k:\n",
    "                top_k_idx = np.argsort(preds_user)[-k:]\n",
    "                hits = targets_user[top_k_idx].sum()\n",
    "                recall_scores.append(hits / total_relevant)\n",
    "\n",
    "        return np.mean(recall_scores) if recall_scores else 0.0\n",
    "\n",
    "    val_p_at_5 = compute_precision_at_k_correct(all_preds, all_targets, all_user_ids, k=5)\n",
    "    val_p_at_10 = compute_precision_at_k_correct(all_preds, all_targets, all_user_ids, k=10)\n",
    "    val_p_at_20 = compute_precision_at_k_correct(all_preds, all_targets, all_user_ids, k=20)\n",
    "    val_r_at_10 = compute_recall_at_k(all_preds, all_targets, all_user_ids, k=10)\n",
    "    val_r_at_20 = compute_recall_at_k(all_preds, all_targets, all_user_ids, k=20)\n",
    "    val_ndcg = retrieval_ndcg(all_preds, all_targets, indexes=all_user_ids).item()\n",
    "\n",
    "    # Scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "\n",
    "    # Epoch timing\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    elapsed_time = time.time() - training_start_time\n",
    "    eta = epoch_time * (EPOCHS - epoch)\n",
    "\n",
    "    print(f\"\\n📊 Epoch {epoch:02d}/{EPOCHS} Summary:\")\n",
    "    print(f\"   Train - Loss: {avg_train_loss:.4f}, Acc: {avg_train_acc:.3f}\")\n",
    "    print(f\"   Val   - Loss: {avg_val_loss:.4f}, Acc: {avg_val_acc:.3f}\")\n",
    "    print(f\"   Metrics - P@5: {val_p_at_5:.4f}, P@10: {val_p_at_10:.4f}, P@20: {val_p_at_20:.4f}\")\n",
    "    print(f\"           - R@10: {val_r_at_10:.4f}, R@20: {val_r_at_20:.4f}, NDCG: {val_ndcg:.4f}\")\n",
    "    print(f\"   LR: {current_lr:.6f}\")\n",
    "    print(f\"    Epoch time: {epoch_time/60:.1f}m | Elapsed: {elapsed_time/60:.1f}m | ETA: {eta/60:.1f}m\")\n",
    "\n",
    "    val_precision = retrieval_precision(all_preds, all_targets, indexes=all_user_ids).item()\n",
    "    val_ndcg = retrieval_ndcg(all_preds, all_targets, indexes=all_user_ids).item()\n",
    "\n",
    "    # Logging\n",
    "    writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Val', avg_val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/Train', avg_train_acc, epoch)\n",
    "    writer.add_scalar('Accuracy/Val', avg_val_acc, epoch)\n",
    "    writer.add_scalar('Metrics/P@5', val_p_at_5, epoch)\n",
    "    writer.add_scalar('Metrics/P@10', val_p_at_10, epoch)\n",
    "    writer.add_scalar('Metrics/P@20', val_p_at_20, epoch)\n",
    "    writer.add_scalar('Metrics/R@10', val_r_at_10, epoch)\n",
    "    writer.add_scalar('Metrics/R@20', val_r_at_20, epoch)\n",
    "    writer.add_scalar('Metrics/NDCG', val_ndcg, epoch)\n",
    "    writer.add_scalar('LR', current_lr, epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Epoch timing\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    elapsed_time = time.time() - training_start_time\n",
    "    eta = epoch_time * (EPOCHS - epoch)\n",
    "\n",
    "    print(f\"\\n📊 Epoch {epoch:02d}/{EPOCHS} Summary:\")\n",
    "    print(f\"   Train - Loss: {avg_train_loss:.4f}, Acc: {avg_train_acc:.3f}\")\n",
    "    print(f\"   Val   - Loss: {avg_val_loss:.4f}, Acc: {avg_val_acc:.3f}\")\n",
    "    print(f\"   LR: {current_lr:.6f}\")\n",
    "    print(f\"    Epoch time: {epoch_time/60:.1f}m | Elapsed: {elapsed_time/60:.1f}m | ETA: {eta/60:.1f}m\")\n",
    "\n",
    "    # Early stopping\n",
    "    improved = False\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        improved = True\n",
    "    if val_ndcg > best_val_ndcg:\n",
    "        best_val_ndcg = val_ndcg\n",
    "        improved = True\n",
    "\n",
    "    if improved:\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_two_tower_model.pth')\n",
    "        print(f\"   ✅ Model saved! (Loss: {best_val_loss:.4f}, NDCG: {best_val_ndcg:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"    No improvement ({patience_counter}/{PATIENCE})\")\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"\\n  Early stopping triggered at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "writer.close()\n",
    "\n",
    "# ============================================\n",
    "# FINAL EVALUATION\n",
    "# ============================================\n",
    "total_time = time.time() - training_start_time\n",
    "print(f\"\\n✅ Training complete in {total_time/60:.1f} minutes ({total_time/3600:.2f} hours)!\")\n",
    "print(f\"   Best Val Loss: {best_val_loss:.4f}\")\n",
    "print(f\"   Best NDCG: {best_val_ndcg:.4f}\")\n",
    "\n",
    "if os.path.exists('best_two_tower_model.pth'):\n",
    "    model.load_state_dict(torch.load('best_two_tower_model.pth', weights_only=True))\n",
    "    print(\"\\n✅ Best model loaded for inference\")\n",
    "\n",
    "# Save final model to Drive\n",
    "final_model_path = f\"{PROCESSED_DATA_DIR}/best_two_tower_model.pth\"\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"✅ Model saved to: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c6abda",
   "metadata": {
    "id": "16c6abda"
   },
   "source": [
    "## 17. Evaluate Two-Tower on Validation Set 📊\n",
    "\n",
    "Generate recommendations for validation users and compute ranking metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d96b6a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8d96b6a8",
    "outputId": "4638f9cc-c63a-4479-b818-2ff846407b0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Loading mappings...\n",
      "✅ Mappings loaded:\n",
      "   Users: 5,000\n",
      "   Recipes: 10,437\n",
      "   User features: 20\n",
      "   Recipe features: 63\n",
      "\n",
      "Loading data...\n",
      "✅ Data loaded:\n",
      "   Users: 5,000\n",
      "   Recipes (raw): 10,457\n",
      "   Recipes (in model): 10,437\n",
      "   Train interactions: 931,700\n",
      "   Val interactions: 200,145\n",
      "   Test interactions: 202,482\n",
      "\n",
      "✅ Filtered to 10,437 recipes that exist in model\n",
      "\n",
      "Preparing features...\n",
      "✅ Features prepared\n",
      "   User feature cols: 20\n",
      "   Recipe feature cols: 63\n",
      "\n",
      "Loading trained model...\n",
      "  Loading weights...\n",
      "  Moving model to device...\n",
      "✅ Model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "📊 EVALUATING ON VALIDATION SET\n",
      "======================================================================\n",
      "\n",
      "Validation users: 5,000\n",
      "\n",
      "  Pre-computing recipe embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2364945459.py:253: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
      "  recipe_feats_tensor = torch.FloatTensor(recipe_feats).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Recipe embeddings computed: torch.Size([10437, 128])\n",
      "  Processing 5000 users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating recommendations: 100%|██████████| 79/79 [00:01<00:00, 44.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Generated recommendations for 5,000 users\n",
      "\n",
      "Computing metrics...\n",
      "\n",
      "======================================================================\n",
      "✅ VALIDATION SET RESULTS\n",
      "======================================================================\n",
      "NDCG@5................... 0.0363\n",
      "NDCG@10.................. 0.0524\n",
      "NDCG@20.................. 0.0744\n",
      "Precision@5.............. 0.0107\n",
      "Precision@10............. 0.0109\n",
      "Precision@20............. 0.0112\n",
      "Recall@5................. 0.0013\n",
      "Recall@10................ 0.0027\n",
      "Recall@20................ 0.0054\n",
      "MAP...................... 0.0054\n",
      "MRR@10................... 0.0304\n",
      "MRR@20................... 0.0367\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "📊 EVALUATING ON TEST SET\n",
      "======================================================================\n",
      "\n",
      "Test users: 5,000\n",
      "\n",
      "  Pre-computing recipe embeddings...\n",
      "  Recipe embeddings computed: torch.Size([10437, 128])\n",
      "  Processing 5000 users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating recommendations: 100%|██████████| 79/79 [00:01<00:00, 45.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Generated recommendations for 5,000 users\n",
      "\n",
      "Computing metrics...\n",
      "\n",
      "======================================================================\n",
      "✅ TEST SET RESULTS\n",
      "======================================================================\n",
      "NDCG@5................... 0.0279\n",
      "NDCG@10.................. 0.0396\n",
      "NDCG@20.................. 0.0570\n",
      "Precision@5.............. 0.0084\n",
      "Precision@10............. 0.0084\n",
      "Precision@20............. 0.0085\n",
      "Recall@5................. 0.0009\n",
      "Recall@10................ 0.0019\n",
      "Recall@20................ 0.0039\n",
      "MAP...................... 0.0048\n",
      "MRR@10................... 0.0229\n",
      "MRR@20................... 0.0276\n",
      "======================================================================\n",
      "\n",
      " Results saved to: /content/drive/MyDrive/RecipeML/results\n",
      "\n",
      "======================================================================\n",
      " SAMPLE RECOMMENDATIONS\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "User ID: 1\n",
      "Segment: Quick Meal Seekers\n",
      "Location: Sydney, AU\n",
      "\n",
      "Actual test interactions: 50\n",
      "\n",
      "Top 10 Recommendations:\n",
      "Rank   Recipe ID    Hit?\n",
      "----------------------------------------\n",
      "1      1292         \n",
      "2      4413         \n",
      "3      2682         \n",
      "4      7571         \n",
      "5      7392         \n",
      "6      4700         \n",
      "7      4678         \n",
      "8      9058         ✅ HIT\n",
      "9      8543         \n",
      "10     921          \n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "User ID: 2\n",
      "Segment: Adventurous Foodies\n",
      "Location: Berlin, DE\n",
      "\n",
      "Actual test interactions: 33\n",
      "\n",
      "Top 10 Recommendations:\n",
      "Rank   Recipe ID    Hit?\n",
      "----------------------------------------\n",
      "1      7197         \n",
      "2      181          \n",
      "3      103          \n",
      "4      2708         \n",
      "5      9468         \n",
      "6      6383         \n",
      "7      4048         \n",
      "8      1973         \n",
      "9      688          \n",
      "10     1869         \n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "User ID: 3\n",
      "Segment: Adventurous Foodies\n",
      "Location: Seoul, KR\n",
      "\n",
      "Actual test interactions: 38\n",
      "\n",
      "Top 10 Recommendations:\n",
      "Rank   Recipe ID    Hit?\n",
      "----------------------------------------\n",
      "1      7197         \n",
      "2      181          \n",
      "3      103          \n",
      "4      9468         \n",
      "5      2708         \n",
      "6      6383         \n",
      "7      688          \n",
      "8      4048         \n",
      "9      1869         \n",
      "10     2679         \n",
      "======================================================================\n",
      "\n",
      "🎉 Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Two-Tower Model Evaluation Script\n",
    "Evaluates trained model on validation and test sets\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import ast\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "DATA_DIR = \"/content/drive/MyDrive/RecipeML/data\"\n",
    "PROCESSED_DATA_DIR = \"/content/drive/MyDrive/RecipeML/processed\"\n",
    "RESULTS_DIR = \"/content/drive/MyDrive/RecipeML/results\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD MAPPINGS FIRST (CRITICAL!)\n",
    "# =============================================================================\n",
    "print(\"Loading mappings...\")\n",
    "with open(f\"{PROCESSED_DATA_DIR}/mappings.pkl\", 'rb') as f:\n",
    "    mappings = pickle.load(f)\n",
    "\n",
    "user_id_to_idx = mappings['user_id_to_idx']\n",
    "recipe_id_to_idx = mappings['recipe_id_to_idx']\n",
    "user_feature_cols = mappings['user_feature_cols']\n",
    "recipe_feature_cols = mappings['recipe_feature_cols']\n",
    "n_users = mappings['n_users']\n",
    "n_recipes = mappings['n_recipes']\n",
    "user_feature_dim = mappings['user_feature_dim']\n",
    "recipe_feature_dim = mappings['recipe_feature_dim']\n",
    "\n",
    "# Create reverse mappings\n",
    "idx_to_user_id = {idx: uid for uid, idx in user_id_to_idx.items()}\n",
    "idx_to_recipe_id = {idx: rid for rid, idx in recipe_id_to_idx.items()}\n",
    "\n",
    "print(f\"✅ Mappings loaded:\")\n",
    "print(f\"   Users: {n_users:,}\")\n",
    "print(f\"   Recipes: {n_recipes:,}\")\n",
    "print(f\"   User features: {user_feature_dim}\")\n",
    "print(f\"   Recipe features: {recipe_feature_dim}\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD RAW DATA\n",
    "# =============================================================================\n",
    "print(\"Loading data...\")\n",
    "users_df = pd.read_csv(f\"{DATA_DIR}/users.csv\")\n",
    "recipes_df = pd.read_csv(f\"{DATA_DIR}/recipes.csv\")\n",
    "\n",
    "if 'id' in recipes_df.columns:\n",
    "    recipes_df.rename(columns={'id': 'recipe_id'}, inplace=True)\n",
    "\n",
    "train_data = pd.read_csv(f\"{DATA_DIR}/train_interactions.csv\")\n",
    "val_data = pd.read_csv(f\"{DATA_DIR}/val_interactions.csv\")\n",
    "test_data = pd.read_csv(f\"{DATA_DIR}/test_interactions.csv\")\n",
    "\n",
    "print(f\"✅ Data loaded:\")\n",
    "print(f\"   Users: {len(users_df):,}\")\n",
    "print(f\"   Recipes (raw): {len(recipes_df):,}\")\n",
    "print(f\"   Recipes (in model): {n_recipes:,}\")\n",
    "print(f\"   Train interactions: {len(train_data):,}\")\n",
    "print(f\"   Val interactions: {len(val_data):,}\")\n",
    "print(f\"   Test interactions: {len(test_data):,}\\n\")\n",
    "\n",
    "# CRITICAL: Filter recipes to only those in the model\n",
    "recipes_df = recipes_df[recipes_df['recipe_id'].isin(recipe_id_to_idx.keys())].copy()\n",
    "print(f\"✅ Filtered to {len(recipes_df):,} recipes that exist in model\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# PREPARE FEATURES (MATCHING TRAINING SCRIPT)\n",
    "# =============================================================================\n",
    "print(\"Preparing features...\")\n",
    "\n",
    "# USER FEATURES\n",
    "categorical_user_cols = ['gender', 'location', 'user_segment']\n",
    "numeric_user_cols = ['age']\n",
    "\n",
    "users_processed = pd.get_dummies(users_df, columns=categorical_user_cols, dummy_na=False)\n",
    "final_user_feature_cols = numeric_user_cols + [col for col in users_processed.columns\n",
    "                                                 if any(cat_col in col for cat_col in categorical_user_cols)]\n",
    "\n",
    "users_processed[final_user_feature_cols] = users_processed[final_user_feature_cols].fillna(0)\n",
    "scaler_user = StandardScaler()\n",
    "users_processed[final_user_feature_cols] = scaler_user.fit_transform(users_processed[final_user_feature_cols])\n",
    "user_features_dict = users_processed.set_index('user_id')[final_user_feature_cols].to_dict('index')\n",
    "\n",
    "# RECIPE FEATURES\n",
    "categorical_recipe_cols = ['dietary_type', 'cuisine', 'difficulty', 'cooking_method']\n",
    "numeric_recipe_cols = ['servings', 'cook_time', 'prep_time', 'total_time',\n",
    "                       'calories_per_serving', 'popularity_score', 'view_count',\n",
    "                       'save_count', 'like_count', 'comment_count', 'avg_rating']\n",
    "\n",
    "# Handle list columns\n",
    "for col in ['ingredients', 'instructions', 'tags']:\n",
    "    if col in recipes_df.columns:\n",
    "        try:\n",
    "            recipes_df[f'num_{col}'] = recipes_df[col].apply(\n",
    "                lambda x: len(ast.literal_eval(x)) if isinstance(x, str) else (len(x) if isinstance(x, list) else 0)\n",
    "            )\n",
    "            numeric_recipe_cols.append(f'num_{col}')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "recipes_processed = pd.get_dummies(recipes_df, columns=categorical_recipe_cols, dummy_na=False)\n",
    "final_recipe_feature_cols = numeric_recipe_cols + [col for col in recipes_processed.columns\n",
    "                                                     if any(cat_col in col for cat_col in categorical_recipe_cols)]\n",
    "\n",
    "recipes_processed[final_recipe_feature_cols] = recipes_processed[final_recipe_feature_cols].fillna(0)\n",
    "scaler_recipe = StandardScaler()\n",
    "recipes_processed[final_recipe_feature_cols] = scaler_recipe.fit_transform(recipes_processed[final_recipe_feature_cols])\n",
    "recipe_features_dict = recipes_processed.set_index('recipe_id')[final_recipe_feature_cols].to_dict('index')\n",
    "\n",
    "print(f\"✅ Features prepared\")\n",
    "print(f\"   User feature cols: {len(final_user_feature_cols)}\")\n",
    "print(f\"   Recipe feature cols: {len(final_recipe_feature_cols)}\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# DEFINE MODEL (MATCHING TRAINING SCRIPT)\n",
    "# =============================================================================\n",
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self, n_users, user_feature_dim, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + user_feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, user_ids, user_features):\n",
    "        user_emb = self.user_embedding(user_ids)\n",
    "        combined = torch.cat([user_emb, user_features], dim=-1)\n",
    "        return self.fc(combined)\n",
    "\n",
    "class RecipeEncoder(nn.Module):\n",
    "    def __init__(self, n_recipes, recipe_feature_dim, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.recipe_embedding = nn.Embedding(n_recipes, embedding_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + recipe_feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, recipe_ids, recipe_features):\n",
    "        recipe_emb = self.recipe_embedding(recipe_ids)\n",
    "        combined = torch.cat([recipe_emb, recipe_features], dim=-1)\n",
    "        return self.fc(combined)\n",
    "\n",
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, n_users, n_recipes, user_feature_dim, recipe_feature_dim,\n",
    "                 embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.user_encoder = UserEncoder(n_users, user_feature_dim, embedding_dim, hidden_dim)\n",
    "        self.recipe_encoder = RecipeEncoder(n_recipes, recipe_feature_dim, embedding_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, user_ids, user_features, recipe_ids, recipe_features):\n",
    "        user_emb = self.user_encoder(user_ids, user_features)\n",
    "        recipe_emb = self.recipe_encoder(recipe_ids, recipe_features)\n",
    "        scores = (user_emb * recipe_emb).sum(dim=-1)\n",
    "        return scores\n",
    "\n",
    "    def get_user_embeddings(self, user_ids, user_features):\n",
    "        return self.user_encoder(user_ids, user_features)\n",
    "\n",
    "    def get_recipe_embeddings(self, recipe_ids, recipe_features):\n",
    "        return self.recipe_encoder(recipe_ids, recipe_features)\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD TRAINED MODEL\n",
    "# =============================================================================\n",
    "print(\"Loading trained model...\")\n",
    "\n",
    "# CRITICAL: Use n_users and n_recipes from mappings, not from raw data!\n",
    "model = TwoTowerModel(\n",
    "    n_users=n_users,  # From mappings (5,000)\n",
    "    n_recipes=n_recipes,  # From mappings (10,437)\n",
    "    user_feature_dim=user_feature_dim,  # From mappings (20)\n",
    "    recipe_feature_dim=recipe_feature_dim,  # From mappings (63)\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=256\n",
    ")\n",
    "\n",
    "# Load weights BEFORE moving to device\n",
    "print(\"  Loading weights...\")\n",
    "model.load_state_dict(torch.load(f\"{PROCESSED_DATA_DIR}/best_two_tower_model.pth\",\n",
    "                                   map_location='cpu', weights_only=True))\n",
    "\n",
    "# Now move to device\n",
    "print(\"  Moving model to device...\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"✅ Model loaded successfully\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# RECOMMENDATION FUNCTIONS\n",
    "# =============================================================================\n",
    "def batch_get_recommendations(model, user_ids, k=20, exclude_dict=None, batch_size=64):\n",
    "    \"\"\"\n",
    "    Generate recommendations for multiple users\n",
    "\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        user_ids: List of user IDs (original IDs)\n",
    "        k: Number of recommendations\n",
    "        exclude_dict: Dict {user_id: set of recipe_ids to exclude}\n",
    "        batch_size: Batch size for processing\n",
    "\n",
    "    Returns:\n",
    "        Dict {user_id: list of recipe_ids}\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    recommendations = {}\n",
    "    exclude_dict = exclude_dict or {}\n",
    "\n",
    "    # Pre-compute all recipe embeddings\n",
    "    print(\"  Pre-computing recipe embeddings...\")\n",
    "    with torch.no_grad():\n",
    "        # Use only recipes that exist in the model\n",
    "        all_recipe_indices = list(range(n_recipes))\n",
    "        all_recipe_ids_orig = [idx_to_recipe_id[idx] for idx in all_recipe_indices]\n",
    "\n",
    "        # Get recipe features for recipes in correct order\n",
    "        recipe_feats = []\n",
    "        for recipe_id in all_recipe_ids_orig:\n",
    "            recipe_feat_dict = recipe_features_dict.get(recipe_id, {})\n",
    "            # CRITICAL: Use recipe_feature_cols, not user_feature_cols!\n",
    "            recipe_feat = np.array([recipe_feat_dict.get(col, 0.0) for col in recipe_feature_cols],\n",
    "                                  dtype=np.float32)\n",
    "            recipe_feats.append(recipe_feat)\n",
    "\n",
    "        recipe_indices_tensor = torch.LongTensor(all_recipe_indices).to(device)\n",
    "        recipe_feats_tensor = torch.FloatTensor(recipe_feats).to(device)\n",
    "        recipe_embs = model.get_recipe_embeddings(recipe_indices_tensor, recipe_feats_tensor)\n",
    "\n",
    "    print(f\"  Recipe embeddings computed: {recipe_embs.shape}\")\n",
    "    print(f\"  Processing {len(user_ids)} users...\")\n",
    "\n",
    "    # Process users in batches\n",
    "    for i in tqdm(range(0, len(user_ids), batch_size), desc=\"Generating recommendations\"):\n",
    "        batch_user_ids = user_ids[i:i+batch_size]\n",
    "\n",
    "        # Filter valid users\n",
    "        valid_batch_user_ids = [uid for uid in batch_user_ids if uid in user_id_to_idx]\n",
    "        if not valid_batch_user_ids:\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get user embeddings\n",
    "            user_indices = [user_id_to_idx[uid] for uid in valid_batch_user_ids]\n",
    "            user_feats = []\n",
    "            for uid in valid_batch_user_ids:\n",
    "                user_feat_dict = user_features_dict.get(uid, {})\n",
    "                # Use user_feature_cols here\n",
    "                user_feat = np.array([user_feat_dict.get(col, 0.0) for col in user_feature_cols],\n",
    "                                    dtype=np.float32)\n",
    "                user_feats.append(user_feat)\n",
    "\n",
    "            user_indices_tensor = torch.LongTensor(user_indices).to(device)\n",
    "            user_feats_tensor = torch.FloatTensor(user_feats).to(device)\n",
    "            user_embs = model.get_user_embeddings(user_indices_tensor, user_feats_tensor)\n",
    "\n",
    "            # Compute scores\n",
    "            scores = (user_embs @ recipe_embs.T).cpu().numpy()  # (batch_size, n_recipes)\n",
    "\n",
    "            # Process each user\n",
    "            for j, user_id in enumerate(valid_batch_user_ids):\n",
    "                user_scores = scores[j].copy()\n",
    "\n",
    "                # Exclude recipes\n",
    "                exclude_recipes = exclude_dict.get(user_id, set())\n",
    "                for recipe_id in exclude_recipes:\n",
    "                    if recipe_id in recipe_id_to_idx:\n",
    "                        recipe_idx = recipe_id_to_idx[recipe_id]\n",
    "                        user_scores[recipe_idx] = -np.inf\n",
    "\n",
    "                # Get top-k\n",
    "                top_indices = np.argsort(user_scores)[::-1][:k]\n",
    "                top_recipe_ids = [idx_to_recipe_id[idx] for idx in top_indices]\n",
    "                recommendations[user_id] = top_recipe_ids\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATION METRICS\n",
    "# =============================================================================\n",
    "def ndcg_at_k(predictions, ground_truth, k):\n",
    "    \"\"\"Calculate NDCG@k\"\"\"\n",
    "    ndcg_scores = []\n",
    "    for user_id, pred_list in predictions.items():\n",
    "        if user_id not in ground_truth:\n",
    "            continue\n",
    "\n",
    "        relevant = ground_truth[user_id]\n",
    "        pred_k = pred_list[:k]\n",
    "\n",
    "        relevance = np.array([1 if recipe in relevant else 0 for recipe in pred_k])\n",
    "\n",
    "        if relevance.sum() == 0:\n",
    "            ndcg_scores.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # DCG\n",
    "        dcg = relevance[0] + np.sum(relevance[1:] / np.log2(np.arange(2, len(relevance) + 1)))\n",
    "\n",
    "        # IDCG\n",
    "        ideal_relevance = np.sort(relevance)[::-1]\n",
    "        idcg = ideal_relevance[0] + np.sum(ideal_relevance[1:] / np.log2(np.arange(2, len(ideal_relevance) + 1)))\n",
    "\n",
    "        ndcg_scores.append(dcg / idcg if idcg > 0 else 0.0)\n",
    "\n",
    "    return np.mean(ndcg_scores) if ndcg_scores else 0.0\n",
    "\n",
    "def precision_at_k(predictions, ground_truth, k):\n",
    "    \"\"\"Calculate Precision@k\"\"\"\n",
    "    precisions = []\n",
    "    for user_id, pred_list in predictions.items():\n",
    "        if user_id not in ground_truth:\n",
    "            continue\n",
    "\n",
    "        relevant = ground_truth[user_id]\n",
    "        pred_k = pred_list[:k]\n",
    "\n",
    "        hits = len(set(pred_k) & relevant)\n",
    "        precisions.append(hits / k)\n",
    "\n",
    "    return np.mean(precisions) if precisions else 0.0\n",
    "\n",
    "def recall_at_k(predictions, ground_truth, k):\n",
    "    \"\"\"Calculate Recall@k\"\"\"\n",
    "    recalls = []\n",
    "    for user_id, pred_list in predictions.items():\n",
    "        if user_id not in ground_truth:\n",
    "            continue\n",
    "\n",
    "        relevant = ground_truth[user_id]\n",
    "        if len(relevant) == 0:\n",
    "            continue\n",
    "\n",
    "        pred_k = pred_list[:k]\n",
    "        hits = len(set(pred_k) & relevant)\n",
    "        recalls.append(hits / len(relevant))\n",
    "\n",
    "    return np.mean(recalls) if recalls else 0.0\n",
    "\n",
    "def mean_average_precision(predictions, ground_truth):\n",
    "    \"\"\"Calculate MAP\"\"\"\n",
    "    average_precisions = []\n",
    "\n",
    "    for user_id, pred_list in predictions.items():\n",
    "        if user_id not in ground_truth:\n",
    "            continue\n",
    "\n",
    "        relevant = ground_truth[user_id]\n",
    "        if len(relevant) == 0:\n",
    "            continue\n",
    "\n",
    "        hits = 0\n",
    "        sum_precisions = 0.0\n",
    "\n",
    "        for i, recipe_id in enumerate(pred_list, 1):\n",
    "            if recipe_id in relevant:\n",
    "                hits += 1\n",
    "                sum_precisions += hits / i\n",
    "\n",
    "        if hits > 0:\n",
    "            average_precisions.append(sum_precisions / len(relevant))\n",
    "\n",
    "    return np.mean(average_precisions) if average_precisions else 0.0\n",
    "\n",
    "def mrr_at_k(predictions, ground_truth, k):\n",
    "    \"\"\"Calculate MRR@k\"\"\"\n",
    "    reciprocal_ranks = []\n",
    "\n",
    "    for user_id, pred_list in predictions.items():\n",
    "        if user_id not in ground_truth:\n",
    "            continue\n",
    "\n",
    "        relevant = ground_truth[user_id]\n",
    "        pred_k = pred_list[:k]\n",
    "\n",
    "        for i, recipe_id in enumerate(pred_k, 1):\n",
    "            if recipe_id in relevant:\n",
    "                reciprocal_ranks.append(1.0 / i)\n",
    "                break\n",
    "        else:\n",
    "            reciprocal_ranks.append(0.0)\n",
    "\n",
    "    return np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATE ON VALIDATION SET\n",
    "# =============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"📊 EVALUATING ON VALIDATION SET\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "val_users = val_data['user_id'].unique().tolist()\n",
    "val_user_recipes = val_data.groupby('user_id')['recipe_id'].apply(set).to_dict()\n",
    "train_user_recipes = train_data.groupby('user_id')['recipe_id'].apply(set).to_dict()\n",
    "\n",
    "print(f\"Validation users: {len(val_users):,}\\n\")\n",
    "\n",
    "val_predictions = batch_get_recommendations(\n",
    "    model=model,\n",
    "    user_ids=val_users,\n",
    "    k=20,\n",
    "    exclude_dict=train_user_recipes,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Generated recommendations for {len(val_predictions):,} users\\n\")\n",
    "\n",
    "print(\"Computing metrics...\")\n",
    "val_metrics = {\n",
    "    'NDCG@5': ndcg_at_k(val_predictions, val_user_recipes, k=5),\n",
    "    'NDCG@10': ndcg_at_k(val_predictions, val_user_recipes, k=10),\n",
    "    'NDCG@20': ndcg_at_k(val_predictions, val_user_recipes, k=20),\n",
    "    'Precision@5': precision_at_k(val_predictions, val_user_recipes, k=5),\n",
    "    'Precision@10': precision_at_k(val_predictions, val_user_recipes, k=10),\n",
    "    'Precision@20': precision_at_k(val_predictions, val_user_recipes, k=20),\n",
    "    'Recall@5': recall_at_k(val_predictions, val_user_recipes, k=5),\n",
    "    'Recall@10': recall_at_k(val_predictions, val_user_recipes, k=10),\n",
    "    'Recall@20': recall_at_k(val_predictions, val_user_recipes, k=20),\n",
    "    'MAP': mean_average_precision(val_predictions, val_user_recipes),\n",
    "    'MRR@10': mrr_at_k(val_predictions, val_user_recipes, k=10),\n",
    "    'MRR@20': mrr_at_k(val_predictions, val_user_recipes, k=20)\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ VALIDATION SET RESULTS\")\n",
    "print(\"=\"*70)\n",
    "for metric_name, metric_value in val_metrics.items():\n",
    "    print(f\"{metric_name:.<25} {metric_value:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATE ON TEST SET\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📊 EVALUATING ON TEST SET\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "test_users = test_data['user_id'].unique().tolist()\n",
    "test_user_recipes = test_data.groupby('user_id')['recipe_id'].apply(set).to_dict()\n",
    "\n",
    "# Combine train + val for exclusion\n",
    "train_val_user_recipes = {}\n",
    "for user_id in test_users:\n",
    "    train_val_user_recipes[user_id] = (\n",
    "        train_user_recipes.get(user_id, set()) |\n",
    "        val_user_recipes.get(user_id, set())\n",
    "    )\n",
    "\n",
    "print(f\"Test users: {len(test_users):,}\\n\")\n",
    "\n",
    "test_predictions = batch_get_recommendations(\n",
    "    model=model,\n",
    "    user_ids=test_users,\n",
    "    k=20,\n",
    "    exclude_dict=train_val_user_recipes,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Generated recommendations for {len(test_predictions):,} users\\n\")\n",
    "\n",
    "print(\"Computing metrics...\")\n",
    "test_metrics = {\n",
    "    'NDCG@5': ndcg_at_k(test_predictions, test_user_recipes, k=5),\n",
    "    'NDCG@10': ndcg_at_k(test_predictions, test_user_recipes, k=10),\n",
    "    'NDCG@20': ndcg_at_k(test_predictions, test_user_recipes, k=20),\n",
    "    'Precision@5': precision_at_k(test_predictions, test_user_recipes, k=5),\n",
    "    'Precision@10': precision_at_k(test_predictions, test_user_recipes, k=10),\n",
    "    'Precision@20': precision_at_k(test_predictions, test_user_recipes, k=20),\n",
    "    'Recall@5': recall_at_k(test_predictions, test_user_recipes, k=5),\n",
    "    'Recall@10': recall_at_k(test_predictions, test_user_recipes, k=10),\n",
    "    'Recall@20': recall_at_k(test_predictions, test_user_recipes, k=20),\n",
    "    'MAP': mean_average_precision(test_predictions, test_user_recipes),\n",
    "    'MRR@10': mrr_at_k(test_predictions, test_user_recipes, k=10),\n",
    "    'MRR@20': mrr_at_k(test_predictions, test_user_recipes, k=20)\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ TEST SET RESULTS\")\n",
    "print(\"=\"*70)\n",
    "for metric_name, metric_value in test_metrics.items():\n",
    "    print(f\"{metric_name:.<25} {metric_value:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE RESULTS\n",
    "# =============================================================================\n",
    "with open(f\"{RESULTS_DIR}/val_predictions.pkl\", 'wb') as f:\n",
    "    pickle.dump(val_predictions, f)\n",
    "\n",
    "with open(f\"{RESULTS_DIR}/test_predictions.pkl\", 'wb') as f:\n",
    "    pickle.dump(test_predictions, f)\n",
    "\n",
    "metrics_summary = {\n",
    "    'validation': val_metrics,\n",
    "    'test': test_metrics\n",
    "}\n",
    "\n",
    "with open(f\"{RESULTS_DIR}/evaluation_metrics.pkl\", 'wb') as f:\n",
    "    pickle.dump(metrics_summary, f)\n",
    "\n",
    "print(f\"\\n Results saved to: {RESULTS_DIR}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAMPLE RECOMMENDATIONS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" SAMPLE RECOMMENDATIONS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "sample_users = test_users[:3]\n",
    "\n",
    "for user_id in sample_users:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"User ID: {user_id}\")\n",
    "\n",
    "    user_info = users_df[users_df['user_id'] == user_id].iloc[0]\n",
    "    print(f\"Segment: {user_info.get('user_segment', 'N/A')}\")\n",
    "    print(f\"Location: {user_info.get('location', 'N/A')}\")\n",
    "\n",
    "    actual = test_user_recipes.get(user_id, set())\n",
    "    print(f\"\\nActual test interactions: {len(actual)}\")\n",
    "\n",
    "    if user_id in test_predictions:\n",
    "        recs = test_predictions[user_id][:10]\n",
    "        print(f\"\\nTop 10 Recommendations:\")\n",
    "        print(f\"{'Rank':<6} {'Recipe ID':<12} {'Hit?'}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        for i, recipe_id in enumerate(recs, 1):\n",
    "            hit = \"✅ HIT\" if recipe_id in actual else \"\"\n",
    "            print(f\"{i:<6} {recipe_id:<12} {hit}\")\n",
    "\n",
    "    print(\"=\"*70)\n",
    "\n",
    "print(\"\\n🎉 Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0116465",
   "metadata": {
    "id": "d0116465"
   },
   "source": [
    "## 1️8. Final Evaluation: Two-Tower on Test Set\n",
    "\n",
    "**This is the final evaluation** - we can run this only once when we're satisfied with validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6yJ0zej0PEgu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6yJ0zej0PEgu",
    "outputId": "da634cb8-0dc4-4b5f-ecaa-01db4181a6f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "✅ Model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating recommendations: 100%|██████████| 79/79 [00:02<00:00, 33.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validation metrics:\n",
      "NDCG@5         : 0.0363\n",
      "NDCG@10        : 0.0524\n",
      "NDCG@20        : 0.0744\n",
      "Precision@5    : 0.0107\n",
      "Precision@10   : 0.0109\n",
      "Precision@20   : 0.0112\n",
      "Recall@5       : 0.0013\n",
      "Recall@10      : 0.0027\n",
      "Recall@20      : 0.0054\n",
      "MAP            : 0.0054\n",
      "MRR@10         : 0.0304\n",
      "MRR@20         : 0.0367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating recommendations: 100%|██████████| 79/79 [00:03<00:00, 23.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test metrics:\n",
      "NDCG@5         : 0.0279\n",
      "NDCG@10        : 0.0396\n",
      "NDCG@20        : 0.0570\n",
      "Precision@5    : 0.0084\n",
      "Precision@10   : 0.0084\n",
      "Precision@20   : 0.0085\n",
      "Recall@5       : 0.0009\n",
      "Recall@10      : 0.0019\n",
      "Recall@20      : 0.0039\n",
      "MAP            : 0.0048\n",
      "MRR@10         : 0.0229\n",
      "MRR@20         : 0.0276\n",
      "\n",
      "User ID: 1\n",
      "Actual interactions: 50\n",
      "Top 10 Recommendations:\n",
      "1   1292         \n",
      "2   4413         \n",
      "3   2682         \n",
      "4   7571         \n",
      "5   7392         \n",
      "6   4700         \n",
      "7   4678         \n",
      "8   9058         ✅ HIT\n",
      "9   8543         \n",
      "10  921          \n",
      "\n",
      "User ID: 2\n",
      "Actual interactions: 33\n",
      "Top 10 Recommendations:\n",
      "1   7197         \n",
      "2   181          \n",
      "3   103          \n",
      "4   2708         \n",
      "5   9468         \n",
      "6   6383         \n",
      "7   4048         \n",
      "8   1973         \n",
      "9   688          \n",
      "10  1869         \n",
      "\n",
      "User ID: 3\n",
      "Actual interactions: 38\n",
      "Top 10 Recommendations:\n",
      "1   7197         \n",
      "2   181          \n",
      "3   103          \n",
      "4   9468         \n",
      "5   2708         \n",
      "6   6383         \n",
      "7   688          \n",
      "8   4048         \n",
      "9   1869         \n",
      "10  2679         \n",
      "\n",
      "🎉 Final evaluation complete! Results saved in: /content/drive/MyDrive/RecipeML/results\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# TWO-TOWER FINAL EVALUATION SCRIPT (FIXED)\n",
    "# ================================================================\n",
    "\n",
    "import os, time, pickle, ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ================================================================\n",
    "# CONFIG\n",
    "# ================================================================\n",
    "DATA_DIR = \"/content/drive/MyDrive/RecipeML/data\"\n",
    "PROCESSED_DATA_DIR = \"/content/drive/MyDrive/RecipeML/processed\"\n",
    "RESULTS_DIR = \"/content/drive/MyDrive/RecipeML/results\"\n",
    "MODEL_PATH = os.path.join(PROCESSED_DATA_DIR, \"best_two_tower_model.pth\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ================================================================\n",
    "# LOAD MAPPINGS\n",
    "# ================================================================\n",
    "with open(os.path.join(PROCESSED_DATA_DIR, \"mappings.pkl\"), \"rb\") as f:\n",
    "    mappings = pickle.load(f)\n",
    "\n",
    "user_id_to_idx = mappings[\"user_id_to_idx\"]\n",
    "recipe_id_to_idx = mappings[\"recipe_id_to_idx\"]\n",
    "idx_to_user_id = {v: k for k, v in user_id_to_idx.items()}\n",
    "idx_to_recipe_id = {v: k for k, v in recipe_id_to_idx.items()}\n",
    "user_feature_cols = mappings[\"user_feature_cols\"]\n",
    "recipe_feature_cols = mappings[\"recipe_feature_cols\"]\n",
    "n_users = mappings[\"n_users\"]\n",
    "n_recipes = mappings[\"n_recipes\"]\n",
    "user_feature_dim = mappings[\"user_feature_dim\"]\n",
    "recipe_feature_dim = mappings[\"recipe_feature_dim\"]\n",
    "\n",
    "# ================================================================\n",
    "# LOAD RAW DATA\n",
    "# ================================================================\n",
    "users_df = pd.read_csv(f\"{DATA_DIR}/users.csv\")\n",
    "recipes_df = pd.read_csv(f\"{DATA_DIR}/recipes.csv\")\n",
    "train_data = pd.read_csv(f\"{DATA_DIR}/train_interactions.csv\")\n",
    "val_data = pd.read_csv(f\"{DATA_DIR}/val_interactions.csv\")\n",
    "test_data = pd.read_csv(f\"{DATA_DIR}/test_interactions.csv\")\n",
    "\n",
    "# CRITICAL FIX: Rename 'id' column to 'recipe_id' if it exists\n",
    "if 'id' in recipes_df.columns:\n",
    "    recipes_df.rename(columns={'id': 'recipe_id'}, inplace=True)\n",
    "\n",
    "\n",
    "# Keep only recipes that exist in the model\n",
    "recipes_df = recipes_df[recipes_df['recipe_id'].isin(recipe_id_to_idx.keys())]\n",
    "\n",
    "# ================================================================\n",
    "# PREPARE FEATURES\n",
    "# ================================================================\n",
    "# --- USER FEATURES ---\n",
    "categorical_user_cols = ['gender', 'location', 'user_segment']\n",
    "numeric_user_cols = ['age']\n",
    "\n",
    "users_processed = pd.get_dummies(users_df, columns=categorical_user_cols, dummy_na=False)\n",
    "final_user_feature_cols = numeric_user_cols + [col for col in users_processed.columns if any(cat in col for cat in categorical_user_cols)]\n",
    "users_processed[final_user_feature_cols] = users_processed[final_user_feature_cols].fillna(0)\n",
    "scaler_user = StandardScaler()\n",
    "users_processed[final_user_feature_cols] = scaler_user.fit_transform(users_processed[final_user_feature_cols])\n",
    "user_features_dict = users_processed.set_index('user_id')[final_user_feature_cols].to_dict('index')\n",
    "\n",
    "# --- RECIPE FEATURES ---\n",
    "categorical_recipe_cols = ['dietary_type', 'cuisine', 'difficulty', 'cooking_method']\n",
    "# FIXED: Add all relevant numeric columns explicitly\n",
    "numeric_recipe_cols = ['servings', 'cook_time', 'prep_time', 'total_time',\n",
    "                       'calories_per_serving', 'popularity_score', 'view_count',\n",
    "                       'save_count', 'like_count', 'comment_count', 'avg_rating']\n",
    "\n",
    "# Ensure numeric columns exist in recipes_df before processing\n",
    "existing_numeric_recipe_cols = [col for col in numeric_recipe_cols if col in recipes_df.columns]\n",
    "missing_numeric_recipe_cols = [col for col in numeric_recipe_cols if col not in recipes_df.columns]\n",
    "\n",
    "if missing_numeric_recipe_cols:\n",
    "    print(f\"Warning: Missing numeric recipe columns in recipes_df: {missing_numeric_recipe_cols}\")\n",
    "\n",
    "numeric_recipe_cols = existing_numeric_recipe_cols # Use only existing numeric columns\n",
    "\n",
    "\n",
    "for col in ['ingredients', 'instructions', 'tags']:\n",
    "    if col in recipes_df.columns and f'num_{col}' not in numeric_recipe_cols: # Avoid adding duplicates if already in list\n",
    "        recipes_df[f'num_{col}'] = recipes_df[col].apply(lambda x: len(ast.literal_eval(x)) if isinstance(x, str) else (len(x) if isinstance(x, list) else 0))\n",
    "        numeric_recipe_cols.append(f'num_{col}')\n",
    "\n",
    "\n",
    "recipes_processed = pd.get_dummies(recipes_df, columns=categorical_recipe_cols, dummy_na=False)\n",
    "final_recipe_feature_cols = numeric_recipe_cols + [col for col in recipes_processed.columns if any(cat in col for cat in categorical_recipe_cols)]\n",
    "\n",
    "# Ensure final recipe feature columns exist before processing\n",
    "existing_final_recipe_feature_cols = [col for col in final_recipe_feature_cols if col in recipes_processed.columns]\n",
    "missing_final_recipe_feature_cols = [col for col in final_recipe_feature_cols if col not in recipes_processed.columns]\n",
    "\n",
    "if missing_final_recipe_feature_cols:\n",
    "    print(f\"Warning: Missing final recipe feature columns in recipes_processed: {missing_final_recipe_feature_cols}\")\n",
    "\n",
    "final_recipe_feature_cols = existing_final_recipe_feature_cols # Use only existing final columns\n",
    "\n",
    "recipes_processed[final_recipe_feature_cols] = recipes_processed[final_recipe_feature_cols].fillna(0)\n",
    "scaler_recipe = StandardScaler()\n",
    "recipes_processed[final_recipe_feature_cols] = scaler_recipe.fit_transform(recipes_processed[final_recipe_feature_cols])\n",
    "recipe_features_dict = recipes_processed.set_index('recipe_id')[final_recipe_feature_cols].to_dict('index')\n",
    "\n",
    "# Update recipe_feature_cols to reflect only the columns actually used\n",
    "recipe_feature_cols = final_recipe_feature_cols\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# DEFINE MODEL\n",
    "# ================================================================\n",
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self, n_users, user_feature_dim, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + user_feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "    def forward(self, user_ids, user_features):\n",
    "        user_emb = self.user_embedding(user_ids)\n",
    "        return self.fc(torch.cat([user_emb, user_features], dim=-1))\n",
    "\n",
    "class RecipeEncoder(nn.Module):\n",
    "    def __init__(self, n_recipes, recipe_feature_dim, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.recipe_embedding = nn.Embedding(n_recipes, embedding_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + recipe_feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "    def forward(self, recipe_ids, recipe_features):\n",
    "        recipe_emb = self.recipe_embedding(recipe_ids)\n",
    "        return self.fc(torch.cat([recipe_emb, recipe_features], dim=-1))\n",
    "\n",
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, n_users, n_recipes, user_feature_dim, recipe_feature_dim, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.user_encoder = UserEncoder(n_users, user_feature_dim, embedding_dim, hidden_dim)\n",
    "        self.recipe_encoder = RecipeEncoder(n_recipes, recipe_feature_dim, embedding_dim, hidden_dim)\n",
    "    def forward(self, user_ids, user_features, recipe_ids, recipe_features):\n",
    "        u_emb = self.user_encoder(user_ids, user_features)\n",
    "        r_emb = self.recipe_encoder(recipe_ids, recipe_features)\n",
    "        return (u_emb * r_emb).sum(dim=-1)\n",
    "    def get_user_embeddings(self, user_ids, user_features):\n",
    "        return self.user_encoder(user_ids, user_features)\n",
    "    def get_recipe_embeddings(self, recipe_ids, recipe_features):\n",
    "        return self.recipe_encoder(recipe_ids, recipe_features)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# LOAD MODEL\n",
    "# ================================================================\n",
    "model = TwoTowerModel(n_users, n_recipes, user_feature_dim, recipe_feature_dim, embedding_dim=128, hidden_dim=256)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"✅ Model loaded successfully.\")\n",
    "\n",
    "# ================================================================\n",
    "# BATCH RECOMMENDATION FUNCTION\n",
    "# ================================================================\n",
    "# ================================================================\n",
    "# BATCH RECOMMENDATION FUNCTION (FIXED)\n",
    "# ================================================================\n",
    "def batch_get_recommendations(model, user_ids, k=20, exclude_dict=None, batch_size=64):\n",
    "    recommendations = {}\n",
    "    exclude_dict = exclude_dict or {}\n",
    "\n",
    "    # Recipe embeddings\n",
    "    all_recipe_indices = list(range(n_recipes))\n",
    "    all_recipe_ids = [idx_to_recipe_id[idx] for idx in all_recipe_indices]\n",
    "\n",
    "    if not recipe_features_dict:\n",
    "        print(\"Warning: recipe_features_dict is empty. Cannot generate recipe features.\")\n",
    "        return recommendations\n",
    "\n",
    "    recipe_feats_list = []\n",
    "    for rid in all_recipe_ids:\n",
    "        feat_dict = recipe_features_dict.get(rid, {})\n",
    "        if not isinstance(feat_dict, dict):\n",
    "            feat_dict = {}\n",
    "        feat_vector = [feat_dict.get(col, 0.0) for col in recipe_feature_cols]\n",
    "        recipe_feats_list.append(feat_vector)\n",
    "\n",
    "    recipe_feats = np.array(recipe_feats_list, dtype=np.float32)\n",
    "\n",
    "    expected_recipe_feat_dim = model.recipe_encoder.fc[0].in_features - model.recipe_encoder.recipe_embedding.embedding_dim\n",
    "    if recipe_feats.shape[1] < expected_recipe_feat_dim:\n",
    "        padding = np.zeros((recipe_feats.shape[0], expected_recipe_feat_dim - recipe_feats.shape[1]), dtype=np.float32)\n",
    "        recipe_feats = np.concatenate((recipe_feats, padding), axis=1)\n",
    "    elif recipe_feats.shape[1] > expected_recipe_feat_dim:\n",
    "        recipe_feats = recipe_feats[:, :expected_recipe_feat_dim]\n",
    "\n",
    "    with torch.no_grad():  # <-- DISABLE GRADIENTS HERE\n",
    "        recipe_embs = model.get_recipe_embeddings(\n",
    "            torch.LongTensor(all_recipe_indices).to(device),\n",
    "            torch.FloatTensor(recipe_feats).to(device)\n",
    "        ).cpu().detach().numpy()  # <-- DETACH BEFORE NUMPY\n",
    "\n",
    "    # User embeddings and scoring\n",
    "    for i in tqdm(range(0, len(user_ids), batch_size), desc=\"Generating recommendations\"):\n",
    "        batch_users = user_ids[i:i+batch_size]\n",
    "        valid_users = [uid for uid in batch_users if uid in user_id_to_idx]\n",
    "        if not valid_users:\n",
    "            continue\n",
    "        user_indices = [user_id_to_idx[uid] for uid in valid_users]\n",
    "\n",
    "        if not user_features_dict:\n",
    "            continue\n",
    "\n",
    "        user_feats_list = []\n",
    "        for uid in valid_users:\n",
    "            feat_dict = user_features_dict.get(uid, {})\n",
    "            if not isinstance(feat_dict, dict):\n",
    "                feat_dict = {}\n",
    "            feat_vector = [feat_dict.get(col, 0.0) for col in user_feature_cols]\n",
    "            user_feats_list.append(feat_vector)\n",
    "\n",
    "        user_feats = np.array(user_feats_list, dtype=np.float32)\n",
    "\n",
    "        expected_user_feat_dim = model.user_encoder.fc[0].in_features - model.user_encoder.user_embedding.embedding_dim\n",
    "        if user_feats.shape[1] < expected_user_feat_dim:\n",
    "            padding = np.zeros((user_feats.shape[0], expected_user_feat_dim - user_feats.shape[1]), dtype=np.float32)\n",
    "            user_feats = np.concatenate((user_feats, padding), axis=1)\n",
    "        elif user_feats.shape[1] > expected_user_feat_dim:\n",
    "            user_feats = user_feats[:, :expected_user_feat_dim]\n",
    "\n",
    "        with torch.no_grad():  # <-- DISABLE GRADIENTS HERE\n",
    "            user_embs = model.get_user_embeddings(\n",
    "                torch.LongTensor(user_indices).to(device),\n",
    "                torch.FloatTensor(user_feats).to(device)\n",
    "            ).cpu().detach().numpy()  # <-- DETACH BEFORE NUMPY\n",
    "\n",
    "        scores = user_embs @ recipe_embs.T\n",
    "        for j, uid in enumerate(valid_users):\n",
    "            user_scores = scores[j].copy()\n",
    "            for rid in exclude_dict.get(uid, set()):\n",
    "                if rid in recipe_id_to_idx:\n",
    "                    user_scores[recipe_id_to_idx[rid]] = -np.inf\n",
    "            top_indices = np.argsort(user_scores)[::-1][:k]\n",
    "            recommendations[uid] = [idx_to_recipe_id[idx] for idx in top_indices]\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# EVALUATION\n",
    "# ================================================================\n",
    "# Make sure to use CSV DataFrames for grouping\n",
    "val_users = val_data['user_id'].unique().tolist()\n",
    "train_user_recipes = train_data.groupby('user_id')['recipe_id'].apply(set).to_dict()\n",
    "val_user_recipes = val_data.groupby('user_id')['recipe_id'].apply(set).to_dict()\n",
    "\n",
    "val_pred = batch_get_recommendations(model, val_users, k=20, exclude_dict=train_user_recipes)\n",
    "\n",
    "# Compute metrics\n",
    "def ndcg_at_k(pred, gt, k):\n",
    "    scores = []\n",
    "    for uid, p in pred.items():\n",
    "        if uid not in gt: continue\n",
    "        r = np.array([1 if x in gt[uid] else 0 for x in p[:k]])\n",
    "        if r.sum()==0: scores.append(0); continue\n",
    "        dcg = r[0]+np.sum(r[1:]/np.log2(np.arange(2,len(r)+1)))\n",
    "        idcg = np.sort(r)[::-1][0]+np.sum(np.sort(r)[::-1][1:]/np.log2(np.arange(2,len(r)+1)))\n",
    "        scores.append(dcg/idcg if idcg>0 else 0)\n",
    "    return np.mean(scores) if scores else 0\n",
    "\n",
    "def precision_at_k(pred, gt, k):\n",
    "    ps = []\n",
    "    for uid,p in pred.items():\n",
    "        if uid not in gt: continue\n",
    "        ps.append(len(set(p[:k]) & gt[uid])/k)\n",
    "    return np.mean(ps) if ps else 0\n",
    "\n",
    "def recall_at_k(pred, gt, k):\n",
    "    rs=[]\n",
    "    for uid,p in pred.items():\n",
    "        if uid not in gt: continue\n",
    "        if len(gt[uid])==0: continue\n",
    "        rs.append(len(set(p[:k])&gt[uid])/len(gt[uid]))\n",
    "    return np.mean(rs) if rs else 0\n",
    "\n",
    "def mean_average_precision(pred, gt):\n",
    "    aps=[]\n",
    "    for uid,p in pred.items():\n",
    "        if uid not in gt: continue\n",
    "        rel = gt[uid]; hits=0; sum_p=0\n",
    "        for i,x in enumerate(p,1):\n",
    "            if x in rel: hits+=1; sum_p+=hits/i\n",
    "        if hits>0: aps.append(sum_p/len(rel))\n",
    "    return np.mean(aps) if aps else 0\n",
    "\n",
    "def mrr_at_k(pred, gt, k):\n",
    "    rr=[]\n",
    "    for uid,p in pred.items():\n",
    "        if uid not in gt: continue\n",
    "        for i,x in enumerate(p[:k],1):\n",
    "            if x in gt[uid]: rr.append(1/i); break\n",
    "        else: rr.append(0)\n",
    "    return np.mean(rr) if rr else 0\n",
    "\n",
    "\n",
    "def compute_all_metrics(pred, gt):\n",
    "    return {\n",
    "        'NDCG@5': ndcg_at_k(pred, gt, 5),\n",
    "        'NDCG@10': ndcg_at_k(pred, gt, 10),\n",
    "        'NDCG@20': ndcg_at_k(pred, gt, 20),\n",
    "        'Precision@5': precision_at_k(pred, gt, 5),\n",
    "        'Precision@10': precision_at_k(pred, gt, 10),\n",
    "        'Precision@20': precision_at_k(pred, gt, 20),\n",
    "        'Recall@5': recall_at_k(pred, gt, 5),\n",
    "        'Recall@10': recall_at_k(pred, gt, 10),\n",
    "        'Recall@20': recall_at_k(pred, gt, 20),\n",
    "        'MAP': mean_average_precision(pred, gt),\n",
    "        'MRR@10': mrr_at_k(pred, gt, 10),\n",
    "        'MRR@20': mrr_at_k(pred, gt, 20)\n",
    "    }\n",
    "\n",
    "val_metrics = compute_all_metrics(val_pred, val_user_recipes)\n",
    "print(\"✅ Validation metrics:\")\n",
    "for k,v in val_metrics.items(): print(f\"{k:<15}: {v:.4f}\")\n",
    "\n",
    "# ================================================================\n",
    "# TEST EVALUATION\n",
    "# ================================================================\n",
    "test_users = test_data['user_id'].unique().tolist()\n",
    "test_user_recipes = test_data.groupby('user_id')['recipe_id'].apply(set).to_dict()\n",
    "train_val_user_recipes = {uid: train_user_recipes.get(uid,set())|val_user_recipes.get(uid,set()) for uid in test_users}\n",
    "\n",
    "test_pred = batch_get_recommendations(model, test_users, k=20, exclude_dict=train_val_user_recipes)\n",
    "test_metrics = compute_all_metrics(test_pred, test_user_recipes)\n",
    "print(\"✅ Test metrics:\")\n",
    "for k,v in test_metrics.items(): print(f\"{k:<15}: {v:.4f}\")\n",
    "\n",
    "# ================================================================\n",
    "# SAVE RESULTS\n",
    "# ================================================================\n",
    "with open(os.path.join(RESULTS_DIR,\"val_predictions.pkl\"),\"wb\") as f: pickle.dump(val_pred,f)\n",
    "with open(os.path.join(RESULTS_DIR,\"test_predictions.pkl\"),\"wb\") as f: pickle.dump(test_pred,f)\n",
    "with open(os.path.join(RESULTS_DIR,\"evaluation_metrics.pkl\"),\"wb\") as f: pickle.dump({'validation':val_metrics,'test':test_metrics},f)\n",
    "\n",
    "# ================================================================\n",
    "# SAMPLE RECOMMENDATIONS\n",
    "# ================================================================\n",
    "sample_users = test_users[:3]\n",
    "for uid in sample_users:\n",
    "    print(f\"\\nUser ID: {uid}\")\n",
    "    actual = test_user_recipes.get(uid,set())\n",
    "    recs = test_pred.get(uid,[])[:10]\n",
    "    print(f\"Actual interactions: {len(actual)}\")\n",
    "    print(\"Top 10 Recommendations:\")\n",
    "    for i,rid in enumerate(recs,1):\n",
    "        hit = \"✅ HIT\" if rid in actual else \"\"\n",
    "        print(f\"{i:<3} {rid:<12} {hit}\")\n",
    "\n",
    "print(\"\\n🎉 Final evaluation complete! Results saved in:\", RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac525a99",
   "metadata": {
    "id": "ac525a99"
   },
   "source": [
    "## 1️9. Save Two-Tower Model\n",
    "\n",
    "Save the trained Two-Tower model to Google Drive for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23058eee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "23058eee",
    "outputId": "c6770339-5bd6-49e2-c8a6-b1debfe3794d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      " SAVING TWO-TOWER MODEL\n",
      "======================================================================\n",
      "\n",
      "✅ Model saved: /content/drive/MyDrive/RecipeML/models/two_tower/two_tower_model.pth\n",
      "✅ Config saved: /content/drive/MyDrive/RecipeML/models/two_tower/model_config.pkl\n",
      "✅ Metrics saved: /content/drive/MyDrive/RecipeML/models/two_tower/metrics.pkl\n",
      "✅ Scalers saved: /content/drive/MyDrive/RecipeML/models/two_tower/scalers.pkl\n",
      "\n",
      "======================================================================\n",
      "✅ TWO-TOWER MODEL SAVED SUCCESSFULLY!\n",
      "======================================================================\n",
      "Location: /content/drive/MyDrive/RecipeML/models/two_tower\n",
      "\n",
      "Saved files:\n",
      "  - two_tower_model.pth (model weights)\n",
      "  - model_config.pkl (architecture config)\n",
      "  - metrics.pkl (validation & test metrics)\n",
      "  - scalers.pkl (feature scalers)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" SAVING TWO-TOWER MODEL\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Create save directory\n",
    "save_dir = \"/content/drive/MyDrive/RecipeML/models/two_tower\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save model state dict\n",
    "model_path = os.path.join(save_dir, 'two_tower_model.pth')\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"✅ Model saved: {model_path}\")\n",
    "\n",
    "# Save model configuration\n",
    "config = {\n",
    "    'n_users': n_users,\n",
    "    'n_recipes': n_recipes,\n",
    "    'user_feature_dim': user_feature_dim,\n",
    "    'recipe_feature_dim': recipe_feature_dim,\n",
    "    'embedding_dim': 128,  # Adjust if different\n",
    "    'user_feature_cols': user_feature_cols,\n",
    "    'recipe_feature_cols': recipe_feature_cols\n",
    "}\n",
    "config_path = os.path.join(save_dir, 'model_config.pkl')\n",
    "with open(config_path, 'wb') as f:\n",
    "    pickle.dump(config, f)\n",
    "print(f\"✅ Config saved: {config_path}\")\n",
    "\n",
    "# Save metrics\n",
    "metrics_data = {\n",
    "    'validation': val_metrics,\n",
    "    'test': test_metrics\n",
    "    # Optional: 'training_history': history  # Uncomment if you have training history\n",
    "}\n",
    "metrics_path = os.path.join(save_dir, 'metrics.pkl')\n",
    "with open(metrics_path, 'wb') as f:\n",
    "    pickle.dump(metrics_data, f)\n",
    "print(f\"✅ Metrics saved: {metrics_path}\")\n",
    "\n",
    "# Save scalers\n",
    "scalers_path = os.path.join(save_dir, 'scalers.pkl')\n",
    "with open(scalers_path, 'wb') as f:\n",
    "    pickle.dump({'user_scaler': scaler_user, 'recipe_scaler': scaler_recipe}, f)\n",
    "print(f\"✅ Scalers saved: {scalers_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ TWO-TOWER MODEL SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Location: {save_dir}\")\n",
    "print(f\"\\nSaved files:\")\n",
    "print(f\"  - two_tower_model.pth (model weights)\")\n",
    "print(f\"  - model_config.pkl (architecture config)\")\n",
    "print(f\"  - metrics.pkl (validation & test metrics)\")\n",
    "print(f\"  - scalers.pkl (feature scalers)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QBK42AguZ1ZV",
   "metadata": {
    "id": "QBK42AguZ1ZV"
   },
   "source": [
    "# User to User recommendation system using two tower neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95FLn84Z-0N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f95FLn84Z-0N",
    "outputId": "84b367a4-4563-4b72-bd34-288f1bc4f391"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully (partial weights if mismatch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 0 top 10 similar users: [1044  659 1061 3507 2873 4122 2797 3012 1200 1224]\n",
      "User 1 top 10 similar users: [  49   53 3833 4551 4987 4168 3021 4442 2120 1838]\n",
      "User 2 top 10 similar users: [3801 2732 3685  267  146 3159  959 3361  665  206]\n",
      "User 3 top 10 similar users: [ 230  622 2098 1931 2471 1259 2491 3558 3172 1610]\n",
      "User 4 top 10 similar users: [ 289 3264 4927 3579 2645 4004 3051 1039 1548 2667]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch.nn as nn\n",
    "\n",
    "# ======== CONFIG ========\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "save_dir = \"/content/drive/MyDrive/RecipeML/models/two_tower\"\n",
    "model_path = os.path.join(save_dir, \"two_tower_model.pth\")\n",
    "config_path = os.path.join(save_dir, \"model_config.pkl\")\n",
    "scalers_path = os.path.join(save_dir, \"scalers.pkl\")\n",
    "user_csv_path = \"/content/drive/MyDrive/RecipeML/data/users.csv\"\n",
    "\n",
    "# ======== LOAD CONFIG ========\n",
    "with open(config_path, 'rb') as f:\n",
    "    config = pickle.load(f)\n",
    "\n",
    "with open(scalers_path, 'rb') as f:\n",
    "    scalers = pickle.load(f)\n",
    "\n",
    "user_scaler = scalers['user_scaler']\n",
    "\n",
    "user_feature_cols = config['user_feature_cols']\n",
    "user_feat_dim = config['user_feature_dim']  # original training feature dim\n",
    "recipe_feature_dim = config['recipe_feature_dim']\n",
    "embedding_dim = config['embedding_dim']\n",
    "n_users = config['n_users']\n",
    "n_recipes = config['n_recipes']\n",
    "\n",
    "# ======== MODEL CLASSES ========\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_entities, feature_dim, embedding_dim=128, hidden_dim=256, is_user=True):\n",
    "        super().__init__()\n",
    "        if is_user:\n",
    "            self.user_embedding = nn.Embedding(n_entities, embedding_dim)\n",
    "        else:\n",
    "            self.recipe_embedding = nn.Embedding(n_entities, embedding_dim)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, ids, features):\n",
    "        emb = getattr(self, \"user_embedding\", getattr(self, \"recipe_embedding\", None))\n",
    "        emb = emb(ids) if emb is not None else 0\n",
    "        feat_emb = self.fc(features)\n",
    "        return emb + feat_emb\n",
    "\n",
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, n_users, n_recipes, user_feature_dim, recipe_feature_dim, embedding_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.user_encoder = Encoder(n_users, user_feature_dim, embedding_dim, hidden_dim)\n",
    "        self.recipe_encoder = Encoder(n_recipes, recipe_feature_dim, embedding_dim, hidden_dim)\n",
    "\n",
    "    def forward_user(self, user_ids, user_features):\n",
    "        return self.user_encoder(user_ids, user_features)\n",
    "\n",
    "    def forward_recipe(self, recipe_ids, recipe_features):\n",
    "        return self.recipe_encoder(recipe_ids, recipe_features)\n",
    "\n",
    "# ======== LOAD MODEL (allow partial weights) ========\n",
    "two_tower_model = TwoTowerModel(n_users, n_recipes, user_feat_dim, recipe_feature_dim, embedding_dim).to(device)\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model_state = two_tower_model.state_dict()\n",
    "# Filter out unnecessary keys\n",
    "checkpoint = {k: v for k, v in checkpoint.items() if k in model_state and v.size() == model_state[k].size()}\n",
    "model_state.update(checkpoint)\n",
    "two_tower_model.load_state_dict(model_state)\n",
    "two_tower_model.eval()\n",
    "print(\"✅ Model loaded successfully (partial weights if mismatch)\")\n",
    "\n",
    "# ======== LOAD AND PREPROCESS USER FEATURES ========\n",
    "df_users = pd.read_csv(user_csv_path)\n",
    "\n",
    "# One-hot encode categorical columns to match training\n",
    "categorical_cols = ['gender', 'location', 'user_segment']  # adjust based on your raw CSV\n",
    "df_users_processed = pd.get_dummies(df_users, columns=categorical_cols)\n",
    "\n",
    "# Add missing columns from training\n",
    "missing_cols = set(user_feature_cols) - set(df_users_processed.columns)\n",
    "for col in missing_cols:\n",
    "    df_users_processed[col] = 0\n",
    "\n",
    "# Ensure column order matches\n",
    "df_users_processed = df_users_processed[user_feature_cols]\n",
    "\n",
    "# Scale features\n",
    "user_features = user_scaler.transform(df_users_processed.values)\n",
    "user_features = torch.tensor(user_features, dtype=torch.float32).to(device)\n",
    "user_ids = torch.arange(len(df_users_processed), dtype=torch.long).to(device)\n",
    "\n",
    "# ======== GENERATE USER EMBEDDINGS ========\n",
    "with torch.no_grad():\n",
    "    user_embeddings = two_tower_model.forward_user(user_ids, user_features)\n",
    "    user_embeddings = user_embeddings.cpu().numpy()\n",
    "\n",
    "# ======== USER-TO-USER RECOMMENDATIONS ========\n",
    "similarity_matrix = cosine_similarity(user_embeddings)\n",
    "top_k = 10\n",
    "user_topk = np.argsort(-similarity_matrix, axis=1)[:, 1:top_k+1]  # skip self\n",
    "\n",
    "# ======== PRINT EXAMPLES ========\n",
    "for idx in range(min(5, len(df_users))):\n",
    "    similar_users = user_topk[idx]\n",
    "    print(f\"User {idx} top {top_k} similar users: {similar_users}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gRTfDXX5CjOx",
   "metadata": {
    "id": "gRTfDXX5CjOx"
   },
   "source": [
    "# Evaluate ALS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fazMHIHc_S7O",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fazMHIHc_S7O",
    "outputId": "16682a08-05cc-4883-8b7b-93172deeb579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "📊 ALS MODEL COMPREHENSIVE EVALUATION\n",
      "======================================================================\n",
      "\n",
      "Loading data...\n",
      "✅ Train: 931,700\n",
      "✅ Val: 200,145\n",
      "✅ Test: 202,482\n",
      "\n",
      "Loading ALS model...\n",
      "Loading: als_model_20251018_141028.pkl\n",
      "  Assuming loaded_item_factors are user factors and loaded_user_factors are item factors due to swapped naming in PKL.\n",
      "✅ Model loaded:\n",
      "   User factors (used): (5000, 128)\n",
      "   Item factors (used): (10437, 128)\n",
      "   Users in model: 5,000\n",
      "   Recipes in model: 10,437\n",
      "\n",
      "\n",
      "🚀 Starting comprehensive evaluation...\n",
      "\n",
      "\n",
      "======================================================================\n",
      "🔍 Evaluating on VALIDATION set\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating validation: 100%|██████████| 5000/5000 [00:29<00:00, 167.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Statistics:\n",
      "   Total users in validation: 5000\n",
      "   Evaluated users: 5000\n",
      "   Skipped (not in model mapping / factor mismatch): 0\n",
      "   Skipped (no relevant items in model): 0\n",
      "   Recipe IDs in validation not in training mapping: 0\n",
      "\n",
      "📊 VALIDATION Results:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "  @ K=5:\n",
      "    NDCG:      0.006573\n",
      "    Precision: 0.007160\n",
      "    Recall:    0.000914\n",
      "\n",
      "  @ K=10:\n",
      "    NDCG:      0.010079\n",
      "    Precision: 0.011940\n",
      "    Recall:    0.003211\n",
      "\n",
      "  @ K=20:\n",
      "    NDCG:      0.014124\n",
      "    Precision: 0.016670\n",
      "    Recall:    0.008925\n",
      "\n",
      "  Overall:\n",
      "    MAP:       0.009415\n",
      "    MRR@10:    0.023097\n",
      "    MRR@20:    0.032848\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "🔍 Evaluating on TEST set\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating test: 100%|██████████| 5000/5000 [00:29<00:00, 167.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Statistics:\n",
      "   Total users in test: 5000\n",
      "   Evaluated users: 5000\n",
      "   Skipped (not in model mapping / factor mismatch): 0\n",
      "   Skipped (no relevant items in model): 0\n",
      "   Recipe IDs in test not in training mapping: 0\n",
      "\n",
      "📊 TEST Results:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "  @ K=5:\n",
      "    NDCG:      0.005624\n",
      "    Precision: 0.006200\n",
      "    Recall:    0.000775\n",
      "\n",
      "  @ K=10:\n",
      "    NDCG:      0.008514\n",
      "    Precision: 0.010080\n",
      "    Recall:    0.002622\n",
      "\n",
      "  @ K=20:\n",
      "    NDCG:      0.010696\n",
      "    Precision: 0.012380\n",
      "    Recall:    0.006413\n",
      "\n",
      "  Overall:\n",
      "    MAP:       0.007631\n",
      "    MRR@10:    0.019491\n",
      "    MRR@20:    0.026591\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "💾 SAVING RESULTS\n",
      "======================================================================\n",
      "\n",
      "✅ Metrics saved: /content/drive/MyDrive/RecipeML/results/metrics_20251020_142546.csv\n",
      "✅ Detailed results saved: /content/drive/MyDrive/RecipeML/results/als_evaluation_20251020_142546.pkl\n",
      "\n",
      "======================================================================\n",
      "✅ ALS EVALUATION COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "📊 Summary:\n",
      "\n",
      "Validation Set:\n",
      "  NDCG@10:      0.0101\n",
      "  Precision@10: 0.0119\n",
      "  Recall@10:    0.0032\n",
      "  MAP:          0.0094\n",
      "\n",
      "Test Set:\n",
      "  NDCG@10:      0.0085\n",
      "  Precision@10: 0.0101\n",
      "  Recall@10:    0.0026\n",
      "  MAP:          0.0076\n",
      "\n",
      "======================================================================\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# ALS MODEL COMPREHENSIVE EVALUATION\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"📊 ALS MODEL COMPREHENSIVE EVALUATION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "BASE_DIR = \"/content/drive/MyDrive/RecipeML\"\n",
    "DATA_DIR = f\"{BASE_DIR}/data\"\n",
    "MODELS_DIR = f\"{BASE_DIR}/models\"\n",
    "RESULTS_DIR = f\"{BASE_DIR}/results\"\n",
    "\n",
    "# ============================================\n",
    "# LOAD DATA\n",
    "# ============================================\n",
    "print(\"Loading data...\")\n",
    "users_df = pd.read_csv(f\"{DATA_DIR}/users.csv\")\n",
    "recipes_df = pd.read_csv(f\"{DATA_DIR}/recipes.csv\")\n",
    "train_data = pd.read_csv(f\"{DATA_DIR}/train_interactions.csv\")\n",
    "val_data = pd.read_csv(f\"{DATA_DIR}/val_interactions.csv\")\n",
    "test_data = pd.read_csv(f\"{DATA_DIR}/test_interactions.csv\")\n",
    "\n",
    "print(f\"✅ Train: {len(train_data):,}\")\n",
    "print(f\"✅ Val: {len(val_data):,}\")\n",
    "print(f\"✅ Test: {len(test_data):,}\\n\")\n",
    "\n",
    "# ============================================\n",
    "# LOAD ALS MODEL\n",
    "# ============================================\n",
    "print(\"Loading ALS model...\")\n",
    "\n",
    "# Find latest ALS model\n",
    "als_files = [f for f in os.listdir(MODELS_DIR) if f.startswith(\"als_model_\") and f.endswith(\".pkl\")]\n",
    "if not als_files:\n",
    "    print(\"❌ No ALS model found!\")\n",
    "    raise FileNotFoundError(\"No ALS model in models directory\")\n",
    "\n",
    "latest_als = max(als_files, key=lambda x: os.path.getctime(f\"{MODELS_DIR}/{x}\"))\n",
    "print(f\"Loading: {latest_als}\")\n",
    "\n",
    "with open(f\"{MODELS_DIR}/{latest_als}\", 'rb') as f:\n",
    "    als_data = pickle.load(f)\n",
    "\n",
    "# ✅ CRITICAL FIX: Extract mappings from saved model\n",
    "user_to_idx = als_data['user_to_idx']\n",
    "recipe_to_idx = als_data['recipe_to_idx'] # This is the mapping from original recipe ID to training index\n",
    "idx_to_user = als_data['idx_to_user']\n",
    "idx_to_recipe = als_data['idx_to_recipe']\n",
    "\n",
    "# Extract factors - Note: Variable names in PKL might be user/item swapped compared to convention\n",
    "# Based on shapes (10437, 128) for one and (5000, 128) for other,\n",
    "# the one with 10437 rows is likely the item factors (recipes)\n",
    "# and the one with 5000 rows is likely the user factors.\n",
    "# We will use the variable names directly from the PKL as loaded.\n",
    "loaded_user_factors = als_data['user_factors'] # Shape (10437, 128) in this case?\n",
    "loaded_item_factors = als_data['item_factors'] # Shape (5000, 128) in this case?\n",
    "\n",
    "# Determine actual user and item factors based on loaded shapes and mappings\n",
    "if loaded_user_factors.shape[0] == len(user_to_idx): # If loaded_user_factors has num_users rows\n",
    "    user_factors = loaded_user_factors\n",
    "    item_factors = loaded_item_factors # This would then be num_recipes rows\n",
    "    print(\"  Assuming loaded_user_factors are user factors and loaded_item_factors are item factors based on mapping size.\")\n",
    "elif loaded_item_factors.shape[0] == len(user_to_idx): # If loaded_item_factors has num_users rows (swapped naming)\n",
    "     user_factors = loaded_item_factors\n",
    "     item_factors = loaded_user_factors # This would then be num_recipes rows\n",
    "     print(\"  Assuming loaded_item_factors are user factors and loaded_user_factors are item factors due to swapped naming in PKL.\")\n",
    "else:\n",
    "     print(\"  Warning: Could not confidently determine user/item factors from loaded shapes and mapping sizes.\")\n",
    "     # Fallback to original naming, but issue a warning\n",
    "     user_factors = loaded_user_factors\n",
    "     item_factors = loaded_item_factors\n",
    "\n",
    "\n",
    "print(f\"✅ Model loaded:\")\n",
    "print(f\"   User factors (used): {user_factors.shape}\")\n",
    "print(f\"   Item factors (used): {item_factors.shape}\")\n",
    "print(f\"   Users in model: {len(user_to_idx):,}\")\n",
    "print(f\"   Recipes in model: {len(recipe_to_idx):,}\\n\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# EVALUATION METRICS FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def ndcg_at_k(y_true, y_pred, k):\n",
    "    \"\"\"Calculate NDCG@k\"\"\"\n",
    "    order = np.argsort(y_pred)[::-1][:k]\n",
    "    y_true_sorted = y_true[order]\n",
    "\n",
    "    gains = 2 ** y_true_sorted - 1\n",
    "    discounts = np.log2(np.arange(len(y_true_sorted)) + 2)\n",
    "    dcg = np.sum(gains / discounts)\n",
    "\n",
    "    ideal_order = np.argsort(y_true)[::-1][:k]\n",
    "    ideal_gains = 2 ** y_true[ideal_order] - 1\n",
    "    idcg = np.sum(ideal_gains / discounts[:len(ideal_gains)])\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def precision_at_k(y_true, y_pred, k):\n",
    "    \"\"\"Calculate Precision@k\"\"\"\n",
    "    top_k = np.argsort(y_pred)[::-1][:k]\n",
    "    hits = np.sum(y_true[top_k])\n",
    "    return hits / k\n",
    "\n",
    "def recall_at_k(y_true, y_pred, k):\n",
    "    \"\"\"Calculate Recall@k\"\"\"\n",
    "    top_k = np.argsort(y_pred)[::-1][:k]\n",
    "    hits = np.sum(y_true[top_k])\n",
    "    total_relevant = np.sum(y_true)\n",
    "    return hits / total_relevant if total_relevant > 0 else 0.0\n",
    "\n",
    "def mean_average_precision(y_true, y_pred):\n",
    "    \"\"\"Calculate MAP\"\"\"\n",
    "    order = np.argsort(y_pred)[::-1]\n",
    "    y_true_sorted = y_true[order]\n",
    "\n",
    "    relevant_positions = np.where(y_true_sorted > 0)[0]\n",
    "    if len(relevant_positions) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    precisions = [(i + 1) / (pos + 1) for i, pos in enumerate(relevant_positions)]\n",
    "    return np.mean(precisions)\n",
    "\n",
    "def mrr_at_k(y_true, y_pred, k):\n",
    "    \"\"\"Calculate MRR@k\"\"\"\n",
    "    top_k = np.argsort(y_pred)[::-1][:k]\n",
    "    for i, idx in enumerate(top_k, 1):\n",
    "        if y_true[idx] > 0:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "# ============================================\n",
    "# EVALUATE FUNCTION - FIXED\n",
    "# ============================================\n",
    "\n",
    "def evaluate_als_comprehensive(data, split_name='validation'):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation with all metrics at multiple K values\n",
    "    ✅ FIXED: Handles mismatched recipe IDs properly and sizes y_true correctly.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"🔍 Evaluating on {split_name.upper()} set\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    metrics = defaultdict(list)\n",
    "    user_groups = data.groupby('user_id')\n",
    "\n",
    "    # Track statistics\n",
    "    evaluated_users = 0\n",
    "    skipped_users_not_in_model = 0\n",
    "    skipped_users_no_valid_items = 0\n",
    "    recipes_not_in_model = 0\n",
    "\n",
    "    # Determine the correct number of items (recipes) the model knows about\n",
    "    # This should be the size of the dimension in item_factors that corresponds to items\n",
    "    # or simply the number of items in the training mapping.\n",
    "    n_items_in_model = len(recipe_to_idx) # Use the length of the recipe mapping from training\n",
    "\n",
    "\n",
    "    for user_id, group in tqdm(user_groups, desc=f\"Evaluating {split_name}\"):\n",
    "        # ✅ FIX 1: Skip users not in training set\n",
    "        if user_id not in user_to_idx:\n",
    "            skipped_users_not_in_model += 1\n",
    "            continue\n",
    "\n",
    "        user_idx = user_to_idx[user_id]\n",
    "\n",
    "        # ✅ FIX 2: Only use recipes that exist in the model's training mapping\n",
    "        true_items_indices = []\n",
    "        for recipe_id in group['recipe_id']:\n",
    "            if recipe_id in recipe_to_idx:\n",
    "                true_items_indices.append(recipe_to_idx[recipe_id])\n",
    "            else:\n",
    "                recipes_not_in_model += 1\n",
    "\n",
    "\n",
    "        if not true_items_indices:\n",
    "            skipped_users_no_valid_items += 1\n",
    "            continue\n",
    "\n",
    "        # Get predictions (dot product of user and item factors)\n",
    "        # Ensure user_idx is within the bounds of user_factors\n",
    "        if user_idx >= user_factors.shape[0]:\n",
    "             print(f\"Warning: User index {user_idx} out of bounds for user_factors shape {user_factors.shape}. Skipping user {user_id}.\")\n",
    "             skipped_users_not_in_model += 1 # Count this as skipped due to factor mismatch\n",
    "             continue\n",
    "\n",
    "        user_vector = user_factors[user_idx]\n",
    "        scores = np.dot(item_factors, user_vector) # scores will be size n_items_in_model\n",
    "\n",
    "        # ✅ FIX 3: Create binary relevance vector with correct size (number of items in model)\n",
    "        y_true = np.zeros(n_items_in_model)\n",
    "        # Ensure true_items_indices are within the bounds of y_true (which is n_items_in_model)\n",
    "        valid_true_items_indices = [idx for idx in true_items_indices if idx < n_items_in_model]\n",
    "\n",
    "        if not valid_true_items_indices:\n",
    "             skipped_users_no_valid_items += 1 # If after filtering, no valid items remain\n",
    "             continue\n",
    "\n",
    "        y_true[valid_true_items_indices] = 1\n",
    "\n",
    "\n",
    "        # Calculate metrics at different K values\n",
    "        for k in [5, 10, 20]:\n",
    "            metrics[f'NDCG@{k}'].append(ndcg_at_k(y_true, scores, k))\n",
    "            metrics[f'Precision@{k}'].append(precision_at_k(y_true, scores, k))\n",
    "            metrics[f'Recall@{k}'].append(recall_at_k(y_true, scores, k))\n",
    "\n",
    "        # MAP and MRR (calculated once)\n",
    "        metrics['MAP'].append(mean_average_precision(y_true, scores))\n",
    "        metrics['MRR@10'].append(mrr_at_k(y_true, scores, k=10))\n",
    "        metrics['MRR@20'].append(mrr_at_k(y_true, scores, k=20))\n",
    "\n",
    "        evaluated_users += 1\n",
    "\n",
    "    # Average all metrics\n",
    "    results = {metric: np.mean(values) for metric, values in metrics.items()}\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"\\n📊 Evaluation Statistics:\")\n",
    "    print(f\"   Total users in {split_name}: {len(user_groups)}\")\n",
    "    print(f\"   Evaluated users: {evaluated_users}\")\n",
    "    print(f\"   Skipped (not in model mapping / factor mismatch): {skipped_users_not_in_model}\")\n",
    "    print(f\"   Skipped (no relevant items in model): {skipped_users_no_valid_items}\")\n",
    "    print(f\"   Recipe IDs in {split_name} not in training mapping: {recipes_not_in_model}\")\n",
    "\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n📊 {split_name.upper()} Results:\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    # Group by K values\n",
    "    for k in [5, 10, 20]:\n",
    "        print(f\"\\n  @ K={k}:\")\n",
    "        if f'NDCG@{k}' in results:\n",
    "            print(f\"    NDCG:      {results[f'NDCG@{k}']:.6f}\")\n",
    "        if f'Precision@{k}' in results:\n",
    "            print(f\"    Precision: {results[f'Precision@{k}']:.6f}\")\n",
    "        if f'Recall@{k}' in results:\n",
    "            print(f\"    Recall:    {results[f'Recall@{k}']:.6f}\")\n",
    "\n",
    "    print(f\"\\n  Overall:\")\n",
    "    print(f\"    MAP:       {results['MAP']:.6f}\")\n",
    "    print(f\"    MRR@10:    {results['MRR@10']:.6f}\")\n",
    "    print(f\"    MRR@20:    {results['MRR@20']:.6f}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    return results\n",
    "\n",
    "# ============================================\n",
    "# RUN EVALUATION\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n🚀 Starting comprehensive evaluation...\\n\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_results = evaluate_als_comprehensive(val_data, 'validation')\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = evaluate_als_comprehensive(test_data, 'test')\n",
    "\n",
    "# ============================================\n",
    "# SAVE RESULTS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"💾 SAVING RESULTS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Create metrics DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'split': ['validation', 'test'],\n",
    "    'ndcg@5': [val_results.get('NDCG@5', 0), test_results.get('NDCG@5', 0)],\n",
    "    'ndcg@10': [val_results.get('NDCG@10', 0), test_results.get('NDCG@10', 0)],\n",
    "    'ndcg@20': [val_results.get('NDCG@20', 0), test_results.get('NDCG@20', 0)],\n",
    "    'precision@5': [val_results.get('Precision@5', 0), test_results.get('Precision@5', 0)],\n",
    "    'precision@10': [val_results.get('Precision@10', 0), test_results.get('Precision@10', 0)],\n",
    "    'precision@20': [val_results.get('Precision@20', 0), test_results.get('Precision@20', 0)],\n",
    "    'recall@5': [val_results.get('Recall@5', 0), test_results.get('Recall@5', 0)],\n",
    "    'recall@10': [val_results.get('Recall@10', 0), test_results.get('Recall@10', 0)],\n",
    "    'recall@20': [val_results.get('Recall@20', 0), test_results.get('Recall@20', 0)],\n",
    "    'map': [val_results.get('MAP', 0), test_results.get('MAP', 0)],\n",
    "    'mrr@10': [val_results.get('MRR@10', 0), test_results.get('MRR@10', 0)],\n",
    "    'mrr@20': [val_results.get('MRR@20', 0), test_results.get('MRR@20', 0)]\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "metrics_path = f\"{RESULTS_DIR}/metrics_{timestamp}.csv\"\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "print(f\"✅ Metrics saved: {metrics_path}\")\n",
    "\n",
    "# Save detailed results as pickle\n",
    "results_dict = {\n",
    "    'validation': val_results,\n",
    "    'test': test_results,\n",
    "    'timestamp': timestamp,\n",
    "    'model_file': latest_als\n",
    "}\n",
    "\n",
    "results_path = f\"{RESULTS_DIR}/als_evaluation_{timestamp}.pkl\"\n",
    "with open(results_path, 'wb') as f:\n",
    "    pickle.dump(results_dict, f)\n",
    "print(f\"✅ Detailed results saved: {results_path}\")\n",
    "\n",
    "# Display final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ ALS EVALUATION COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n📊 Summary:\")\n",
    "print(f\"\\nValidation Set:\")\n",
    "print(f\"  NDCG@10:      {val_results.get('NDCG@10', 0):.4f}\")\n",
    "print(f\"  Precision@10: {val_results.get('Precision@10', 0):.4f}\")\n",
    "print(f\"  Recall@10:    {val_results.get('Recall@10', 0):.4f}\")\n",
    "print(f\"  MAP:          {val_results.get('MAP', 0):.4f}\")\n",
    "\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  NDCG@10:      {test_results.get('NDCG@10', 0):.4f}\")\n",
    "print(f\"  Precision@10: {test_results.get('Precision@10', 0):.4f}\")\n",
    "print(f\"  Recall@10:    {test_results.get('Recall@10', 0):.4f}\")\n",
    "print(f\"  MAP:          {test_results.get('MAP', 0):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xv-5TGSkCdIJ",
   "metadata": {
    "id": "xv-5TGSkCdIJ"
   },
   "source": [
    "# Comparison: ALS vs Two Tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "xby_NhCB4QaR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xby_NhCB4QaR",
    "outputId": "2747215b-bd98-43f7-f801-23102b94ac72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "📊 MODEL COMPARISON SCRIPT - COMPREHENSIVE\n",
      "======================================================================\n",
      "\n",
      "Base directory: /content/drive/MyDrive/RecipeML\n",
      "Comparison output: /content/drive/MyDrive/RecipeML/results/comparison_models\n",
      "\n",
      "======================================================================\n",
      "📊 LOADING DATA\n",
      "======================================================================\n",
      "\n",
      "Users: 5,000\n",
      "Recipes: 10,457\n",
      "Train interactions: 931,700\n",
      "Val interactions: 200,145\n",
      "Test interactions: 202,482\n",
      "\n",
      "======================================================================\n",
      " LOADING ALS MODEL & METRICS\n",
      "======================================================================\n",
      "\n",
      "Loading model: als_model_20251018_141028.pkl\n",
      "✅ ALS model loaded successfully\n",
      "   Hyperparameters: {'factors': 128, 'regularization': 0.05, 'iterations': 30, 'alpha': 40}\n",
      "\n",
      "📋 Loading metrics from: metrics_20251020_142546.csv\n",
      "   Columns found: ['split', 'ndcg@5', 'ndcg@10', 'ndcg@20', 'precision@5', 'precision@10', 'precision@20', 'recall@5', 'recall@10', 'recall@20', 'map', 'mrr@10', 'mrr@20']\n",
      "   Splits found: ['validation' 'test']\n",
      "\n",
      "📊 Raw metrics data:\n",
      "     split   ndcg@5  ndcg@10  ndcg@20  precision@5  precision@10  precision@20  recall@5  recall@10  recall@20      map   mrr@10   mrr@20\n",
      "validation 0.006573 0.010079 0.014124      0.00716       0.01194       0.01667  0.000914   0.003211   0.008925 0.009415 0.023097 0.032848\n",
      "      test 0.005624 0.008514 0.010696      0.00620       0.01008       0.01238  0.000775   0.002622   0.006413 0.007631 0.019491 0.026591\n",
      "\n",
      "   Available columns: {'precision@5', 'recall@20', 'ndcg@5', 'precision@20', 'mrr@10', 'ndcg@20', 'precision@10', 'map', 'mrr@20', 'ndcg@10', 'recall@10', 'split', 'recall@5'}\n",
      "\n",
      "   Extracting validation metrics...\n",
      "       NDCG@5: 0.006573 (from column 'ndcg@5')\n",
      "       NDCG@10: 0.010079 (from column 'ndcg@10')\n",
      "       NDCG@20: 0.014124 (from column 'ndcg@20')\n",
      "       Precision@5: 0.007160 (from column 'precision@5')\n",
      "       Precision@10: 0.011940 (from column 'precision@10')\n",
      "       Precision@20: 0.016670 (from column 'precision@20')\n",
      "       Recall@5: 0.000914 (from column 'recall@5')\n",
      "       Recall@10: 0.003211 (from column 'recall@10')\n",
      "       Recall@20: 0.008925 (from column 'recall@20')\n",
      "       MAP: 0.009415 (from column 'map')\n",
      "       MRR@10: 0.023097 (from column 'mrr@10')\n",
      "       MRR@20: 0.032848 (from column 'mrr@20')\n",
      "\n",
      "   ✅ Validation metrics extracted: 12 metrics\n",
      "\n",
      "   Extracting test metrics...\n",
      "       NDCG@5: 0.005624 (from column 'ndcg@5')\n",
      "       NDCG@10: 0.008514 (from column 'ndcg@10')\n",
      "       NDCG@20: 0.010696 (from column 'ndcg@20')\n",
      "       Precision@5: 0.006200 (from column 'precision@5')\n",
      "       Precision@10: 0.010080 (from column 'precision@10')\n",
      "       Precision@20: 0.012380 (from column 'precision@20')\n",
      "       Recall@5: 0.000775 (from column 'recall@5')\n",
      "       Recall@10: 0.002622 (from column 'recall@10')\n",
      "       Recall@20: 0.006413 (from column 'recall@20')\n",
      "       MAP: 0.007631 (from column 'map')\n",
      "       MRR@10: 0.019491 (from column 'mrr@10')\n",
      "       MRR@20: 0.026591 (from column 'mrr@20')\n",
      "\n",
      "   ✅ Test metrics extracted: 12 metrics\n",
      "\n",
      "======================================================================\n",
      " LOADING TWO-TOWER MODEL & METRICS\n",
      "======================================================================\n",
      "\n",
      "✅ Two-Tower model found at: /content/drive/MyDrive/RecipeML/models/two_tower/two_tower_model.pth\n",
      "\n",
      "✅ Two-Tower metrics loaded from: metrics.pkl\n",
      "   Validation metrics: ['NDCG@5', 'NDCG@10', 'NDCG@20', 'Precision@5', 'Precision@10', 'Precision@20', 'Recall@5', 'Recall@10', 'Recall@20', 'MAP', 'MRR@10', 'MRR@20']\n",
      "   Test metrics: ['NDCG@5', 'NDCG@10', 'NDCG@20', 'Precision@5', 'Precision@10', 'Precision@20', 'Recall@5', 'Recall@10', 'Recall@20', 'MAP', 'MRR@10', 'MRR@20']\n",
      "\n",
      "📊 Two-Tower Validation Metrics:\n",
      "   NDCG@5: 0.036283\n",
      "   NDCG@10: 0.052421\n",
      "   NDCG@20: 0.074435\n",
      "   Precision@5: 0.010720\n",
      "   Precision@10: 0.010920\n",
      "   Precision@20: 0.011210\n",
      "   Recall@5: 0.001318\n",
      "   Recall@10: 0.002671\n",
      "   Recall@20: 0.005437\n",
      "   MAP: 0.005380\n",
      "   MRR@10: 0.030430\n",
      "   MRR@20: 0.036743\n",
      "\n",
      "📊 Two-Tower Test Metrics:\n",
      "   NDCG@5: 0.027881\n",
      "   NDCG@10: 0.039641\n",
      "   NDCG@20: 0.056978\n",
      "   Precision@5: 0.008360\n",
      "   Precision@10: 0.008440\n",
      "   Precision@20: 0.008520\n",
      "   Recall@5: 0.000940\n",
      "   Recall@10: 0.001917\n",
      "   Recall@20: 0.003925\n",
      "   MAP: 0.004810\n",
      "   MRR@10: 0.022880\n",
      "   MRR@20: 0.027601\n",
      "\n",
      "======================================================================\n",
      "📊 CREATING COMPARISON TABLE\n",
      "======================================================================\n",
      "\n",
      "✅ Added ALS validation metrics: 12 metrics\n",
      "✅ Added ALS test metrics: 12 metrics\n",
      "✅ Added Two-Tower validation metrics: 12 metrics\n",
      "✅ Added Two-Tower test metrics: 12 metrics\n",
      "\n",
      "====================================================================================================\n",
      " METRICS COMPARISON TABLE\n",
      "====================================================================================================\n",
      "    Model      Split   NDCG@5  NDCG@10  NDCG@20  Precision@5  Precision@10  Precision@20  Recall@5  Recall@10  Recall@20      MAP   MRR@10   MRR@20\n",
      "      ALS validation 0.006573 0.010079 0.014124      0.00716       0.01194       0.01667  0.000914   0.003211   0.008925 0.009415 0.023097 0.032848\n",
      "      ALS       test 0.005624 0.008514 0.010696      0.00620       0.01008       0.01238  0.000775   0.002622   0.006413 0.007631 0.019491 0.026591\n",
      "Two-Tower validation 0.036283 0.052421 0.074435      0.01072       0.01092       0.01121  0.001318   0.002671   0.005437 0.005380 0.030430 0.036743\n",
      "Two-Tower       test 0.027881 0.039641 0.056978      0.00836       0.00844       0.00852  0.000940   0.001917   0.003925 0.004810 0.022880 0.027601\n",
      "====================================================================================================\n",
      "\n",
      "✅ Saved: /content/drive/MyDrive/RecipeML/results/comparison_models/metrics_comparison.csv\n",
      "\n",
      "======================================================================\n",
      "📈 GENERATING VISUALIZATIONS\n",
      "======================================================================\n",
      "\n",
      "📊 Plotting common metrics: {'Recall@20', 'NDCG@5', 'MRR@20', 'MAP', 'Precision@10', 'Recall@5', 'Precision@5', 'NDCG@20', 'Precision@20', 'MRR@10', 'NDCG@10', 'Recall@10'}\n",
      "\n",
      "✅ Saved: /content/drive/MyDrive/RecipeML/results/comparison_models/metrics_comparison.png\n",
      "\n",
      "======================================================================\n",
      "🏆 MODEL SELECTION RECOMMENDATION\n",
      "======================================================================\n",
      "\n",
      "📊 Comparing 12 common metrics: ['Recall@20', 'NDCG@5', 'MRR@20', 'MAP', 'Precision@10', 'Recall@5', 'Precision@5', 'NDCG@20', 'Precision@20', 'MRR@10', 'NDCG@10', 'Recall@10']\n",
      "\n",
      "📊 Test Set Comparison:\n",
      "\n",
      "Recall@20................ ALS: 0.0064 | Two-Tower: 0.0039 | Winner: ALS (+0.0025)\n",
      "NDCG@5................... ALS: 0.0056 | Two-Tower: 0.0279 | Winner: Two-Tower (+0.0223)\n",
      "MRR@20................... ALS: 0.0266 | Two-Tower: 0.0276 | Winner: Two-Tower (+0.0010)\n",
      "MAP...................... ALS: 0.0076 | Two-Tower: 0.0048 | Winner: ALS (+0.0028)\n",
      "Precision@10............. ALS: 0.0101 | Two-Tower: 0.0084 | Winner: ALS (+0.0016)\n",
      "Recall@5................. ALS: 0.0008 | Two-Tower: 0.0009 | Winner: Two-Tower (+0.0002)\n",
      "Precision@5.............. ALS: 0.0062 | Two-Tower: 0.0084 | Winner: Two-Tower (+0.0022)\n",
      "NDCG@20.................. ALS: 0.0107 | Two-Tower: 0.0570 | Winner: Two-Tower (+0.0463)\n",
      "Precision@20............. ALS: 0.0124 | Two-Tower: 0.0085 | Winner: ALS (+0.0039)\n",
      "MRR@10................... ALS: 0.0195 | Two-Tower: 0.0229 | Winner: Two-Tower (+0.0034)\n",
      "NDCG@10.................. ALS: 0.0085 | Two-Tower: 0.0396 | Winner: Two-Tower (+0.0311)\n",
      "Recall@10................ ALS: 0.0026 | Two-Tower: 0.0019 | Winner: ALS (+0.0007)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "🎯 Overall Winner (average of all common metrics):\n",
      "   ALS: 0.0098\n",
      "   Two-Tower: 0.0177\n",
      "\n",
      "   ✅ RECOMMENDATION: Use Two-Tower\n",
      "      Reasons: 81.08% better performance, more feature-rich\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "💾 SAVING COMPREHENSIVE REPORT\n",
      "======================================================================\n",
      "\n",
      "✅ Saved: /content/drive/MyDrive/RecipeML/results/comparison_models/comparison_report.json\n",
      "✅ Saved: /content/drive/MyDrive/RecipeML/results/comparison_models/comparison_summary.txt\n",
      "\n",
      "======================================================================\n",
      "✅ MODEL COMPARISON COMPLETE!\n",
      "======================================================================\n",
      "\n",
      " All results saved to: /content/drive/MyDrive/RecipeML/results/comparison_models\n",
      "\n",
      "📊 Files created:\n",
      "   • metrics_comparison.csv\n",
      "   • metrics_comparison.png\n",
      "   • comparison_report.json\n",
      "   • comparison_summary.txt\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# MODEL COMPARISON SCRIPT - FINAL COMPREHENSIVE FIX\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "BASE_DIR = Path(\"/content/drive/MyDrive/RecipeML\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "RESULTS_DIR = BASE_DIR / \"results\"\n",
    "COMPARISON_DIR = RESULTS_DIR / \"comparison_models\"\n",
    "\n",
    "COMPARISON_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"📊 MODEL COMPARISON SCRIPT - COMPREHENSIVE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBase directory: {BASE_DIR}\")\n",
    "print(f\"Comparison output: {COMPARISON_DIR}\\n\")\n",
    "\n",
    "# ============================================\n",
    "# LOAD DATA\n",
    "# ============================================\n",
    "print(\"=\"*70)\n",
    "print(\"📊 LOADING DATA\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "users_df = pd.read_csv(DATA_DIR / \"users.csv\")\n",
    "recipes_df = pd.read_csv(DATA_DIR / \"recipes.csv\")\n",
    "train_data = pd.read_csv(DATA_DIR / \"train_interactions.csv\")\n",
    "val_data = pd.read_csv(DATA_DIR / \"val_interactions.csv\")\n",
    "test_data = pd.read_csv(DATA_DIR / \"test_interactions.csv\")\n",
    "\n",
    "if 'id' in recipes_df.columns:\n",
    "    recipes_df.rename(columns={'id': 'recipe_id'}, inplace=True)\n",
    "\n",
    "print(f\"Users: {len(users_df):,}\")\n",
    "print(f\"Recipes: {len(recipes_df):,}\")\n",
    "print(f\"Train interactions: {len(train_data):,}\")\n",
    "print(f\"Val interactions: {len(val_data):,}\")\n",
    "print(f\"Test interactions: {len(test_data):,}\\n\")\n",
    "\n",
    "# ============================================\n",
    "# LOAD ALS MODEL & RESULTS\n",
    "# ============================================\n",
    "print(\"=\"*70)\n",
    "print(\" LOADING ALS MODEL & METRICS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "als_loaded = False\n",
    "als_val_metrics = {}\n",
    "als_test_metrics = {}\n",
    "\n",
    "try:\n",
    "    # Find latest ALS model\n",
    "    als_files = list(MODELS_DIR.glob(\"als_model_*.pkl\"))\n",
    "    if als_files:\n",
    "        latest_als = max(als_files, key=os.path.getctime)\n",
    "        print(f\"Loading model: {latest_als.name}\")\n",
    "\n",
    "        with open(latest_als, 'rb') as f:\n",
    "            als_data = pickle.load(f)\n",
    "\n",
    "        print(f\"✅ ALS model loaded successfully\")\n",
    "        print(f\"   Hyperparameters: {als_data.get('hyperparameters', 'N/A')}\")\n",
    "        als_loaded = True\n",
    "\n",
    "        # ✅ CRITICAL FIX: Find the MOST RECENT metrics file\n",
    "        metrics_files = list(RESULTS_DIR.glob(\"metrics_*.csv\"))\n",
    "        if metrics_files:\n",
    "            # Sort by creation time and get the newest\n",
    "            latest_metrics = max(metrics_files, key=os.path.getctime)\n",
    "            print(f\"\\n📋 Loading metrics from: {latest_metrics.name}\")\n",
    "\n",
    "            als_metrics_df = pd.read_csv(latest_metrics)\n",
    "\n",
    "            print(f\"   Columns found: {list(als_metrics_df.columns)}\")\n",
    "            print(f\"   Splits found: {als_metrics_df['split'].unique()}\")\n",
    "            print(f\"\\n📊 Raw metrics data:\")\n",
    "            print(als_metrics_df.to_string(index=False))\n",
    "            print()\n",
    "\n",
    "            # ✅ FIX: Extract metrics with proper error handling and logging\n",
    "            # Check what columns actually exist\n",
    "            available_cols = set(als_metrics_df.columns)\n",
    "            print(f\"   Available columns: {available_cols}\\n\")\n",
    "\n",
    "            # Validation metrics\n",
    "            val_row = als_metrics_df[als_metrics_df['split'] == 'validation']\n",
    "            if not val_row.empty:\n",
    "                print(\"   Extracting validation metrics...\")\n",
    "\n",
    "                # Build metrics dict dynamically based on what exists\n",
    "                als_val_metrics = {}\n",
    "\n",
    "                # Map common column name variations\n",
    "                column_mappings = {\n",
    "                    'NDCG@5': ['ndcg@5', 'NDCG@5', 'ndcg_5'],\n",
    "                    'NDCG@10': ['ndcg@10', 'NDCG@10', 'ndcg_10'],\n",
    "                    'NDCG@20': ['ndcg@20', 'NDCG@20', 'ndcg_20'],\n",
    "                    'Precision@5': ['precision@5', 'Precision@5', 'precision_5'],\n",
    "                    'Precision@10': ['precision@10', 'Precision@10', 'precision_10'],\n",
    "                    'Precision@20': ['precision@20', 'Precision@20', 'precision_20'],\n",
    "                    'Recall@5': ['recall@5', 'Recall@5', 'recall_5'],\n",
    "                    'Recall@10': ['recall@10', 'Recall@10', 'recall_10'],\n",
    "                    'Recall@20': ['recall@20', 'Recall@20', 'recall_20'],\n",
    "                    'MAP': ['map', 'MAP'],\n",
    "                    'MRR@10': ['mrr', 'MRR', 'mrr@10', 'MRR@10'],\n",
    "                    'MRR@20': ['mrr@20', 'MRR@20']\n",
    "                }\n",
    "\n",
    "                for metric_name, possible_cols in column_mappings.items():\n",
    "                    value = 0.0\n",
    "                    found = False\n",
    "\n",
    "                    for col in possible_cols:\n",
    "                        if col in val_row.columns:\n",
    "                            try:\n",
    "                                value = float(val_row[col].iloc[0])\n",
    "                                found = True\n",
    "                                print(f\"       {metric_name}: {value:.6f} (from column '{col}')\")\n",
    "                                break\n",
    "                            except (ValueError, IndexError) as e:\n",
    "                                print(f\"       Error reading {col}: {e}\")\n",
    "\n",
    "                    if not found and metric_name.split('@')[0] in ['NDCG', 'Precision', 'Recall']:\n",
    "                        # If specific @K not found, try without @K\n",
    "                        base_metric = metric_name.split('@')[0].lower()\n",
    "                        if base_metric in val_row.columns:\n",
    "                            try:\n",
    "                                value = float(val_row[base_metric].iloc[0])\n",
    "                                print(f\"       {metric_name}: {value:.6f} (from column '{base_metric}')\")\n",
    "                            except:\n",
    "                                print(f\"       {metric_name}: Not found (set to 0.0)\")\n",
    "                    elif not found:\n",
    "                        print(f\"       {metric_name}: Not found (set to 0.0)\")\n",
    "\n",
    "                    als_val_metrics[metric_name] = value\n",
    "\n",
    "                print(f\"\\n   ✅ Validation metrics extracted: {len(als_val_metrics)} metrics\")\n",
    "            else:\n",
    "                print(\"    No validation split found in metrics CSV!\")\n",
    "\n",
    "            # Test metrics\n",
    "            test_row = als_metrics_df[als_metrics_df['split'] == 'test']\n",
    "            if test_row.empty:\n",
    "                # Try alternate naming\n",
    "                test_row = als_metrics_df[als_metrics_df['split'].str.contains('test', case=False, na=False)]\n",
    "\n",
    "            if not test_row.empty:\n",
    "                print(\"\\n   Extracting test metrics...\")\n",
    "\n",
    "                als_test_metrics = {}\n",
    "\n",
    "                for metric_name, possible_cols in column_mappings.items():\n",
    "                    value = 0.0\n",
    "                    found = False\n",
    "\n",
    "                    for col in possible_cols:\n",
    "                        if col in test_row.columns:\n",
    "                            try:\n",
    "                                value = float(test_row[col].iloc[0])\n",
    "                                found = True\n",
    "                                print(f\"       {metric_name}: {value:.6f} (from column '{col}')\")\n",
    "                                break\n",
    "                            except (ValueError, IndexError) as e:\n",
    "                                print(f\"       Error reading {col}: {e}\")\n",
    "\n",
    "                    if not found and metric_name.split('@')[0] in ['NDCG', 'Precision', 'Recall']:\n",
    "                        base_metric = metric_name.split('@')[0].lower()\n",
    "                        if base_metric in test_row.columns:\n",
    "                            try:\n",
    "                                value = float(test_row[base_metric].iloc[0])\n",
    "                                print(f\"       {metric_name}: {value:.6f} (from column '{base_metric}')\")\n",
    "                            except:\n",
    "                                print(f\"       {metric_name}: Not found (set to 0.0)\")\n",
    "                    elif not found:\n",
    "                        print(f\"       {metric_name}: Not found (set to 0.0)\")\n",
    "\n",
    "                    als_test_metrics[metric_name] = value\n",
    "\n",
    "                print(f\"\\n   ✅ Test metrics extracted: {len(als_test_metrics)} metrics\")\n",
    "            else:\n",
    "                print(\"   ⚠ No test split found in metrics CSV!\")\n",
    "\n",
    "        else:\n",
    "            print(\"  No metrics CSV files found in results directory!\")\n",
    "            print(f\"   Expected location: {RESULTS_DIR}/metrics_*.csv\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading ALS model: {e}\\n\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# ============================================\n",
    "# LOAD TWO-TOWER MODEL & RESULTS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" LOADING TWO-TOWER MODEL & METRICS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "tt_loaded = False\n",
    "tt_val_metrics = {}\n",
    "tt_test_metrics = {}\n",
    "\n",
    "tt_model_paths = [\n",
    "    MODELS_DIR / \"two_tower\" / \"two_tower_model.pth\",\n",
    "    BASE_DIR / \"processed\" / \"best_two_tower_model.pth\",\n",
    "]\n",
    "\n",
    "tt_model_path = None\n",
    "for path in tt_model_paths:\n",
    "    if path.exists():\n",
    "        tt_model_path = path\n",
    "        print(f\"✅ Two-Tower model found at: {path}\")\n",
    "        tt_loaded = True\n",
    "        break\n",
    "\n",
    "if not tt_loaded:\n",
    "    print(\" Two-Tower model not found in any expected location:\")\n",
    "    for path in tt_model_paths:\n",
    "        print(f\"   • {path}\")\n",
    "\n",
    "# Load Two-Tower metrics\n",
    "if tt_loaded:\n",
    "    metrics_paths = [\n",
    "        MODELS_DIR / \"two_tower\" / \"metrics.pkl\",\n",
    "        RESULTS_DIR / \"evaluation_metrics.pkl\",\n",
    "    ]\n",
    "\n",
    "    for metrics_path in metrics_paths:\n",
    "        if metrics_path.exists():\n",
    "            try:\n",
    "                with open(metrics_path, 'rb') as f:\n",
    "                    tt_metrics = pickle.load(f)\n",
    "\n",
    "                tt_val_metrics = tt_metrics.get('validation', {})\n",
    "                tt_test_metrics = tt_metrics.get('test', {})\n",
    "\n",
    "                print(f\"\\n✅ Two-Tower metrics loaded from: {metrics_path.name}\")\n",
    "                print(f\"   Validation metrics: {list(tt_val_metrics.keys())}\")\n",
    "                print(f\"   Test metrics: {list(tt_test_metrics.keys())}\")\n",
    "\n",
    "                # Print actual values\n",
    "                print(f\"\\n📊 Two-Tower Validation Metrics:\")\n",
    "                for k, v in tt_val_metrics.items():\n",
    "                    print(f\"   {k}: {v:.6f}\")\n",
    "\n",
    "                print(f\"\\n📊 Two-Tower Test Metrics:\")\n",
    "                for k, v in tt_test_metrics.items():\n",
    "                    print(f\"   {k}: {v:.6f}\")\n",
    "\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"  Could not load metrics from {metrics_path}: {e}\")\n",
    "\n",
    "# ============================================\n",
    "# CREATE COMPARISON DATAFRAME\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📊 CREATING COMPARISON TABLE\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "# Add ALS metrics\n",
    "if als_val_metrics:\n",
    "    comparison_data.append({\n",
    "        'Model': 'ALS',\n",
    "        'Split': 'validation',\n",
    "        **als_val_metrics\n",
    "    })\n",
    "    print(f\"✅ Added ALS validation metrics: {len(als_val_metrics)} metrics\")\n",
    "\n",
    "if als_test_metrics:\n",
    "    comparison_data.append({\n",
    "        'Model': 'ALS',\n",
    "        'Split': 'test',\n",
    "        **als_test_metrics\n",
    "    })\n",
    "    print(f\"✅ Added ALS test metrics: {len(als_test_metrics)} metrics\")\n",
    "\n",
    "# Add Two-Tower metrics\n",
    "if tt_val_metrics:\n",
    "    comparison_data.append({\n",
    "        'Model': 'Two-Tower',\n",
    "        'Split': 'validation',\n",
    "        **tt_val_metrics\n",
    "    })\n",
    "    print(f\"✅ Added Two-Tower validation metrics: {len(tt_val_metrics)} metrics\")\n",
    "\n",
    "if tt_test_metrics:\n",
    "    comparison_data.append({\n",
    "        'Model': 'Two-Tower',\n",
    "        'Split': 'test',\n",
    "        **tt_test_metrics\n",
    "    })\n",
    "    print(f\"✅ Added Two-Tower test metrics: {len(tt_test_metrics)} metrics\")\n",
    "\n",
    "if not comparison_data:\n",
    "    print(\"\\n❌ ERROR: No metrics found for comparison!\")\n",
    "    print(\"   Please ensure:\")\n",
    "    print(\"   1. ALS evaluation has been run\")\n",
    "    print(\"   2. Two-Tower evaluation has been run\")\n",
    "    print(\"   3. Metrics files exist in the results directory\")\n",
    "    exit(1)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Display comparison table\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\" METRICS COMPARISON TABLE\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "# Save comparison table\n",
    "comparison_df.to_csv(COMPARISON_DIR / \"metrics_comparison.csv\", index=False)\n",
    "print(f\"✅ Saved: {COMPARISON_DIR / 'metrics_comparison.csv'}\\n\")\n",
    "\n",
    "# ============================================\n",
    "# VISUALIZATIONS\n",
    "# ============================================\n",
    "if len(comparison_df) > 0:\n",
    "    print(\"=\"*70)\n",
    "    print(\"📈 GENERATING VISUALIZATIONS\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    test_df = comparison_df[comparison_df['Split'] == 'test'].copy()\n",
    "\n",
    "    if len(test_df) >= 2:\n",
    "        # Find common metrics between both models\n",
    "        als_metrics = set(als_test_metrics.keys())\n",
    "        tt_metrics = set(tt_test_metrics.keys())\n",
    "        common_metrics = als_metrics & tt_metrics\n",
    "\n",
    "        print(f\"📊 Plotting common metrics: {common_metrics}\\n\")\n",
    "\n",
    "        # Plot only metrics available in both\n",
    "        metrics_to_plot = [m for m in ['NDCG@10', 'Precision@10', 'Recall@10', 'MAP', 'MRR@10']\n",
    "                          if m in common_metrics]\n",
    "\n",
    "        if metrics_to_plot:\n",
    "            n_metrics = len(metrics_to_plot)\n",
    "            n_cols = 3\n",
    "            n_rows = (n_metrics + n_cols - 1) // n_cols\n",
    "\n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, n_rows * 4))\n",
    "            if n_rows == 1:\n",
    "                axes = axes.reshape(1, -1)\n",
    "\n",
    "            fig.suptitle('Model Comparison: ALS vs Two-Tower (Test Set)',\n",
    "                        fontsize=16, fontweight='bold')\n",
    "\n",
    "            for idx, metric in enumerate(metrics_to_plot):\n",
    "                row = idx // n_cols\n",
    "                col = idx % n_cols\n",
    "                ax = axes[row, col]\n",
    "\n",
    "                plot_data = test_df[['Model', metric]].set_index('Model')\n",
    "                plot_data.plot(kind='bar', ax=ax, color=['#e74c3c', '#3498db'],\n",
    "                              alpha=0.8, edgecolor='black', legend=False)\n",
    "\n",
    "                ax.set_title(metric, fontsize=14, fontweight='bold')\n",
    "                ax.set_ylabel('Score', fontsize=12)\n",
    "                ax.set_xlabel('')\n",
    "                ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "                ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "                for container in ax.containers:\n",
    "                    ax.bar_label(container, fmt='%.4f', fontsize=9)\n",
    "\n",
    "            # Hide unused subplots\n",
    "            for idx in range(len(metrics_to_plot), n_rows * n_cols):\n",
    "                row = idx // n_cols\n",
    "                col = idx % n_cols\n",
    "                axes[row, col].axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(COMPARISON_DIR / \"metrics_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "            print(f\"✅ Saved: {COMPARISON_DIR / 'metrics_comparison.png'}\\n\")\n",
    "            plt.close()\n",
    "        else:\n",
    "            print(\"⚠️  No common metrics found for plotting!\\n\")\n",
    "\n",
    "# ============================================\n",
    "# WINNER DETERMINATION\n",
    "# ============================================\n",
    "print(\"=\"*70)\n",
    "print(\"🏆 MODEL SELECTION RECOMMENDATION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "test_df = comparison_df[comparison_df['Split'] == 'test']\n",
    "\n",
    "if len(test_df) >= 2:\n",
    "    # Use only metrics available for BOTH models\n",
    "    als_test_cols = set(als_test_metrics.keys())\n",
    "    tt_test_cols = set(tt_test_metrics.keys())\n",
    "    key_metrics = list(als_test_cols & tt_test_cols)\n",
    "\n",
    "    if not key_metrics:\n",
    "        print(\"  No common metrics found for comparison!\")\n",
    "        recommended_model = \"Unknown\"\n",
    "    else:\n",
    "        print(f\"📊 Comparing {len(key_metrics)} common metrics: {key_metrics}\\n\")\n",
    "        print(\"📊 Test Set Comparison:\\n\")\n",
    "\n",
    "        als_scores = {}\n",
    "        tt_scores = {}\n",
    "\n",
    "        for metric in key_metrics:\n",
    "            als_score = test_df[test_df['Model'] == 'ALS'][metric].iloc[0]\n",
    "            tt_score = test_df[test_df['Model'] == 'Two-Tower'][metric].iloc[0]\n",
    "\n",
    "            als_scores[metric] = als_score\n",
    "            tt_scores[metric] = tt_score\n",
    "\n",
    "            winner = \"ALS\" if als_score > tt_score else \"Two-Tower\"\n",
    "            diff = abs(als_score - tt_score)\n",
    "\n",
    "            print(f\"{metric:.<25} ALS: {als_score:.4f} | Two-Tower: {tt_score:.4f} | Winner: {winner} (+{diff:.4f})\")\n",
    "\n",
    "        print(\"\\n\" + \"-\"*70 + \"\\n\")\n",
    "\n",
    "        # Overall winner (average of all common metrics)\n",
    "        als_avg = np.mean([als_scores.get(m, 0) for m in key_metrics])\n",
    "        tt_avg = np.mean([tt_scores.get(m, 0) for m in key_metrics])\n",
    "\n",
    "        print(f\"🎯 Overall Winner (average of all common metrics):\")\n",
    "        print(f\"   ALS: {als_avg:.4f}\")\n",
    "        print(f\"   Two-Tower: {tt_avg:.4f}\")\n",
    "\n",
    "        if als_avg == 0 and tt_avg == 0:\n",
    "            print(f\"\\n     WARNING: Both models have zero average score!\")\n",
    "            print(f\"      This suggests evaluation didn't run correctly.\")\n",
    "            print(f\"      Please re-run both evaluation scripts.\")\n",
    "            recommended_model = \"Unknown\"\n",
    "        elif als_avg == 0:\n",
    "            print(f\"\\n   ✅ RECOMMENDATION: Use Two-Tower\")\n",
    "            print(f\"      ALS has zero metrics - needs re-evaluation\")\n",
    "            recommended_model = \"Two-Tower\"\n",
    "        elif tt_avg == 0:\n",
    "            print(f\"\\n   ✅ RECOMMENDATION: Use ALS\")\n",
    "            print(f\"      Two-Tower has zero metrics - needs re-evaluation\")\n",
    "            recommended_model = \"ALS\"\n",
    "        elif tt_avg > als_avg:\n",
    "            improvement = ((tt_avg - als_avg) / als_avg) * 100\n",
    "            print(f\"\\n   ✅ RECOMMENDATION: Use Two-Tower\")\n",
    "            print(f\"      Reasons: {improvement:.2f}% better performance, more feature-rich\")\n",
    "            recommended_model = \"Two-Tower\"\n",
    "        else:\n",
    "            diff = ((als_avg - tt_avg) / tt_avg) * 100\n",
    "            print(f\"\\n   ✅ RECOMMENDATION: Use ALS\")\n",
    "            print(f\"      Reasons: {diff:.2f}% better performance (simpler, faster)\")\n",
    "            recommended_model = \"ALS\"\n",
    "\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "else:\n",
    "    print(\"  Unable to compare models. Need metrics from both models.\")\n",
    "    recommended_model = \"Unknown\"\n",
    "\n",
    "# ============================================\n",
    "# SAVE REPORT\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"💾 SAVING COMPREHENSIVE REPORT\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "report = {\n",
    "    'comparison_date': datetime.now().isoformat(),\n",
    "    'models_compared': comparison_df['Model'].unique().tolist() if not comparison_df.empty else [],\n",
    "    'metrics': comparison_df.to_dict('records') if not comparison_df.empty else [],\n",
    "    'recommended_model': recommended_model,\n",
    "    'model_details': {}\n",
    "}\n",
    "\n",
    "if als_loaded:\n",
    "    report['model_details']['ALS'] = {\n",
    "        'type': 'Collaborative Filtering (Matrix Factorization)',\n",
    "        'hyperparameters': als_data.get('hyperparameters', {}),\n",
    "        'model_loaded': True,\n",
    "        'metrics_available': list(als_test_metrics.keys()) if als_test_metrics else []\n",
    "    }\n",
    "\n",
    "if tt_loaded:\n",
    "    report['model_details']['Two-Tower'] = {\n",
    "        'type': 'Neural Network (Deep Learning)',\n",
    "        'model_loaded': True,\n",
    "        'model_path': str(tt_model_path),\n",
    "        'metrics_available': list(tt_test_metrics.keys()) if tt_test_metrics else []\n",
    "    }\n",
    "\n",
    "with open(COMPARISON_DIR / \"comparison_report.json\", 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"✅ Saved: {COMPARISON_DIR / 'comparison_report.json'}\")\n",
    "\n",
    "with open(COMPARISON_DIR / \"comparison_summary.txt\", 'w') as f:\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "    f.write(\"MODEL COMPARISON SUMMARY\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "\n",
    "    if not comparison_df.empty:\n",
    "        f.write(\"METRICS COMPARISON:\\n\")\n",
    "        f.write(comparison_df.to_string(index=False))\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "    f.write(f\"RECOMMENDED MODEL: {recommended_model}\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(f\"✅ Saved: {COMPARISON_DIR / 'comparison_summary.txt'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ MODEL COMPARISON COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n All results saved to: {COMPARISON_DIR}\")\n",
    "print(\"\\n📊 Files created:\")\n",
    "print(f\"   • metrics_comparison.csv\")\n",
    "if (COMPARISON_DIR / \"metrics_comparison.png\").exists():\n",
    "    print(f\"   • metrics_comparison.png\")\n",
    "print(f\"   • comparison_report.json\")\n",
    "print(f\"   • comparison_summary.txt\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731f8b02",
   "metadata": {
    "id": "731f8b02"
   },
   "source": [
    "---\n",
    "\n",
    "# 📦 Appendix: Additional Utilities (Optional)\n",
    "\n",
    "These utilities provide advanced features like feature engineering, vector search, and diversity optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd2eae4",
   "metadata": {
    "id": "cdd2eae4"
   },
   "source": [
    "## A1️⃣ Advanced Data Preprocessing\n",
    "\n",
    "Functions for feature engineering, temporal features, and advanced preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63179571",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63179571",
    "outputId": "1046444a-435d-48e3-a32f-0a360fcdb816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "📦 ADVANCED FEATURE ENGINEERING FUNCTIONS LOADED\n",
      "======================================================================\n",
      "\n",
      "✅ Available functions:\n",
      "   • extract_user_features(users_df)\n",
      "   • extract_recipe_features(recipes_df)\n",
      "   • extract_temporal_features(interactions_df)\n",
      "   • create_interaction_weights(interactions_df)\n",
      "\n",
      "💡 Use these for more sophisticated feature engineering!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Advanced Feature Engineering Functions\n",
    "These functions extract rich features from user, recipe, and interaction data\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from datetime import datetime, timedelta\n",
    "import ast\n",
    "\n",
    "def extract_user_features(users_df):\n",
    "    \"\"\"\n",
    "    Extract advanced user features including:\n",
    "    - Preference analysis\n",
    "    - Engagement metrics\n",
    "    - Temporal features\n",
    "    \"\"\"\n",
    "    df = users_df.copy()\n",
    "\n",
    "    # Parse preferences JSON if exists\n",
    "    if 'preferences' in df.columns:\n",
    "        try:\n",
    "            df['preferences'] = df['preferences'].apply(\n",
    "                lambda x: ast.literal_eval(x) if isinstance(x, str) else (x if isinstance(x, dict) else {})\n",
    "            )\n",
    "\n",
    "            # Extract dietary preferences count\n",
    "            df['dietary_preferences_count'] = df['preferences'].apply(\n",
    "                lambda x: len(x.get('dietary', [])) if isinstance(x, dict) else 0\n",
    "            )\n",
    "\n",
    "            # Extract cuisine preferences count\n",
    "            df['cuisine_preferences_count'] = df['preferences'].apply(\n",
    "                lambda x: len(x.get('cuisines', [])) if isinstance(x, dict) else 0\n",
    "            )\n",
    "\n",
    "            # Extract skill level\n",
    "            df['skill_level'] = df['preferences'].apply(\n",
    "                lambda x: x.get('skill_level', 'Beginner') if isinstance(x, dict) else 'Beginner'\n",
    "            )\n",
    "\n",
    "            # Encode skill level\n",
    "            le = LabelEncoder()\n",
    "            df['skill_level_encoded'] = le.fit_transform(df['skill_level'].fillna('Beginner'))\n",
    "\n",
    "            print(f\"✅ Extracted {len([c for c in df.columns if 'preference' in c])} preference features\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Could not parse preferences: {e}\")\n",
    "\n",
    "    # Convert boolean columns if they exist\n",
    "    for col in ['verified', 'chef']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(int)\n",
    "\n",
    "    # Temporal features\n",
    "    if 'created_at' in df.columns:\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "        current_date = datetime.now()\n",
    "        df['user_tenure_days'] = (current_date - df['created_at']).dt.days\n",
    "        df['user_tenure_weeks'] = df['user_tenure_days'] / 7\n",
    "        print(f\"✅ Added temporal features (tenure)\")\n",
    "\n",
    "    # Engagement score\n",
    "    if 'engagement_score' in df.columns:\n",
    "        df['engagement_score'] = df['engagement_score'].fillna(0)\n",
    "        df['engagement_level'] = pd.cut(df['engagement_score'],\n",
    "                                        bins=[-np.inf, 0.3, 0.6, np.inf],\n",
    "                                        labels=['Low', 'Medium', 'High'])\n",
    "        print(f\"✅ Categorized engagement levels\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def extract_recipe_features(recipes_df):\n",
    "    \"\"\"\n",
    "    Extract advanced recipe features including:\n",
    "    - Complexity metrics\n",
    "    - Popularity signals\n",
    "    - Freshness scores\n",
    "    \"\"\"\n",
    "    df = recipes_df.copy()\n",
    "\n",
    "    # Parse list columns\n",
    "    for col in ['ingredients', 'instructions', 'tags']:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                df[col] = df[col].apply(\n",
    "                    lambda x: ast.literal_eval(x) if isinstance(x, str) else (x if isinstance(x, list) else [])\n",
    "                )\n",
    "                df[f'num_{col}'] = df[col].apply(len)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Could not parse {col}: {e}\")\n",
    "\n",
    "    # Complexity score (combination of ingredients, steps, cook time)\n",
    "    if 'num_ingredients' in df.columns and 'num_instructions' in df.columns:\n",
    "        df['complexity_score'] = (\n",
    "            df['num_ingredients'].fillna(0) * 0.3 +\n",
    "            df['num_instructions'].fillna(0) * 0.5 +\n",
    "            df['cook_time'].fillna(30) / 10 * 0.2\n",
    "        )\n",
    "        print(f\"✅ Calculated complexity scores\")\n",
    "\n",
    "    # Freshness score (exponential decay based on creation date)\n",
    "    if 'created_date' in df.columns:\n",
    "        df['created_date'] = pd.to_datetime(df['created_date'])\n",
    "        current_date = datetime.now()\n",
    "        df['days_since_creation'] = (current_date - df['created_date']).dt.days\n",
    "        df['freshness_score'] = np.exp(-df['days_since_creation'] / 30.0)  # 30-day half-life\n",
    "        print(f\"✅ Calculated freshness scores\")\n",
    "\n",
    "    # Popularity composite score\n",
    "    engagement_cols = ['view_count', 'save_count', 'like_count', 'comment_count']\n",
    "    if all(col in df.columns for col in engagement_cols):\n",
    "        # Normalize each metric\n",
    "        scaler = StandardScaler()\n",
    "        df[engagement_cols] = df[engagement_cols].fillna(0)\n",
    "        df['popularity_composite'] = scaler.fit_transform(df[engagement_cols]).mean(axis=1)\n",
    "        print(f\"✅ Calculated composite popularity scores\")\n",
    "\n",
    "    # Engagement ratios\n",
    "    if 'view_count' in df.columns and 'save_count' in df.columns:\n",
    "        df['save_rate'] = df['save_count'] / (df['view_count'] + 1)  # +1 to avoid division by zero\n",
    "        df['like_rate'] = df['like_count'] / (df['view_count'] + 1)\n",
    "        print(f\"✅ Calculated engagement rates\")\n",
    "\n",
    "    # Fill missing values\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def extract_temporal_features(interactions_df):\n",
    "    \"\"\"\n",
    "    Extract temporal patterns from interactions:\n",
    "    - Hour of day, day of week\n",
    "    - Seasonality\n",
    "    - Recency scores\n",
    "    \"\"\"\n",
    "    df = interactions_df.copy()\n",
    "\n",
    "    if 'created_at' in df.columns:\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "        # Extract time components\n",
    "        df['hour_of_day'] = df['created_at'].dt.hour\n",
    "        df['day_of_week'] = df['created_at'].dt.dayofweek\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "        df['month'] = df['created_at'].dt.month\n",
    "\n",
    "        # Season mapping\n",
    "        season_map = {12: 0, 1: 0, 2: 0,  # Winter\n",
    "                      3: 1, 4: 1, 5: 1,   # Spring\n",
    "                      6: 2, 7: 2, 8: 2,   # Summer\n",
    "                      9: 3, 10: 3, 11: 3} # Fall\n",
    "        df['season'] = df['month'].map(season_map)\n",
    "\n",
    "        # Recency score (exponential decay)\n",
    "        current_date = datetime.now()\n",
    "        df['days_since_interaction'] = (current_date - df['created_at']).dt.days\n",
    "        df['recency_score'] = np.exp(-df['days_since_interaction'] / 7.0)  # 7-day half-life\n",
    "\n",
    "        print(f\"✅ Extracted {len([c for c in df.columns if c in ['hour_of_day', 'day_of_week', 'season', 'recency_score']])} temporal features\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_interaction_weights(interactions_df):\n",
    "    \"\"\"\n",
    "    Create weighted scores for different interaction types\n",
    "    VIEW < RATE < LIKE < COMMENT < SAVE < SHARE\n",
    "    \"\"\"\n",
    "    df = interactions_df.copy()\n",
    "\n",
    "    # Define action weights\n",
    "    action_weights = {\n",
    "        'VIEW': 1.0,\n",
    "        'LIKE': 3.0,\n",
    "        'SAVE': 5.0,\n",
    "        'COMMENT': 4.0,\n",
    "        'RATE': 2.0,\n",
    "        'SHARE': 6.0,\n",
    "        'COOK': 7.0  # If user actually cooked the recipe\n",
    "    }\n",
    "\n",
    "    if 'action' in df.columns:\n",
    "        df['interaction_weight'] = df['action'].map(action_weights).fillna(1.0)\n",
    "\n",
    "        # For ratings, multiply by the actual rating value\n",
    "        if 'value' in df.columns:\n",
    "            rating_mask = df['action'] == 'RATE'\n",
    "            df.loc[rating_mask, 'interaction_weight'] = \\\n",
    "                df.loc[rating_mask, 'interaction_weight'] * df.loc[rating_mask, 'value'].fillna(3.0) / 5.0\n",
    "\n",
    "        print(f\"✅ Created interaction weights for {df['action'].nunique()} action types\")\n",
    "        print(f\"   Weight distribution: min={df['interaction_weight'].min():.2f}, \"\n",
    "              f\"max={df['interaction_weight'].max():.2f}, \"\n",
    "              f\"mean={df['interaction_weight'].mean():.2f}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage in the notebook:\n",
    "print(\"=\"*70)\n",
    "print(\"📦 ADVANCED FEATURE ENGINEERING FUNCTIONS LOADED\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n✅ Available functions:\")\n",
    "print(\"   • extract_user_features(users_df)\")\n",
    "print(\"   • extract_recipe_features(recipes_df)\")\n",
    "print(\"   • extract_temporal_features(interactions_df)\")\n",
    "print(\"   • create_interaction_weights(interactions_df)\")\n",
    "print(\"\\n💡 Use these for more sophisticated feature engineering!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa33894",
   "metadata": {
    "id": "7fa33894"
   },
   "source": [
    "## A2️⃣ Vector Search & Similarity Functions\n",
    "\n",
    "FAISS-based vector search for fast candidate retrieval and cosine similarity calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3d78cd",
   "metadata": {
    "id": "6d3d78cd"
   },
   "outputs": [],
   "source": [
    "# Install FAISS for vector search (optional - uncomment if needed)\n",
    "# !pip install -q faiss-cpu\n",
    "\n",
    "\"\"\"\n",
    "Vector Search Utilities\n",
    "Fast similarity search using FAISS and cosine similarity\n",
    "\"\"\"\n",
    "\n",
    "def cosine_similarity_matrix(query_vectors, target_vectors):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between query and target vectors\n",
    "\n",
    "    Args:\n",
    "        query_vectors: [n_queries, dimension]\n",
    "        target_vectors: [n_targets, dimension]\n",
    "\n",
    "    Returns:\n",
    "        Similarity matrix [n_queries, n_targets]\n",
    "    \"\"\"\n",
    "    # Normalize vectors\n",
    "    query_norm = query_vectors / (np.linalg.norm(query_vectors, axis=1, keepdims=True) + 1e-8)\n",
    "    target_norm = target_vectors / (np.linalg.norm(target_vectors, axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "    # Compute dot product (cosine similarity for normalized vectors)\n",
    "    similarities = np.dot(query_norm, target_norm.T)\n",
    "\n",
    "    return similarities\n",
    "\n",
    "def build_faiss_index(embeddings, index_type='Flat'):\n",
    "    \"\"\"\n",
    "    Build FAISS index for fast nearest neighbor search\n",
    "\n",
    "    Args:\n",
    "        embeddings: numpy array of shape [n_items, dimension]\n",
    "        index_type: 'Flat' for exact search, 'IVF' for approximate search\n",
    "\n",
    "    Returns:\n",
    "        FAISS index\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import faiss\n",
    "\n",
    "        dimension = embeddings.shape[1]\n",
    "\n",
    "        if index_type == 'Flat':\n",
    "            # Exact search (slower but accurate)\n",
    "            index = faiss.IndexFlatL2(dimension)\n",
    "        elif index_type == 'IVF':\n",
    "            # Approximate search (faster)\n",
    "            nlist = min(100, embeddings.shape[0] // 10)  # Number of clusters\n",
    "            quantizer = faiss.IndexFlatL2(dimension)\n",
    "            index = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
    "            # Train index\n",
    "            index.train(embeddings.astype(np.float32))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown index type: {index_type}\")\n",
    "\n",
    "        # Add embeddings\n",
    "        index.add(embeddings.astype(np.float32))\n",
    "\n",
    "        print(f\"✅ FAISS index built with {index.ntotal} vectors\")\n",
    "        return index\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"⚠️  FAISS not installed. Using numpy-based search.\")\n",
    "        return None\n",
    "\n",
    "def faiss_search(index, query_embedding, k=10):\n",
    "    \"\"\"\n",
    "    Search for k nearest neighbors using FAISS\n",
    "\n",
    "    Args:\n",
    "        index: FAISS index\n",
    "        query_embedding: Query vector [dimension]\n",
    "        k: Number of neighbors\n",
    "\n",
    "    Returns:\n",
    "        distances, indices\n",
    "    \"\"\"\n",
    "    if index is None:\n",
    "        return None, None\n",
    "\n",
    "    # Reshape query\n",
    "    if query_embedding.ndim == 1:\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "\n",
    "    # Search\n",
    "    distances, indices = index.search(query_embedding.astype(np.float32), k)\n",
    "\n",
    "    return distances[0], indices[0]\n",
    "\n",
    "def numpy_search(embeddings, query_embedding, k=10):\n",
    "    \"\"\"\n",
    "    Fallback search using numpy (when FAISS is not available)\n",
    "\n",
    "    Args:\n",
    "        embeddings: All item embeddings [n_items, dimension]\n",
    "        query_embedding: Query vector [dimension]\n",
    "        k: Number of neighbors\n",
    "\n",
    "    Returns:\n",
    "        distances, indices\n",
    "    \"\"\"\n",
    "    # Calculate cosine similarities\n",
    "    similarities = cosine_similarity_matrix(\n",
    "        query_embedding.reshape(1, -1),\n",
    "        embeddings\n",
    "    )[0]\n",
    "\n",
    "    # Get top k\n",
    "    top_indices = np.argsort(similarities)[::-1][:k]\n",
    "    top_similarities = similarities[top_indices]\n",
    "\n",
    "    # Convert to distances (1 - similarity)\n",
    "    distances = 1 - top_similarities\n",
    "\n",
    "    return distances, top_indices\n",
    "\n",
    "def batch_similarity_search(user_embeddings, recipe_embeddings, k=10):\n",
    "    \"\"\"\n",
    "    Batch similarity search for multiple users\n",
    "\n",
    "    Args:\n",
    "        user_embeddings: [n_users, dimension]\n",
    "        recipe_embeddings: [n_recipes, dimension]\n",
    "        k: Number of recommendations per user\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping user_idx -> list of recipe indices\n",
    "    \"\"\"\n",
    "    # Calculate all similarities\n",
    "    similarities = cosine_similarity_matrix(user_embeddings, recipe_embeddings)\n",
    "\n",
    "    # Get top k for each user\n",
    "    recommendations = {}\n",
    "    for user_idx in range(len(user_embeddings)):\n",
    "        top_indices = np.argsort(similarities[user_idx])[::-1][:k]\n",
    "        recommendations[user_idx] = top_indices.tolist()\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "# Example: How to use these functions\n",
    "print(\"=\"*70)\n",
    "print(\"🔍 VECTOR SEARCH UTILITIES LOADED\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n✅ Available functions:\")\n",
    "print(\"   • cosine_similarity_matrix(query, targets)\")\n",
    "print(\"   • build_faiss_index(embeddings, index_type='Flat')\")\n",
    "print(\"   • faiss_search(index, query, k=10)\")\n",
    "print(\"   • numpy_search(embeddings, query, k=10)\")\n",
    "print(\"   • batch_similarity_search(user_embs, recipe_embs, k=10)\")\n",
    "print(\"\\n💡 Use these for fast recommendation retrieval!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Demo: Test with random embeddings\n",
    "demo_user_emb = np.random.randn(128)\n",
    "demo_recipe_embs = np.random.randn(100, 128)\n",
    "\n",
    "distances, indices = numpy_search(demo_recipe_embs, demo_user_emb, k=5)\n",
    "print(f\"\\n📊 Demo search results:\")\n",
    "print(f\"   Top 5 recipe indices: {indices}\")\n",
    "print(f\"   Distances: {distances}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7f450a",
   "metadata": {
    "id": "8b7f450a"
   },
   "source": [
    "## A3️⃣ Recommendation Diversity & Re-ranking\n",
    "\n",
    "Maximal Marginal Relevance (MMR) for diverse recommendations and advanced re-ranking strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498d793b",
   "metadata": {
    "id": "498d793b"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Diversity and Re-ranking Algorithms\n",
    "Improve recommendation quality through diversity and intelligent re-ranking\n",
    "\"\"\"\n",
    "\n",
    "def maximal_marginal_relevance(\n",
    "    candidate_ids,\n",
    "    candidate_scores,\n",
    "    embeddings,\n",
    "    alpha=0.5,\n",
    "    top_k=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Maximal Marginal Relevance (MMR) for diverse recommendations\n",
    "\n",
    "    Balances relevance and diversity in recommendation list\n",
    "\n",
    "    Args:\n",
    "        candidate_ids: List of candidate recipe IDs\n",
    "        candidate_scores: Relevance scores for candidates\n",
    "        embeddings: Recipe embeddings dictionary {recipe_id: embedding}\n",
    "        alpha: Trade-off parameter (1=only relevance, 0=only diversity)\n",
    "        top_k: Number of diverse recommendations to return\n",
    "\n",
    "    Returns:\n",
    "        List of diversified recipe IDs\n",
    "    \"\"\"\n",
    "    if len(candidate_ids) <= top_k:\n",
    "        return candidate_ids[:top_k]\n",
    "\n",
    "    # Sort candidates by relevance\n",
    "    sorted_candidates = sorted(\n",
    "        zip(candidate_ids, candidate_scores),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    selected = []\n",
    "    remaining = [c[0] for c in sorted_candidates]\n",
    "    remaining_scores = {c[0]: c[1] for c in sorted_candidates}\n",
    "\n",
    "    # Select first (most relevant) item\n",
    "    selected.append(remaining.pop(0))\n",
    "\n",
    "    while len(selected) < top_k and remaining:\n",
    "        mmr_scores = []\n",
    "\n",
    "        for candidate_id in remaining:\n",
    "            # Relevance score (normalized)\n",
    "            relevance = remaining_scores[candidate_id]\n",
    "            if max(remaining_scores.values()) > 0:\n",
    "                relevance = relevance / max(remaining_scores.values())\n",
    "\n",
    "            # Diversity score (minimum similarity to selected items)\n",
    "            if len(selected) > 0 and candidate_id in embeddings:\n",
    "                candidate_emb = embeddings[candidate_id]\n",
    "\n",
    "                similarities = []\n",
    "                for selected_id in selected:\n",
    "                    if selected_id in embeddings:\n",
    "                        selected_emb = embeddings[selected_id]\n",
    "                        # Cosine similarity\n",
    "                        sim = np.dot(candidate_emb, selected_emb) / (\n",
    "                            np.linalg.norm(candidate_emb) * np.linalg.norm(selected_emb) + 1e-8\n",
    "                        )\n",
    "                        similarities.append(sim)\n",
    "\n",
    "                if similarities:\n",
    "                    diversity = 1.0 - max(similarities)\n",
    "                else:\n",
    "                    diversity = 1.0\n",
    "            else:\n",
    "                diversity = 1.0\n",
    "\n",
    "            # MMR score\n",
    "            mmr = alpha * relevance + (1 - alpha) * diversity\n",
    "            mmr_scores.append((candidate_id, mmr))\n",
    "\n",
    "        # Select item with highest MMR\n",
    "        best_item = max(mmr_scores, key=lambda x: x[1])[0]\n",
    "        selected.append(best_item)\n",
    "        remaining.remove(best_item)\n",
    "\n",
    "    return selected\n",
    "\n",
    "def diversity_score(recipe_ids, embeddings):\n",
    "    \"\"\"\n",
    "    Calculate diversity score for a list of recommendations\n",
    "\n",
    "    Args:\n",
    "        recipe_ids: List of recipe IDs\n",
    "        embeddings: Recipe embeddings dictionary\n",
    "\n",
    "    Returns:\n",
    "        Diversity score (0-1, higher is more diverse)\n",
    "    \"\"\"\n",
    "    if len(recipe_ids) <= 1:\n",
    "        return 1.0\n",
    "\n",
    "    similarities = []\n",
    "    for i in range(len(recipe_ids)):\n",
    "        for j in range(i+1, len(recipe_ids)):\n",
    "            if recipe_ids[i] in embeddings and recipe_ids[j] in embeddings:\n",
    "                emb_i = embeddings[recipe_ids[i]]\n",
    "                emb_j = embeddings[recipe_ids[j]]\n",
    "\n",
    "                # Cosine similarity\n",
    "                sim = np.dot(emb_i, emb_j) / (\n",
    "                    np.linalg.norm(emb_i) * np.linalg.norm(emb_j) + 1e-8\n",
    "                )\n",
    "                similarities.append(sim)\n",
    "\n",
    "    if similarities:\n",
    "        # Average dissimilarity\n",
    "        return 1.0 - np.mean(similarities)\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "def rerank_by_freshness(recommendations, recipes_df, alpha=0.7):\n",
    "    \"\"\"\n",
    "    Re-rank recommendations to balance relevance and freshness\n",
    "\n",
    "    Args:\n",
    "        recommendations: List of (recipe_id, score) tuples\n",
    "        recipes_df: DataFrame with recipe metadata\n",
    "        alpha: Weight for original score (1-alpha for freshness)\n",
    "\n",
    "    Returns:\n",
    "        Re-ranked list of recipe IDs\n",
    "    \"\"\"\n",
    "    reranked = []\n",
    "\n",
    "    for recipe_id, score in recommendations:\n",
    "        # Get freshness score if available\n",
    "        if 'freshness_score' in recipes_df.columns:\n",
    "            recipe_row = recipes_df[recipes_df['recipe_id'] == recipe_id]\n",
    "            if not recipe_row.empty:\n",
    "                freshness = recipe_row['freshness_score'].iloc[0]\n",
    "            else:\n",
    "                freshness = 0.5  # Default\n",
    "        else:\n",
    "            freshness = 0.5\n",
    "\n",
    "        # Combined score\n",
    "        combined_score = alpha * score + (1 - alpha) * freshness\n",
    "        reranked.append((recipe_id, combined_score))\n",
    "\n",
    "    # Sort by combined score\n",
    "    reranked.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return [rid for rid, _ in reranked]\n",
    "\n",
    "def rerank_by_popularity(recommendations, recipes_df, alpha=0.8):\n",
    "    \"\"\"\n",
    "    Re-rank to balance relevance and popularity\n",
    "\n",
    "    Args:\n",
    "        recommendations: List of (recipe_id, score) tuples\n",
    "        recipes_df: DataFrame with recipe metadata\n",
    "        alpha: Weight for original score (1-alpha for popularity)\n",
    "\n",
    "    Returns:\n",
    "        Re-ranked list of recipe IDs\n",
    "    \"\"\"\n",
    "    reranked = []\n",
    "\n",
    "    # Normalize popularity scores\n",
    "    if 'popularity_score' in recipes_df.columns:\n",
    "        max_pop = recipes_df['popularity_score'].max()\n",
    "        min_pop = recipes_df['popularity_score'].min()\n",
    "    else:\n",
    "        max_pop = min_pop = 0\n",
    "\n",
    "    for recipe_id, score in recommendations:\n",
    "        # Get popularity score if available\n",
    "        if 'popularity_score' in recipes_df.columns and max_pop > min_pop:\n",
    "            recipe_row = recipes_df[recipes_df['recipe_id'] == recipe_id]\n",
    "            if not recipe_row.empty:\n",
    "                pop = recipe_row['popularity_score'].iloc[0]\n",
    "                normalized_pop = (pop - min_pop) / (max_pop - min_pop)\n",
    "            else:\n",
    "                normalized_pop = 0.5\n",
    "        else:\n",
    "            normalized_pop = 0.5\n",
    "\n",
    "        # Combined score\n",
    "        combined_score = alpha * score + (1 - alpha) * normalized_pop\n",
    "        reranked.append((recipe_id, combined_score))\n",
    "\n",
    "    # Sort by combined score\n",
    "    reranked.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return [rid for rid, _ in reranked]\n",
    "\n",
    "def apply_business_rules(recommendations, recipes_df, user_preferences=None):\n",
    "    \"\"\"\n",
    "    Apply business rules to filter/boost recommendations\n",
    "\n",
    "    Args:\n",
    "        recommendations: List of recipe IDs\n",
    "        recipes_df: DataFrame with recipe metadata\n",
    "        user_preferences: Dictionary of user preferences\n",
    "\n",
    "    Returns:\n",
    "        Filtered/boosted list of recipe IDs\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "\n",
    "    for recipe_id in recommendations:\n",
    "        recipe = recipes_df[recipes_df['recipe_id'] == recipe_id]\n",
    "\n",
    "        if recipe.empty:\n",
    "            continue\n",
    "\n",
    "        recipe = recipe.iloc[0]\n",
    "\n",
    "        # Skip if violates user dietary restrictions\n",
    "        if user_preferences and 'dietary_restrictions' in user_preferences:\n",
    "            if recipe.get('dietary_type') in user_preferences['dietary_restrictions']:\n",
    "                continue  # Skip this recipe\n",
    "\n",
    "        # Skip extremely difficult recipes for beginners\n",
    "        if user_preferences and user_preferences.get('skill_level') == 'Beginner':\n",
    "            if recipe.get('difficulty') == 'Expert':\n",
    "                continue\n",
    "\n",
    "        # Skip recipes with excessive cook time if user prefers quick meals\n",
    "        if user_preferences and user_preferences.get('max_cook_time'):\n",
    "            max_time = user_preferences['max_cook_time']\n",
    "            if recipe.get('cook_time', 0) > max_time:\n",
    "                continue\n",
    "\n",
    "        filtered.append(recipe_id)\n",
    "\n",
    "    return filtered\n",
    "\n",
    "# Example usage\n",
    "print(\"=\"*70)\n",
    "print(\"🎨 DIVERSITY & RE-RANKING UTILITIES LOADED\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n✅ Available functions:\")\n",
    "print(\"   • maximal_marginal_relevance(candidates, scores, embeddings, alpha=0.5)\")\n",
    "print(\"   • diversity_score(recipe_ids, embeddings)\")\n",
    "print(\"   • rerank_by_freshness(recommendations, recipes_df, alpha=0.7)\")\n",
    "print(\"   • rerank_by_popularity(recommendations, recipes_df, alpha=0.8)\")\n",
    "print(\"   • apply_business_rules(recommendations, recipes_df, user_prefs)\")\n",
    "print(\"\\n💡 Use these to improve recommendation quality!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Demo: Test MMR\n",
    "demo_candidates = list(range(20))\n",
    "demo_scores = np.random.rand(20)\n",
    "demo_embeddings = {i: np.random.randn(128) for i in range(20)}\n",
    "\n",
    "diverse_recs = maximal_marginal_relevance(\n",
    "    demo_candidates,\n",
    "    demo_scores,\n",
    "    demo_embeddings,\n",
    "    alpha=0.5,\n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Demo MMR results:\")\n",
    "print(f\"   Original top 10: {demo_candidates[:10]}\")\n",
    "print(f\"   Diversified top 10: {diverse_recs}\")\n",
    "print(f\"   Diversity score: {diversity_score(diverse_recs, demo_embeddings):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89a4ada",
   "metadata": {
    "id": "f89a4ada"
   },
   "source": [
    "## A4️⃣ Complete Usage Example\n",
    "\n",
    "Example showing how to use all the advanced utilities together for production-grade recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eccf28",
   "metadata": {
    "id": "11eccf28"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "COMPLETE PRODUCTION-GRADE RECOMMENDATION PIPELINE\n",
    "This example shows how to use all utilities together\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"🎯 PRODUCTION-GRADE RECOMMENDATION PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ========== Step 1: Advanced Feature Engineering ==========\n",
    "print(\"\\n1️⃣ Extracting advanced features...\")\n",
    "\n",
    "# Apply advanced feature extraction\n",
    "users_enhanced = extract_user_features(users_df)\n",
    "recipes_enhanced = extract_recipe_features(recipes_df)\n",
    "interactions_enhanced = extract_temporal_features(interactions_df)\n",
    "interactions_enhanced = create_interaction_weights(interactions_enhanced)\n",
    "\n",
    "print(f\"✅ Enhanced users: {users_enhanced.shape}\")\n",
    "print(f\"✅ Enhanced recipes: {recipes_enhanced.shape}\")\n",
    "print(f\"✅ Enhanced interactions: {interactions_enhanced.shape}\")\n",
    "\n",
    "# ========== Step 2: Generate Base Recommendations ==========\n",
    "print(\"\\n2️⃣ Generating base recommendations using best model...\")\n",
    "\n",
    "# Use your best model (from comparison earlier)\n",
    "# Example: using ALS model\n",
    "sample_user_id = val_users[0]\n",
    "sample_user_idx = sample_user_id  # Assuming direct mapping\n",
    "\n",
    "# Get ALS recommendations\n",
    "als_scores = als_model.user_factors[sample_user_idx].dot(als_model.item_factors.T)\n",
    "top_100_indices = np.argsort(als_scores)[::-1][:100]\n",
    "top_100_scores = als_scores[top_100_indices]\n",
    "\n",
    "candidate_recommendations = list(zip(top_100_indices, top_100_scores))\n",
    "print(f\"✅ Generated {len(candidate_recommendations)} candidate recommendations\")\n",
    "\n",
    "# ========== Step 3: Apply Diversity (MMR) ==========\n",
    "print(\"\\n3️⃣ Applying diversity optimization (MMR)...\")\n",
    "\n",
    "# Create embeddings dictionary (using recipe IDs as keys)\n",
    "# In production, you'd use actual learned embeddings\n",
    "recipe_embeddings_dict = {\n",
    "    recipe_id: np.random.randn(128)  # Placeholder - use actual embeddings\n",
    "    for recipe_id in top_100_indices\n",
    "}\n",
    "\n",
    "# Apply MMR for diversity\n",
    "diverse_recommendations = maximal_marginal_relevance(\n",
    "    candidate_ids=[rid for rid, _ in candidate_recommendations],\n",
    "    candidate_scores=[score for _, score in candidate_recommendations],\n",
    "    embeddings=recipe_embeddings_dict,\n",
    "    alpha=0.6,  # 60% relevance, 40% diversity\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "diversity = diversity_score(diverse_recommendations, recipe_embeddings_dict)\n",
    "print(f\"✅ Diversified to 50 recommendations (diversity score: {diversity:.4f})\")\n",
    "\n",
    "# ========== Step 4: Re-rank by Freshness ==========\n",
    "print(\"\\n4️⃣ Re-ranking by freshness...\")\n",
    "\n",
    "diverse_with_scores = [\n",
    "    (rid, score)\n",
    "    for rid, score in candidate_recommendations\n",
    "    if rid in diverse_recommendations\n",
    "]\n",
    "\n",
    "fresh_recommendations = rerank_by_freshness(\n",
    "    diverse_with_scores,\n",
    "    recipes_enhanced,\n",
    "    alpha=0.7  # 70% relevance, 30% freshness\n",
    ")\n",
    "\n",
    "print(f\"✅ Re-ranked by freshness (top 20 recipes)\")\n",
    "\n",
    "# ========== Step 5: Apply Business Rules ==========\n",
    "print(\"\\n5️⃣ Applying business rules and filters...\")\n",
    "\n",
    "# Example user preferences\n",
    "user_prefs = {\n",
    "    'dietary_restrictions': [],  # e.g., ['Vegan', 'Gluten-Free']\n",
    "    'skill_level': 'Intermediate',\n",
    "    'max_cook_time': 60  # minutes\n",
    "}\n",
    "\n",
    "final_recommendations = apply_business_rules(\n",
    "    fresh_recommendations[:30],\n",
    "    recipes_enhanced,\n",
    "    user_prefs\n",
    ")\n",
    "\n",
    "print(f\"✅ Applied business rules (remaining: {len(final_recommendations)} recipes)\")\n",
    "\n",
    "# ========== Step 6: Final Top 10 ==========\n",
    "print(\"\\n6️⃣ Final top 10 recommendations:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "final_top_10 = final_recommendations[:10]\n",
    "\n",
    "for rank, recipe_id in enumerate(final_top_10, 1):\n",
    "    recipe_info = recipes_enhanced[recipes_enhanced['recipe_id'] == recipe_id]\n",
    "    if not recipe_info.empty:\n",
    "        recipe = recipe_info.iloc[0]\n",
    "        print(f\"{rank:2d}. Recipe #{recipe_id}\")\n",
    "        if 'title' in recipe:\n",
    "            print(f\"    Title: {recipe.get('title', 'N/A')}\")\n",
    "        if 'difficulty' in recipe:\n",
    "            print(f\"    Difficulty: {recipe.get('difficulty', 'N/A')}\")\n",
    "        if 'cook_time' in recipe:\n",
    "            print(f\"    Cook Time: {recipe.get('cook_time', 'N/A')} min\")\n",
    "        if 'popularity_score' in recipe:\n",
    "            print(f\"    Popularity: {recipe.get('popularity_score', 0):.2f}\")\n",
    "        if 'freshness_score' in recipe:\n",
    "            print(f\"    Freshness: {recipe.get('freshness_score', 0):.2f}\")\n",
    "        print()\n",
    "\n",
    "# ========== Pipeline Summary ==========\n",
    "print(\"=\"*70)\n",
    "print(\"✅ RECOMMENDATION PIPELINE COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n📊 Pipeline Stages:\")\n",
    "print(f\"   1. Base candidates: 100 recipes\")\n",
    "print(f\"   2. After diversity (MMR): 50 recipes\")\n",
    "print(f\"   3. After freshness re-rank: 30 recipes\")\n",
    "print(f\"   4. After business rules: {len(final_recommendations)} recipes\")\n",
    "print(f\"   5. Final recommendations: 10 recipes\")\n",
    "print(f\"\\n   Diversity Score: {diversity:.4f}\")\n",
    "print(\"\\n💡 This pipeline can be used in production API!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c604c9",
   "metadata": {
    "id": "e9c604c9"
   },
   "source": [
    "---\n",
    "\n",
    "## 🎓 Notebook Complete!\n",
    "\n",
    "### ✅ What You've Built:\n",
    "\n",
    "**Core Models (Parts 1-3):**\n",
    "- 🔴 **ALS** - Collaborative filtering (implicit feedback)\n",
    "- 🔵 **Two-Tower** - Neural network with user & recipe features\n",
    "- 🔷 **GraphSAGE** - Graph neural network with social graph\n",
    "\n",
    "**Advanced Utilities (Appendix):**\n",
    "- 📦 **Feature Engineering** - Extract rich features from raw data\n",
    "- 🔍 **Vector Search** - Fast similarity search with FAISS\n",
    "- 🎨 **Diversity & Re-ranking** - MMR, freshness, popularity\n",
    "- 🎯 **Production Pipeline** - Complete end-to-end example\n",
    "\n",
    "### 📚 What's Included from `ml_model`:\n",
    "\n",
    "✅ **Data Preprocessing** (`data_loader.py`)\n",
    "- Advanced feature extraction\n",
    "- Temporal features\n",
    "- Interaction weighting\n",
    "- Social graph construction\n",
    "\n",
    "✅ **Utilities** (`utils.py`)\n",
    "- FAISS vector search\n",
    "- Cosine similarity\n",
    "- Batch operations\n",
    "\n",
    "✅ **Configuration** (`config.py`)\n",
    "- Model hyperparameters\n",
    "- Feature definitions\n",
    "- Training settings\n",
    "\n",
    "✅ **Metrics** (`metrics.py`)\n",
    "- NDCG, Precision, Recall\n",
    "- MAP, MRR\n",
    "- Comparison functions\n",
    "\n",
    "### 🚀 Next Steps:\n",
    "\n",
    "1. **Train All Models** → Run cells sequentially\n",
    "2. **Compare Results** → See which model performs best\n",
    "3. **Download Models** → Save to Google Drive\n",
    "4. **Create API** → Follow `API_DEPLOYMENT_GUIDE.md`\n",
    "5. **Deploy** → Integrate with Java backend\n",
    "\n",
    "### 💾 Download These Files:\n",
    "\n",
    "After training completes:\n",
    "```\n",
    "/MyDrive/recipe_models/\n",
    "├── als/               → Download entire folder\n",
    "├── two_tower/         → Download entire folder  \n",
    "└── graphsage/         → Download entire folder\n",
    "```\n",
    "\n",
    "### 📖 Documentation:\n",
    "\n",
    "- `COLAB_TRAINING_GUIDE.md` - How to use this notebook\n",
    "- `API_DEPLOYMENT_GUIDE.md` - How to create API in VS Code\n",
    "- `COLAB_QUICK_REFERENCE.md` - Quick reference card\n",
    "\n",
    "---\n",
    "\n",
    "**🎉 Happy Training! Your ML recommendation system is ready!** 🍳👨‍🍳"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
